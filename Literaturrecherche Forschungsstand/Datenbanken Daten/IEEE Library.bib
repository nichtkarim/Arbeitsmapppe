% This file was created with Citavi 6.19.2.1

@inproceedings{Abdelraheem.,
 abstract = {Bias in software systems poses ethical concerns that may lead to unintended discrimination, particularly when sensitive variables (e.g., gender, ethnicity) influence decision-making processes. While bias detection in machine learning models has been extensively studied, traditional software systems remain largely unexplored. However, implicit bias can manifest in conditional logic, user role definitions, or static decision trees, which directly influences user experience and access equity in real-world applications (e.g., government services or healthcare platforms). This paper presents a novel static analysis methodology for detecting and quantifying bias in general-purpose software systems. The proposed approach leverages static backward slicing to isolate relevant code, builds a Control Flow Graph (CFG) to trace sensitive variables in conditional branches, and utilizes a Control Dependency Graph (CDG) to assess bias propagation in a weighted-analysis format. Two bias metrics are inferred from the process: Bias Impact Score (BIS), which quantifies how the detected bias influences code execution, and the Bias Severity Score (BSS), which measures the broader implications of the impact. A final composite metric is introduced combining static code structure and ML-based sensitivity analysis. The proposed methodology is evaluated using an LLM-generated dataset of 3920 code snippets from prior research, covering different demographic bias directions such as ethnicity, gender, religion, and occupation. Results, achieving 94.6{\%} accuracy in bias detection, show that the proposed methodology effectively identifies and quantifies bias, allowing developers to mitigate ethical risks early in the software development lifecycle. This research paper provides a foundation for ethical software engineering by offering a systematic and scalable approach to bias detection not only limited to AI-driven models. The methodology currently focuses on Python code and may require adaptation for multi-file projects, reflecting scalability trade-offs. This opens future work opportunities including advanced contextual and implicit bias detection in several frameworks.},
 author = {Abdelraheem, Ahmed and Elbanna, Malak and Elnaggar, Mohamed and Shawky, Doaa},
 title = {Defining a New Metric for Detecting Bias in Software Systems: Towards Ethical Software Engineering},
 keywords = {and program slicing;bias detection;Codes;control flow analysis;ethical software systems;Ethics;Ethnicity;Machine Learning;machine learning (ML);Measurement;Software engineering;Software systems;Software testing;Static analysis;Systematics;User experience},
 pages = {1--6},
 doi = {10.1109/ICEENG64546.2025.11031392}
}


@inproceedings{Pourasad.,
 abstract = {Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM, we predicted usability issues in two open-source apps of a medium complexity and asked two usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.},
 author = {Pourasad, Ali Ebrahimi and Maalej, Walid},
 title = {Does GenAI Make Usability Testing Obsolete?},
 keywords = {AI4SE;AI-Inspired Design;App Development;Foundation models;Large Language Model;Large language models;Mobile applications;Predictive models;Quality Requirements;Recommender systems;Reviews;Software engineering;Source coding;Testing;Usability;Usability Engineering;User experience},
 pages = {437--449},
 doi = {10.1109/ICSE55347.2025.00138}
}


@inproceedings{Qasse.,
 abstract = {The potential of automatic code generation through Model-Driven Engineering (MDE) frameworks has yet to be realized. Beyond their ability to help software professionals write more accurate, reusable code, MDE frameworks could make programming accessible for a new class of domain experts. However, domain experts have been slow to embrace these tools, as they still need to learn how to specify their applications' requirements using the concrete syntax (i.e., textual or graphical) of the new and unified domain-specific language. Conversational interfaces (chatbots) could smooth the learning process and offer a more interactive way for domain experts to specify their application requirements and generate the desired code. If integrated with MDE frameworks, chatbots may offer domain experts with richer domain vocabulary without sacrificing the power of agnosticism that unified modelling frameworks provide. In this paper, we discuss the challenges of integrating chatbots within MDE frameworks and then examine a specific application: the auto-generation of smart contract code based on conversational syntax. We demonstrate how this can be done and evaluate our approach by conducting a user experience survey to assess the usability and functionality of the chatbot framework. The paper concludes by drawing attention to the potential benefits of leveraging Language Models (LLMs) in this context.},
 author = {Qasse, I. and Mishra, S. and {{\th}. J{\'o}nsson}, B. and Khomh, F. and Hamdaqa, M.},
 title = {Chat2Code: A Chatbot for Model Specification and Code Generation, The Case of Smart Contracts},
 keywords = {Automatic Code Generation;Blockchain;Chatbots;Codes;Model-driven Engineering;Natural language processing;Oral communication;Smart contracts;Surveys;Syntactics;Vocabulary},
 pages = {50--60-50--60},
 doi = {10.1109/SSE60056.2023.00018}
}


@article{Qin.2025,
 abstract = {Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to LLMs' limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system). To address the limitation, this paper presents SoapFL, which builds an LLM-driven standard operating procedure (SOP) to automatically localize buggy methods from the entire software. By simulating the behavior of a human developer, SoapFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within specific steps, SoapFL provides useful test behavior or coverage information to LLM through program analysis. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that SoapFL can localize 175 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in SoapFL with the ablation study and demonstrate the usability of SoapFL through a user study. Finally, the cost analysis shows that SoapFL spends an average of only 0.081 dollars and 92 seconds for a single bug.},
 author = {Qin, Yihao and Wang, Shangwen and Lou, Yiling and Dong, Jinhao and Wang, Kaixin and Li, Xiaoling and Mao, Xiaoguang},
 year = {2025},
 title = {SoapFL: A Standard Operating Procedure for LLM-Based Method-Level Fault Localization},
 keywords = {Benchmark testing;Codes;Computer bugs;Debugging;Electronic mail;Fault Localization;Large Language Model;Large language models;Location awareness;Navigation;Standards;Usability},
 pages = {1173--1187},
 volume = {51},
 number = {4},
 issn = {1939-3520},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2025.3543187}
}


@inproceedings{Rafly.,
 abstract = {Kampung Batik Laweyan, a historic neighborhood in Surakarta, Indonesia, is renowned for its rich cultural heritage and tradition of batik textile production. With a history spanning several centuries, this area has become a vibrant hub for batik artisans and enthusiasts, showcasing intricate designs and craftsmanship that reflect local identity. The development of a mobile UI/UX application aimed at enhancing tourism in Kampung Batik Laweyan employs a Multi-User Centered Design (MUCD) methodology. The project unfolds in two iterations, beginning with a Minimum Viable Product (MVP) followed by the collection of feedback from primary user groups, including local business operators, residents, and tourists. Data collection involves interviews with local authorities, observations of the Micro, Small, and Medium Enterprises (MSME) environment in Laweyan, and an evaluation of current tourism marketing strategies. This comprehensive approach ensures that the application effectively caters to the diverse needs of its users. By integrating location-based gamification and a Large Language Model (LLM), the application not only enhances user engagement but also promotes the cultural and economic potential of the region. The System Usability Scale (SUS) assessment reveals an average score of 72.5, indicating a {\textquotedbl}good{\textquotedbl} level of usability, with qualitative feedback affirming its intuitive design and potential for further mobile app development.},
 author = {Rafly, Muhammad Tohir and Arisandi, Desi and Tony, Tony and Pranata, Edward Brainard and Siwi, Samsu Hendra and Priyomarsono, Naniek Widayati},
 title = {Multi User Centered Design (MUCD) in Mobile UI/UX Development for Kampung Batik Laweyan},
 keywords = {Business;Cultural differences;Economics;Interviews;Kampung Batik Laweyan;Large language models;mobile application;Mobile applications;Multi-User Centered Design (MUCD);Textiles;tourism enhancement;UI/UX design;Usability;User centered design;User experience},
 pages = {354--360},
 doi = {10.1109/ICITSI65188.2024.10929148}
}


@inproceedings{Raghi.,
 abstract = {The Software Development Lifecycle (SDLC) is a structured process that guides the development of software projects, encompassing phases from planning to deployment. Traditionally, the SDLC has relied on manual input, making it prone to delays, errors, and inefficiencies. With the recent advancements in Generative AI (GenAI) and Large Language Models (LLMs) such as GPT, it is now feasible to automate substantial portions of the SDLC. This paper presents a novel approach to automating the SDLC using LLMs and the Langchain framework, aiming to streamline the entire software development process. By au-tomating key phases, including project planning, requirements gathering, code generation, testing, and deployment, this research explores how AI can minimize human intervention and accelerate software development timelines. The paper also discusses the potential advantages of AI-driven SDLC automation, such as improved efficiency, consistency, and scalability, while addressing challenges related to its integration. The proposed approach offers a glimpse into the future of software engineering, where AI plays a central role in transforming how software is developed and delivered.},
 author = {Raghi, K. R. and Sudha, K. and {Sreeram A}, M. and {Joshua S}, Steve},
 title = {Software Development Automation Using Generative AI},
 keywords = {AI-driven software development;Automation;Code Generation;Generative AI;Generative AI (GenAI);GPT;LangChain;Large language models (LLMs);Manuals;Planning;Scalability;Scientific computing;SDLC automation;Software;Software Development Lifecycle (SDLC);Software development management;Software engineering;Testing},
 pages = {1--6},
 doi = {10.1109/ICERCS63125.2024.10894980}
}


@inproceedings{Rajendran.,
 abstract = {Modern software systems demand continuous evolution to maintain performance, scalability, and security. Traditional single-agent AI-driven code refactoring approaches are often limited in addressing the multi-faceted constraints (e.g., performance, security, maintainability) that emerge during complex software design tasks. In this paper, we propose a novel Multi-Agent Large Language Model (LLM) Environment for automated software design and refactoring. Our conceptual framework comprises specialized LLM ``experts,'' each trained or fine-tuned on a different aspect of software engineering (performance optimization, security hardening, UI/UX, maintainability). These agents collaborate in a cooperative or competitive fashion-using coordination protocols akin to consensus or auction mechanisms-to synthesize design insights and refactoring recommendations. We present formal definitions of agent interactions (including mathematical notation for termination conditions), a sequence diagram demonstrating agent collaboration, a complexity analysis of the coordination mechanism, and an expanded reference list. Preliminary experimental design is outlined to demonstrate how multi-agent interactions may resolve conflicting design goals more effectively than a single-agent approach. Our aim is to provide a roadmap for integrating multi-agent LLMs into the software development lifecycle, thereby improving development efficiency, reducing technical debt, and enhancing software quality.},
 author = {Rajendran, Vasanth and Besiahgari, Dinesh and Patil, Sachin C. and Chandrashekaraiah, Manjunath and Challagulla, Vishnu},
 title = {A Multi-Agent LLM Environment for Software Design and Refactoring: A Conceptual Framework},
 keywords = {Agent specialization;Auction mechanisms;Code quality;Codes;Consensus protocols;Large language models;Multi-agent systems;Optimization;Scalability;Security;Software design;Software development management;Software engineering;Software quality;Software refactoring;Software systems},
 pages = {488--493},
 doi = {10.1109/SoutheastCon56624.2025.10971563}
}


@inproceedings{Rasool.,
 abstract = {Large language models (LLMs) enable state-of-the-art semantic capabilities to be added to software systems such as semantic search of unstructured documents and text generation. However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience. To address this problem, semantic caches are used to check for answers to similar queries (that may have been phrased differently) without hitting the LLM service. Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system. Adopting semantic cache techniques usually requires testing the effectiveness of a semantic cache (accurate cache hits and misses) which requires a labelled test set of similar queries and responses which is often unavailable. In this paper, we present VaryGen, an approach for using LLMs for test input generation that produces similar questions from unstructured text documents. Our novel approach uses the reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise subtle variations to queries, and 3) evaluate the synthesised test dataset. We evaluated our approach in the domain of a student question and answer system by qualitatively analysing 100 generated queries and result pairs, and conducting an empirical case study with an open source semantic cache. Our results show that query pairs satisfy human expectations of similarity and our generated data demonstrates failure cases of a semantic cache. Additionally, we also evaluate our approach on Qasper dataset. This work is an important first step into test input generation for semantic applications and presents considerations for practitioners when calibrating a semantic cache.CCS CONCEPTS• Software and its engineering $\rightarrow$ Empirical software validation.},
 author = {Rasool, Z. and Barnett, S. and Willie, D. and Kurniawan, S. and Balugo, S. and Thudumu, S. and Abdelrazek, M.},
 title = {LLMs for Test Input Generation for Semantic Applications},
 keywords = {Calibration;Costs;Large Language Model;Query Evaluation;question answering;Semantic Cache;Semantic search;Semantics;Software reliability;Software systems;Test Input Generation;User experience},
 pages = {160--165-160--165}
}


@inproceedings{Parnin.,
 abstract = {A race is underway to embed advanced AI capabil-ities into products. These product ``copilots'' enable users to ask questions in natural language and receive relevant responses that are specific to the user's context. In fact, virtually every large technology company is looking to add these capabilities to their software products. However, for most software engineers, this is often their first encounter with integrating AI-powered technol-ogy. Furthermore, software engineering processes and tools have not caught up with the challenges and scale involved with building AI-powered applications. In this work, we present the findings of an interview study with 26 professional software engineers responsible for building product copilots at various companies. From our interviews, we found pain points at every step of the engineering process and the challenges that strained existing development practices. We then conducted group brainstorming sessions to collaborative on opportunities and tool designs for the broader software engineering community.},
 author = {Parnin, Chris and Soares, Gustavo and Pandita, Rahul and Gulwani, Sumit and Rich, Jessica and Henley, Austin Z.},
 title = {Building Your Own Product Copilot: Challenges, Opportunities, and Needs},
 keywords = {AI;Artificial intelligence;Best practices;Buildings;Companies;intelligent applications;Interviews;large-language models;Pain;pain points;Software;Software development management;Software engineering;Testing},
 pages = {338--348},
 doi = {10.1109/SANER64311.2025.00039}
}


@inproceedings{Richards.,
 abstract = {As Large Language Models (LLMs) are increasingly adopted in software engineering, recently in the form of conversational assistants, ensuring these technologies align with developers' needs is essential. The limitations of traditional human-centered methods for evaluating LLM-based tools at scale raise the need for automatic evaluation. In this paper, we advocate combining insights from human-computer interaction (HCI) and artificial intelligence (AI) research to enable human-centered automatic evaluation of LLM-based conversational SE assistants. We identify requirements for such evaluation and challenges down the road, working towards a framework that ensures these assistants are designed and deployed in line with user needs.},
 author = {Richards, Jonan and Wessel, Mairieli},
 title = {Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants},
 keywords = {Conferences;conversational agent;evaluation;HCI;Human computer interaction;Large language models;LLM;Roads;SE;Software engineering},
 pages = {6--10},
 doi = {10.1109/BotSE67031.2025.00009}
}


@inproceedings{Sahoo.,
 abstract = {The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been re-leased, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66{\%} on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50{\%} of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08{\%} for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.},
 author = {Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Gebhardt, Richard and Mandel, Louis and Buratti, Luca},
 title = {Ansible Lightspeed: A Code Generation Service for IT Automation},
 keywords = {ansible;Automation;code completion;Codes;Domain specific languages;generative models;ide;Large language models;Machine Learning;Natural languages;Productivity;Sentiment analysis;Software;Software engineering;User Study},
 pages = {2148--2158}
}


@inproceedings{SaiKumar.,
 abstract = {Quick changes in phishing attempts make sophisticated detection techniques absolutely vital. This work presents a phishing detection system aiming at increasing accuracy and adaptability by combining machine learning with artificial intelligence-driven analysis. Along with Google's Gemini 2.0 Flash LLM for more in-depth contextual analysis, it makes use of a Gradient Boosting Classifier (GBC) taught on 30 URL characteristics. SMote for class balance and tuned hyperparameters help to improve the model by means of fine-tuning of the detection performance. Two parts characterize the development process: the first phase concentrates on establishing the ML model, which comprises URL categorization, shortlink identification, and phishing pattern analysis; the second phase employs JSP and NetBeans to build a Java-based web application. Three portals comprise the system: an administrative portal for user management, phishing trend analysis and URL classification; a user portal for secure web browsing, suggestions, and automatic phishing alerts; and an attacker simulation site to test phishing detection capability. Python is connected with NetBeans to provide real-time classification and security analysis; an automatic access control method warns or temporarily limits users who surpass limits on browsing harmful URLs. This research presents a very accurate phishing detection system with real-time alarms and enhanced protection against changing threats by combining machine learning with AI-driven analysis.},
 author = {{Sai Kumar}, Pusarla Venkata and Yadav, Mende Nikhil and Reddy, Eppa Chaithanya and Sathya, V.},
 title = {Smart-Shield: A Hybrid ML-AI Framework for Advanced Phishing Detection using Gradient Boosting and LLM Analysis},
 keywords = {Accuracy;Artificial intelligence;Boosting;Google's Gemini LLM;Gradient Boosting Classifier;Java;Java Server Pages;Machine Learning;NetBeans;Phishing;Phishing Detection;Portals;Protection;Real-time systems;Servers;Social networking (online);Uniform resource locators;URL Classification},
 pages = {1467--1473},
 doi = {10.1109/ICPCSN65854.2025.11035754}
}


@inproceedings{Sarda.,
 abstract = {Ensuring the reliability and availability of cloud services relies heavily on efficient root cause analysis (RCA) for cloud incidents. Traditionally, RCA involved labor-intensive manual investigations across various data sources, such as logs, metrics, and traces. Despite the increasing adoption of AI-driven assistants for RCA, their effectiveness for Site Reliability Engineers (SREs) is often hindered by low accuracy due to the inherent complexity of the task. This study introduces an on-call system powered by a large language model (LLM) designed to automate RCA processes for cloud incidents in practical, privacy-aware industrial settings. Our approach integrates incoming multimodal datasets (Logs, Metrics, Traces, Alerts-LMTA), aggregates critical runtime diagnostic information, including developer set alerts with precise offset values and thresholds, and utilizes LMTA data to predict the root cause category of incidents. We evaluated the effectiveness of our approach using two open-source, real-world-like datasets, demonstrating that the proposed LLM based approach can achieve an RCA accuracy of up to 97{\%}.},
 author = {Sarda, Komal and Namrud, Zakeya and Watts, Ian and Shwartz, Larisa and Nagar, Seema and Mohapatra, Prateeti and Litoiu, Marin},
 title = {Augmenting Automatic Root-Cause Identification with Incident Alerts Using LLM},
 keywords = {Accuracy;Cloud native applications;Complexity theory;Computational modeling;Incident management;Large language models;Manuals;Measurement;Reliability engineering;Root cause analysis;Runtime;Soft sensors},
 pages = {1--10},
 doi = {10.1109/CASCON62161.2024.10838171}
}


@article{Sasaki.2025,
 abstract = {Advancements in large language models (LLMs) have enhanced their ability to handle ambiguous user instructions. However, effective prompt patterns remain crucial for usability and comprehension. This article presents a taxonomy of prompt engineering patterns for software engineering. It is based on a systematic literature review that was conducted in early 2023, when LLMs still faced significant limitations in context length and inference capabilities. Our study explores techniques that enhance the usability and reliability of LLMs, emphasizing the ongoing importance of well-designed prompts in optimizing task performance. Our findings highlight the critical role of prompt patterns in maximizing LLM's potential, even as their capabilities continue to evolve.},
 author = {Sasaki, Yuya and Washizaki, Hironori and Li, Jialong and Yoshioka, Nobukazu and Ubayashi, Naoyasu and Fukazawa, Yoshiaki},
 year = {2025},
 title = {Landscape and Taxonomy of Prompt Engineering Patterns in Software Engineering},
 keywords = {Large language models;Performance evaluation;Prompt engineering;Quality assessment;Software engineering;Taxonomy;Usability;User experience;User-generated content},
 pages = {41--49},
 volume = {27},
 number = {1},
 issn = {1941-045X},
 journal = {IT Professional},
 doi = {10.1109/MITP.2024.3525458}
}


@inproceedings{Senthamarai.,
 abstract = {Introduces a groundbreaking IAC Code Generator for automated Terraform script creation. Empowers developers by treating infrastructure as a versioned, programmable artifact. Enhances efficiency, reduces time-to-market, ensures consistency, and promotes collaboration between development and operations teams. Features an intuitive interface, customizable templates, and integrates industry best practices for accessible and accelerated development cycles.},
 author = {Senthamarai, N. and Jeyaselvi, M. and Hemamalini, V.},
 title = {Automatic Cloud Formation Using LLM},
 keywords = {Automation;Best practices;Cloud computing;Codes;Collaboration;Generation;Generators;IAC;Industries;Manuals;Software development management;Software reliability;Template;Terraform},
 pages = {1--6},
 doi = {10.1109/ICoICC64033.2025.11052114}
}


@inproceedings{Sergeyuk.,
 abstract = {To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it's often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.},
 author = {Sergeyuk, A. and Lvova, O. and Titov, S. and Serova, A. and Bagirov, F. and Kirillova, E. and Bryksin, T.},
 title = {Reassessing Java Code Readability Models with a Human-Centered Approach},
 keywords = {AI-Generated Code;Code Readability;Code Readability Models;Codes;Correlation;Data models;Human-Computer Interaction;Java;Predictive models;Productivity;Repertory Grid Technique;Surveys},
 pages = {225--235-225--235}
}


@inproceedings{Shahariar.,
 abstract = {We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92{\%} accuracy while the SOTA baseline achieved 81.5{\%}. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.},
 author = {Shahariar, G. M. and Hasan, T. and Iqbal, A. and Uddin, G.},
 title = {Contrastive Learning for API Aspect Analysis},
 keywords = {API aspects;API review;Aspect detection;Contrastive learning;Documentation;LIME;Linear programming;Performance analysis;Security;Semantics;Training;Transformers},
 pages = {637--648-637--648},
 doi = {10.1109/ASE56229.2023.00064}
}


@inproceedings{Rogozinnikov.,
 abstract = {In the modern world, digitalization is increasingly penetrating into people's daily lives and covers a wide variety of areas of human activity. Electric power industry is no exception. The introduction of SCADA systems has increased the observability of processes occurring in the network and increased the efficiency and speed of decision-making aimed at maintaining a stable operating mode of the EPS. The next step in the development of digital electric power industry is digital twins, which could be integrated into digital services for their visual display or, for example, for conducting processes of modeling the operating mode of the EPS. The introduction of artificial intelligence (AI) is also gaining popularity. More and more advanced AI models are constantly emerging, capable of solving increasingly complex problems. The introduction of generative artificial intelligence can significantly speed up the process of building digital models. This article provides a comparative review of various generative AI models that are further trained to generate descriptions of electric power substations in a strictly formalized form that can be further computer processed.},
 author = {Rogozinnikov, E. I. and Doskalesku, K. V. and Ivanov, A. A. and Marinov, Y. A. and Morozov, S. M. and Yakunin, A. V.},
 title = {Training Generative Models for the Task of Object Description in A Strict Format: Analysis of the Efficiency of Their Architectures},
 keywords = {AI Model Training;Analytical models;Computational modeling;Computer architecture;Digital Models;Generative AI;GPT;Industries;LLM;Recurrent Neural Networks;Software packages;Substations;Training;Transformers;Visualization},
 pages = {1--15},
 doi = {10.1109/RPA65165.2024.10932905}
}


@article{Shang.2024,
 abstract = {Artificial Intelligence-Generated Content (AIGC) technology has revolutionized content creation, distribution, and engagement in the consumer electronics sector, propelling its applications to unprecedented heights. Within this landscape, AIGC-driven conversational agents, exemplified by renowned chatbots like ChatGPT, have gained widespread popularity globally. These advanced conversational agents are instrumental in significantly enhancing user efficiency and overall experience within consumer electronics applications. However, with the increasing integration of vision-language models (VLMs, a representative of AIGC) in consumer electronics, the vulnerability of chatbots to adversarial attacks has become a critical concern. This paper investigates and analyzes the susceptibility of VLMs-empowered chatbots to adversarial manipulation, particularly in the context of consumer electronics applications. The study employs a comprehensive approach, combining vision and language modalities, to explore potential attack vectors and vulnerabilities. Specifically, we designed three adversarial attacks, which all exploited the insufficient alignment of VLMs on multi-modal data to implement effective attacks and make chatbots output harmful content. A series of experiments demonstrate the efficacy of adversarial attacks on three popular chatbot systems, revealing vulnerabilities that may compromise the reliability and security of these systems in real-world scenarios. The findings emphasize the importance of robust defenses against adversarial attacks in VLMs-driven chatbots, urging the development of enhanced security measures to safeguard users and prevent malicious exploitation of consumer electronics. Our data and code are available at https://github.com/yxc0731/VLM-Adversarial-Attacks.},
 author = {Shang, Y. and Liu, Z. and Kang, J. and Hossain, M. S. and Wu, Y.},
 year = {2024},
 title = {Adversarial Attacks on Vision-Language Model-Empowered Chatbots in Consumer Electronics},
 keywords = {adversarial attacks;AIGC;Artificial intelligence;Chatbots;Consumer electronics;Data models;Media;multimodal attacks;Security;vision-language models;Visualization},
 pages = {6075--6083-6075--6083},
 volume = {70},
 number = {3},
 journal = {IEEE Transactions on Consumer Electronics},
 doi = {10.1109/TCE.2024.3417688}
}


@inproceedings{Panchal.,
 abstract = {ONAP is a comprehensive platform for orchestration, management and automation of network and edge computing services for 5G, 6G and Next Generation Networks. Unlike traditional OSSs, it is an open-source project where companies all over the world are collaborating to build different functionalities of an end-to-end Network operating system. For this reason, the ONAP platform has several different sub projects and APIs each performing a specific function to achieve Network Management. There is some complexity associated with using these APIs and knowing and understanding the many parameters associated with them, which impedes adoption. This not only prevents an end- to-end cloud service orchestration like experience for network services, but also increases the time and money spent on network orchestration. This paper proposes and discusses the design of a conversational AI solution that can interface with some significant APIs in ONAP to solve these problems. The conversational AI solution has the potential to significantly simplify network orchestration tasks. This work is being further extended to using Large Language Models (LLMs) to achieve simplified Intent-Based management and orchestration paradigms within ONAP.},
 author = {Panchal, D. and Verma, P. and Baran, I. and Musgrove, D. and Lu, D.},
 title = {Simplifying Network Orchestration using Conversational AI},
 keywords = {5G;6G;Automation;Companies;Complexity theory;Intent Driven Networking;Intent recognition;Intent-Based Networking;Large language models (LLMs);Machine Learning;Natural language processing;Network Function Virtualization;Network Orchestration;Next Generation Networks;Open Network Automation Platform (ONAP);Open Source;Operations support systems (OSS);Oral communication;Software Defined Net-working;Task analysis;User experience},
 pages = {84--89-84--89},
 doi = {10.1109/ICOIN59985.2024.10572160}
}


@inproceedings{Ott.,
 abstract = {The rapid advancement of generative artificial in-telligence (AI) has led to groundbreaking developments in large language models. As large language models generate textual sequences autoregressively, mitigating latency becomes imper-ative for providing a highly immersive interaction experience within a realtime conversation, for example, providing fast and accurate responses to users' questions. Current efforts focus on accelerating inference processes, yet often at the expense of model architecture alterations, leading to compromised quality. In this paper, we explore latency reduction in the case of speech-based conversational agents. We leverage mathematical functions based on Beam Search to analyze autoregressive textual sequences, enabling a nuanced evaluation of semantic quality during auditory interaction, for example, for use within interactive web podcasts. We implemented our concepts and used the software to evaluate the concepts within (1) an automated evaluation of 1000 question-answer pairs and (2) a user survey. The results show that the semantic quality of autoregressive textual sequences could be assessed successfully by our proposed mathematical terms.},
 author = {Ott, N. and Horst, R. and D{\"o}rner, R.},
 title = {Towards Reducing Latency Using Beam Search in an Interactive Conversational Speech Agent},
 keywords = {Beam Search;Computer architecture;Conversational Artificial Intelligence;Entertainment industry;Human-Computer Interaction;Interactive Podcasts;Large language models;Latency Reduction;Media;Oral communication;Semantics;Surveys},
 pages = {1--6-1--6},
 doi = {10.1109/GEM61861.2024.10585772}
}


@inproceedings{Mahto.,
 abstract = {The rapid evolution of web applications has made personalization and intuitive user experiences increasingly essential. The aim of this work is to provide a tailored large language model (LLM) architecture allowing context-aware interactions within online contexts. Using real-world datasets like as Microsoft COCO and Amazon Customer Reviews enhanced with contextual metadata and behavioural analytics, we taught LLMs to grasp customer wants via explicit patterns and contextual signals. To allow the models to dynamically modify their replies, improve user engagement, and expedite processes, sophisticated natural language processing methods and contextual embedding strategies were used. In comparison to conventional static models, extensive experimentation and user testing exposed notable increases in user happiness and interaction efficiency. Significant problems like data privacy, model scalability, and real-time processing were resolved to guarantee the feasibility of these bespoke LLMs throughout several web application environments.},
 author = {Mahto, Manoj Kumar and Srivastava, Durgesh and Kumar, Ranjit and Sah, Basant and Singh, Hare Ram and Maakar, Sunil Kr.},
 title = {Personalized User Interaction in Web Applications using Adaptive LLM Model},
 keywords = {Adaptation models;Analytical models;Behavioural Analytics;Computational modeling;Context modeling;Context-Aware Language Models;Contextual Embedding Strategies;Data privacy;Large language models;LLM Model;Natural language processing;NLP;Reviews;Scalability;Solid modeling;User Interaction Modeling;Web Application Personalization},
 pages = {962--966},
 doi = {10.1109/ICPCT64145.2025.10940477}
}


@inproceedings{Mailach.,
 abstract = {Background: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications. Method: We collected 189 videos from 2022 to 2024 by practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes. Results: The most prevalent topics fall within the theme Design {\&} Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., finetuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges. Implications: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further highlight topics of interest for academics where further research is needed.},
 author = {Mailach, Alina and Simon, Sebastian and Dorn, Johannes and Siegmund, Norbert},
 title = {Themes of Building LLM-Based Applications for Production: A Practitioner's View},
 keywords = {agents;Computer architecture;evaluation;fine-tuning;Large language models;LLM in production;Manuals;Production;Prompt engineering;RAG;Retrieval augmented generation;retrieval-augmented generation;Software engineering;Systematics;Tuning;Videos},
 pages = {18--30},
 doi = {10.1109/CAIN66642.2025.00011}
}


@inproceedings{Mao.,
 abstract = {Symbolic protocol analysis serves as a pivotal technique for protocol design, security analysis, and the safeguarding of information assets. Several modern tools such as Tamarin and ProVerif have been proven successful in modeling and verifying real-world protocols, including complex protocols like TLS 1.3 and 5G AKA. However, developing formal models for protocol verification is a non-trivial task, which hinders the wide adoption of these powerful tools in practical protocol analysis. In this work, we aim to bridge the gap by developing an automatic method for generating symbolic protocol models using Large Language Models (LLMs) from protocol descriptions in natural language document. Although LLMs are powerful in various code generation tasks, it is shown to be ineffective in generating symbolic models (according to our empirical study). Therefore, rather than applying LLMs naively, we carefully decompose the symbolic protocol modeling task into several stages so that a series of formal models are incrementally developed towards generating the final correct symbolic model. Specifically, we apply LLMs for semantic parsing, enable lightweight manual interaction for disambiguation, and develop algorithms to transform the intermediate models for final symbolic model generation. To ensure the correctness of the generated symbolic model, each stage is designed based on a formal execution model and the model transformations are proven sound. To the best of our knowledge, this is the first work aiming to generate symbolic models for protocol verification from natural language documents. We also introduce a benchmark for symbolic protocol model generation, with 18 real-world security protocol's text description and their corresponding symbolic models. We then demonstrate the potential of our tool, which successfully generated correct models of moderate scale in 10 out of 18 cases. Our tool is released at [1].},
 author = {Mao, Ziyu and Wang, Jingyi and Sun, Jun and Qin, Shengchao and Xiong, Jiawen},
 title = {LLM-Aided Automatic Modeling for Security Protocol Verification},
 keywords = {Analytical models;Automatic modeling;Codes;Large language models;LLMs;Manuals;Natural languages;Protocols;Security;Semantics;Software engineering;Symbolic analysis;Transforms},
 pages = {642--654},
 doi = {10.1109/ICSE55347.2025.00197}
}


@inproceedings{Marini.,
 abstract = {In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product. Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes. However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71 {\%} in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time - up to 60 {\%} - spent investigating issues.},
 author = {Marini, L. and Gissl{\'e}n, L. and Sestini, A.},
 title = {Leveraging Large Language Models for Efficient Failure Analysis in Game Development},
 keywords = {Accuracy;Codes;Computer bugs;Failure analysis;Games;Large language models;Natural language processing;Natural languages;Software development;Software quality;Tracing;Validation},
 pages = {1--8-1--8},
 doi = {10.1109/CoG60054.2024.10645540}
}


@inproceedings{Meiner.,
 abstract = {Lecturers are increasingly attempting to use large language models (LLMs) to simplify and make the creation of exercises for students more efficient. Efforts are also being made to automate the exercise creation process in software engineering (SE) education. This study explores the use of advanced LLMs, including GPT-4 and LaMDA, for automated programming exercise creation in higher education and compares the results with related work using GPT-3.5-turbo. Utilizing applications such as ChatGPT, Bing AI Chat, and Google Bard, we identify LLMs capable of initiating different exercise designs. However, manual refinement is crucial for accuracy. Common error patterns across LLMs highlight challenges in complex programming concepts, while specific strengths in various topics showcase model distinctions. This research underscores LLMs' value in exercise generation, emphasizing the critical role of human supervision in refining these processes. Our concise insights cater to educators, practitioners, and other researchers seeking to enhance SE education through LLM applications.},
 author = {Mei{\ss}ner, N. and Speth, S. and Becker, S.},
 title = {Automated Programming Exercise Generation in the Era of Large Language Models},
 keywords = {AI-Generated Exercises;Chatbots;Engineering education;Graphical user interfaces;Internet;Large language models;Manuals;Programming Exercises;Programming profession;Refining;Software engineering;Software Engineering Education;Usability},
 pages = {1--5-1--5},
 doi = {10.1109/CSEET62301.2024.10662984}
}


@inproceedings{Meng.,
 abstract = {Mobile app-based learning is considered a futureoriented educational paradigm and an indispensable mode of learning in the future. Objective and effective evaluation of English learning apps holds significant importance in enhancing teaching effectiveness. The main work of this paper includes the following content. It elucidates the understanding of natural language processing technology, outlines the principles for constructing evaluation criteria for English learning apps, and extensively explores both rational and emotional aspects of evaluation criteria for English learning apps. Finally, it analyzes the developmental trends of natural language processing technology in the comprehensive evaluation of English learning apps. The primary aim of this work is to promote education in a more rational direction by offering a comprehensive set of evaluation criteria for English learning apps, which can be utilized by third-party assessment agencies or individuals. This facilitates a more efficient selection of suitable learning software for end-users and provides valuable recommendations for software developers to enhance their products.},
 author = {Meng, F. and Qi, M. and Bao, S.},
 title = {Comprehensive Evaluation of English Learning Apps Based on Natural Language Processing},
 keywords = {Communication networks;Comprehensive Evaluation;Computational intelligence;Education;English Learning Apps;Market research;Natural language processing;Online Classroom;Software},
 pages = {1--5-1--5},
 doi = {10.1109/CICN59264.2023.10402171}
}


@inproceedings{Miah.,
 abstract = {With the rapid advance of machine learning (ML) technology, large language models (LLMs) are increasingly explored as an intelligent tool to generate program code from natural language specifications. However, existing evaluations of LLMs have focused on their capabilities in comparison with humans. It is desirable to evaluate their usability when deciding on whether to use a LLM in software production. This paper proposes a user centric method for this purpose. It includes metadata in the test cases of a benchmark to describe their usages, conducts testing in a multi-attempt process that mimics the uses of LLMs, measures LLM generated solutions on a set of quality attributes that reflect usability, and evaluates the performance based on user experiences in the uses of LLMs as a tool. The paper also reports a case study with the method in the evaluation of ChatGPT's usability as a code generation tool for the R programming language. Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks. The user experiences are good with overall average number of attempts being 1.61 and the average time of completion being 47.02 seconds. Our experiments also found that the weakest aspect of usability is conciseness, which has a score of 3.80 out of 5.},
 author = {Miah, T. and Zhu, H.},
 title = {User Centric Evaluation of Code Generation Tools (Invited Paper)},
 keywords = {Benchmark testing;Chatbots;ChatGPT;Code Generation;Codes;Computer languages;Large language models;Machine Learning;Metadata;MIMICs;Natural languages;Performance evaluation;R programming language;Usability},
 pages = {109--119-109--119},
 doi = {10.1109/AITest62860.2024.00022}
}


@inproceedings{Ouaazki.,
 abstract = {Given the pervasive reliance on technology in modern society, teaching Computational Thinking (CT) abilities is becoming increasingly relevant. These abilities, such as modeling and coding, have become crucial for a larger audience of students, not only those who wish to become software engineers or computer scientists. Recent advances in Large Language Models (LLMs), such as ChatGPT, provide powerful assistance to complete computational tasks, by simplifying code generation and debugging, and potentially enhancing interactive learning. However, it is not clear if these advances make CT tasks more accessible and inclusive for all students, or if they further contribute to a digital skills divide, favoring the top students. To address this gap, we have created and evaluated a novel learning scenario for transversal CT skills that leveraged LLMs as assistants. We conducted an exploratory field study during the spring semester of 2022, to assess the effectiveness and user experience of LLM-augmented learning. Our results indicate that the usage of ChatGPT as a learning assistant improves learning outcomes. Furthermore, contrary to our predictions, the usage of ChatGPT by students does not depend on prior CT capabilities and as such does not seem to exacerbate prior inequalities.},
 author = {Ouaazki, A. and Bergram, K. and Holzer, A.},
 title = {Leveraging ChatGPT to Enhance Computational Thinking Learning Experiences},
 keywords = {Chatbots;Collaborative Learning;Computational modeling;Education;Human-Computer Interaction;Interactive Learning Environments;Large language models;Software;Springs;Task analysis;User experience},
 pages = {1--7-1--7},
 doi = {10.1109/TALE56641.2023.10398358}
}


@book{Miller.2024,
 abstract = {Create engaging AI experiences by mastering ChatGPT for business and leveraging user interface design practices, research methods, prompt engineering, the feeding lifecycle, and moreKey FeaturesLearn in-demand design thinking and user research techniques applicable to all conversational AI platformsMeasure the quality and evaluate ChatGPT from a customer's perspective for optimal user experienceSet up and use your secure private data, documents, and materials to enhance your ChatGPT modelsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany enterprises grapple with new technology, often hopping on the bandwagon only to abandon it when challenges emerge. This book is your guide to seamlessly integrating ChatGPT into enterprise solutions with a UX-centered approach. UX for Enterprise ChatGPT Solutions empowers you to master effective use case design and adapt UX guidelines through an engaging learning experience. Discover how to prepare your content for success by tailoring interactions to match your audience's voice, style, and tone using prompt-engineering and fine-tuning. For UX professionals, this book is the key to anchoring your expertise in this evolving field. Writers, researchers, product managers, and linguists will learn to make insightful design decisions. You'll explore use cases like ChatGPT-powered chat and recommendation engines, while uncovering the AI magic behind the scenes. The book introduces a and feeding model, enabling you to leverage feedback and monitoring to iterate and refine any Large Language Model solution. Packed with hundreds of tips and tricks, this guide will help you build a continuous improvement cycle suited for AI solutions. By the end, you'll know how to craft powerful, accurate, responsive, and brand-consistent generative AI experiences, revolutionizing your organization's use of ChatGPT.What you will learnAlign with user needs by applying design thinking to tailor ChatGPT to meet customer expectationsHarness user research to enhance chatbots and recommendation enginesTrack quality metrics and learn methods to evaluate and monitor ChatGPT's quality and usabilityEstablish and maintain a uniform style and tone with prompt engineering and fine-tuningApply proven heuristics by monitoring and assessing the UX for conversational experiences with trusted methodsRefine continuously by implementing an ongoing process for chatbot and feedingWho this book is forThis book is for user experience designers, product managers, and product owners of business and enterprise ChatGPT solutions who are interested in learning how to design and implement ChatGPT-4 solutions for enterprise needs. You should have a basic-to-intermediate level of understanding in UI/UX design concepts and fundamental knowledge of ChatGPT-4 and its capabilities.},
 author = {Miller, Richard H. and Johnson, Jeff},
 year = {2024},
 url = {https://ieeexplore.ieee.org/document/10769348},
 publisher = {{Packt Publishing}}
}


@inproceedings{Mougouei.,
 abstract = {Recent criticism of social media platforms by the U.S. Senate Judiciary Committee for neglecting child safety exemplifies how software can undermine human values. This is further complicated by the growing integration of Artificial Intelligence (AI) in software, which introduces inherent challenges such as biases and limited transparency. However, AI also presents opportunities to embed human values into software. To explore these opportunities, we have utilized the reasoning abilities of ChatGPT, a large language model (LLM), in combination with human expertise, to study the use of AI in publications that address human values, across some of the leading software engineering (SE) venues from 2022 to 2023. Our findings confirm the use of AI concepts - mainly General Machine Learning - in around 33 {\%} of these valuealigned publications. The value alignments largely concern pragmatic aspects of Achievement and (personal) Security, while the majority of the values receive less attention. The socially focused values of Conformity and Tradition and the personally focused value of Hedonism are rarely addressed in the SE publications.},
 author = {Mougouei, Davoud and Azarnik, Ahmad and Fahmideh, Mahdi and Mougouei, Elahe and Dam, Hoa Khanh and Khan, Arif Ali and Rafi, Saima and Khan, Javed Ali and Ahmad, Aakash},
 title = {A First Look at AI Trends in Value-Aligned Software Engineering Publications: Human-LLM Insights},
 keywords = {Artificial intelligence;Artificial Intelligence (AI);Chatbots;ChatGPT;Human Values;Large language models;Large language models (LLMs);Market research;Pragmatics;Safety;Security;Social networking (online);Software;Software engineering;Value-Aligned Publications},
 pages = {82--93},
 doi = {10.1109/ICSE-SEIS66351.2025.00014}
}


@inproceedings{Munjal.,
 abstract = {The blind and visually impaired people must face significant challenges while navigating their environment independently. This paper presents ``BlindViz'', a mobile application specially designed to aid the visually impaired by providing real-time object detection and scene description using the latest advancements in the AI technology. This app integrates Large Language Models (LLMs) and leverages You Only Look Once (YOLOv8) for object detection, Easy Optical Character Recognition (EasyOCR) for Text Recognition and leveraging Image Segmentation for natural language scene descriptions. As the word ``Activate'' is spoken into the app, it takes an image, processes it, and delivers a description through an audible output. This paper outlines the development, implementation, and evaluation of BlindViz, along with the suggestions for future enhancements to improve its usability and effectiveness.},
 author = {Munjal, Geetika and Dious, Austin and Agarwal, Devansh and Sharma, Rahul and Jarawat, Gunjan},
 title = {Blindviz --- Intelligent Navigation and Environmental Awareness for the Visually Impaired},
 keywords = {Accuracy;Blind;BlindViz;Image processing;Image segmentation;Large language models;LLMs;Natural language processing;Navigation;Object detection;OpenAI API;Real-time systems;Speech to text;Text Recognition;Usability;User experience;Visual Assistance;Visualization;Visually Impaired;YOLO;YOLOv8},
 pages = {492--496},
 doi = {10.1109/ICDT63985.2025.10986575}
}


@inproceedings{Nabasirye.,
 abstract = {In the evolving landscape of global healthcare, the need for efficient and scalable data analysis has never been more critical. This research explores the integration of Large Language Models (LLMs) and Natural Language Processing (NLP) into the District Health Information System 2 (DHIS2), a platform widely used for health data management in Uganda. The objective of this study is to enhance the usability and data analysis capabilities of DHIS2, making it accessible to both technical and non-technical users. By leveraging AI-driven models like LangChain and Ollama, the research developed an automated data analysis system that allows users to interact with DHIS2 using natural language queries in English. This integration simplifies the extraction of meaningful insights from complex datasets, thereby improving decision-making processes. The system was evaluated for usability, performance, and its potential to enhance public health data analysis. Results demonstrate that the AI-augmented DHIS2 platform significantly reduces analysis time, improves data accessibility, and facilitates timely public health interventions. This innovation presents a scalable solution for improving health information systems globally, particularly in resource-constrained environments.},
 author = {Nabasirye, Angella and Ssali, Irene Wanyana},
 title = {Integrating Natural Language Processing and Large Language Models Into DHIS2 to Improve Health Data Utilization},
 keywords = {Artificial intelligence;Data analysis;Data models;Decision making;DHIS2;Healthcare Informatics;Informatics;Large language models;Natural language processing;NLP;Public healthcare;Software engineering;Technological innovation;Uganda;Usability},
 pages = {47--52},
 doi = {10.1109/SEiGS66664.2025.00012}
}


@inproceedings{Nie.,
 abstract = {The traditional invoice issuance process within tax administration is labor-intensive and prone to errors, necessitating a shift towards digitalization. Despite the advent of digital invoicing systems that streamline invoice generation and automate rule-based audits, integration with existing financial accounting systems remains a challenge. Particularly in the hospitality and bookkeeping sectors, the adoption of these systems is hindered by the lack of standardized software, high costs, and the absence of technical expertise among small and micro enterprises. The integration of digital invoicing systems with diverse financial software presents significant barriers to uniform adaptation. Furthermore, the complexity of tax regulations and the dynamic nature of tax categories require advanced understanding beyond the capabilities of standard Large Language Models (LLMs). The need for a specialized system that can comprehend finance and tax contexts, securely handle sensitive information, and adapt to user interactions is paramount. This paper introduces an autonomous agent based on a finance and tax-specific Large Language Model (LLM) designed to address the aforementioned challenges. The system includes a Specialized Training Framework to enhance domain comprehension, a Hierarchical Memory Architecture for dynamic user interaction, and a Tax Domain Security Module to ensure compliance with tax regulations. The proposed agent aims to improve the efficiency and accuracy of the invoice issuance process, providing a robust solution for tax administration in the digital era.},
 author = {Nie, Runze and Wu, Hao and Ma, Lan and Liu, Zhenyu and Wang, Zhigang and Zhang, Ping},
 title = {Towards a Conversational Invoice Issuance LLM-Based Agent},
 keywords = {Aerodynamics;Autonomous agents;Collaboration;E-Invoice System;Finance;Large language models;Memory architecture;Natural language processing;Regulation;Security;Software;Training},
 pages = {1--5},
 doi = {10.1109/ACAI63924.2024.10899737}
}


@inproceedings{Nikeghbal.,
 abstract = {Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model.In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.CCS CONCEPTS• Software and its engineering $\rightarrow$ Software notations and tools; Software creation and management},
 author = {Nikeghbal, N. and Kargaran, A. H. and Heydarnoori, A.},
 title = {GIRT-Model: Automated Generation of Issue Report Templates},
 keywords = {Bug Report;Bug Template;Codes;Data mining;GitHub;Issue Report Template;Issue Template;Issue Template Generation;Issue Tracker;Meteors;Software;Software development management},
 pages = {407--418-407--418}
}


@inproceedings{Oliveira.,
 abstract = {Existing studies on the use of Large Language Models (LLMs) in software development leverage methodologies that limit their scalability and require intensive manual data collection and analysis, for example, due to the use of video data or think-aloud protocols. We propose the use of a specialized tool capable of automatically collecting fine-grained, relevant data during experiments and case studies. It enables researchers to understand for example how often participants accept or reject suggestions made by LLMs and what kinds of prompts are more likely to trigger accepted suggestions, even in studies targeting a large number of participants. We implement this idea as a Visual Studio Code plugin named AthenaLLM1. It mimics the functionalities of GitHub Copilot and offers seamless integration with OpenAI API models like GPT-4 and GPT-3.5, and compatibility with other models providing an OpenAI-compatible API, e.g., Vicuna [6]. It automatically collects data at a fine level of granularity and covers both the interactions of developers with their IDE, e.g., all changes made in the code, and the products of such interactions, e.g., the generated code, when accepted. Thus, the proposed approach also reduces bias that the experimental process itself may introduce, e.g., due to the need for participants to verbalize their thoughts. In this paper we discuss how AthenaLLM could enable researchers to go both broader (in terms of number of participants) and deeper (in terms of the kinds of research questions that can be tackled).1Extension available at https://github.com/nandooliveira/athena{\_}llm{\_}extension},
 author = {de Oliveira, B. and Castor, F.},
 title = {AthenaLLM: Supporting Experiments with Large Language Models in Software Development},
 keywords = {Adaptation models;Codes;Empirical Software Engineering;Experimentation;Large language models;Manuals;MIMICs;Protocols;Scalability;Software development;Visualization},
 pages = {69--73-69--73}
}


@inproceedings{Olmez.,
 abstract = {In addressing the need for test case generation in software projects and the validation and repair processes, various algorithms and AI models are increasingly being applied with novel approaches. On the other hand, despite the established effectiveness of the Test-Driven Development (TDD) approach in testing and development, there is still a lack of research examining the impact of human-machine interaction on software validation and coding. This paper introduces a tool, the test-skeleton generator, which utilizes an OpenAI model to generate test skeletons. These skele-tons include test names, signatures, and scenario descriptions, omitting the actual test bodies. To explore the implications of this tool, an empirical experiment involving student participation was conducted to assess the conversion of test skeletons into functional tests with human-machine interaction. The study reveals significant insights, indicating that human-machine interaction plays a crucial role in shaping both the testing and programming phases, encouraging students to prioritize writing tests before modifying source code. Teams adopting this approach demonstrate a tendency to produce more tests, leading to higher code coverage. Additionally, our research underscores the growing potential of AI language models to generate tests that closely resemble those written by human developers. Notably, human-machine interaction has proven its significant positive impact on the validation and repair process of AI -generated tests.},
 author = {Olmez, M. M. and Gehringer, E.},
 title = {Automation of Test Skeletons Within Test-Driven Development Projects},
 keywords = {AI tool;Human computer interaction;Human-machine interaction;Human-machine systems;Maintenance engineering;programming;Software algorithms;Source coding;Test Generation;Test skeletons;Test-driven development;Writing},
 pages = {1--10-1--10},
 doi = {10.1109/CSEET62301.2024.10663016}
}


@inproceedings{Mishra.,
 abstract = {The integration of Artificial Intelligence (AI) into educational technologies marks a significant shift in learning methodologies and operational dynamics within educational institutions. At the forefront is an AI-driven virtual mock interview platform designed to address the high Customer Acquisition Costs (CAC) in the edtech sector, especially for interview preparation services. This initiative harnesses a blend of AI technologies, including ADA 2 for creating context-aware embeddings and Machine Learning (ML), to transform the traditional mock interview process into a dynamic, cost-effective system. Central to the platform is its use of advanced Natural Language Processing (NLP) techniques and GPT-4 Large Language Model (LLM), automating the process of mock interviews and providing personalized feedback, ensuring a preparation journey that meets specific candidate needs and mirrors real interview scenarios. A key evaluation among 100 students from a cohort of 1800 demonstrated a 90{\%} cost reduction for three mock interviews, reducing expenses from ₹3000 to just ₹300 per candidate. This cost efficiency significantly enhances access to quality interview preparation, improving student satisfaction and accessibility. Moreover, the platform provides valuable insights into student performance, setting a new standard in educational technology by offering an effective, personalized interview preparation experience. This project reflects a holistic approach to student development and the critical role of technology in addressing the evolving needs of learners},
 author = {Mishra, Prabhat Kumar and Arulappan, Arun Kumar and Ra, In-Ho and {Thanga Mariappan}, L. and {Gina Rose}, G. and Lee, Young-Seok},
 title = {AI-Driven Virtual Mock Interview Development},
 keywords = {ADA 2 Embeddings;Advanced Artificial Intelligence;CAC;Costs;Educational technology;Interviews;Large language models;Learning (artificial intelligence);Machine Learning;Machine Learning Innovations;Mirrors;Mock Interview;Natural language processing;Python Programming;Sentiment analysis;Standards;Transforms;Virtual Mock Interviews},
 pages = {1--4},
 doi = {10.1109/SCISISIS61014.2024.10760210}
}


@inproceedings{Mahmud.,
 abstract = {Third-party libraries (TPLs) are an integral part of Android app development, offering app developers essential tools for enhancing app functionality, design, and integration capabilities. However, the fast-paced evolution of Android APIs introduces compatibility issues not only in Android apps but also in TPLs as they rely heavily on these Android APIs too. These challenges necessitate continuous updates and compatibility checks to maintain apps as well as TPL's compatibility across Android's diverse ecosystem. Prior research primarily focused on detecting compatibility issues induced by native Android APIs in Android apps falling short in detecting the incompatible APIs associated with TPLs due to the additional layer of abstraction they introduce as well as the obfuscation used by the TPL developers.In this paper, we propose LibCT that leverages a pre-trained Large Language Model (LLM), GPT-4, for detecting incompatible APIs in Android TPLs. It leverages GPT-4 to generate tests on TPL API usages and executes them across a wide range of Android devices available in the Amazon Device Farm. In our experimental evaluation, we tested 312 libraries with 12,831 APIs on 86 devices in the Amazon device farm, which revealed 274 incompatible APIs, highlighting the LibCT's capability in identifying both evolution-induced and device-specific compatibility issues.},
 author = {Mahmud, Tarek and Duan, Bin and Che, Meiru and Ngu, Anne and Yang, Guowei},
 title = {Testing Android Third Party Libraries with LLMs to Detect Incompatible APIs},
 keywords = {Android;Ecosystems;Foundation models;Incompatible APIs;Large Language Model;Large language models;Libraries;Object recognition;Software engineering;Test Generation;Test pattern generators;Testing;Third-party libraries},
 pages = {280--291},
 doi = {10.1109/Forge66646.2025.00039}
}


@inproceedings{Sharma.,
 abstract = {This study presents a novel approach to plant disease detection by integrating a Convolutional Neural Network (CNN) with an open-source Language Model (LLM) within a userfriendly web application. From a custom-built dataset assembled using open access sources, consisting of 48 classes representing various plant diseases and healthy specimens, the CNN model achieves an impressive accuracy of 99.73{\%} on the test set and application tests reveal high precision of the model. The framework employs a robust experimental setup, including meticulous data partitioning and hyperparameter tuning, to ensure effective model training and evaluation. While CNN demonstrates exceptional performance in detecting wellrepresented diseases, challenges in accurately classifying underrepresented classes are identified, emphasizing the need for data augmentation strategies to enhance model robustness. The integrated LLM enhances user interaction by providing real-time insights and actionable recommendations based on CNN predictions, making the tool accessible to users with varying agricultural expertise. Further work aims to refine the system through dataset expansion and advanced training techniques, ultimately positioning this tool as an asset for sustainable agricultural practices.},
 author = {Sharma, Siddharth and Tiwari, Divyan and Garg, Avaneesh and Kaushal, Abhishek and Mezina, Anzhelika and Frolka, Jakub and {Kishore Dutta}, Malay},
 title = {Enhancing Plant Disease Detection with CNNs and LLMs: A Comprehensive Approach to Diagnosis and Mitigation},
 pages = {13--18}
}


@inproceedings{Siddiq.,
 abstract = {The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT's generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT's responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python {\&} Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers' discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits.CCS CONCEPTS• Software and its engineering $\rightarrow$ Software performance; Software usability; Empirical software validation.},
 author = {Siddiq, M. L. and Roney, L. and Zhang, J. and Santos, J. C. S.},
 title = {Quality Assessment of ChatGPT Generated Code and their Use by Developers},
 keywords = {Chatbots;ChatGPT;Codes;Data mining;datasets;open-coding;programming;pull-request;quality;Quality assessment;Security;Software performance},
 pages = {152--156-152--156}
}


@inproceedings{WeitlHarms.,
 abstract = {This study explores the use of LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability, addressing a critical challenge in product evaluation where traditional review scores, though convenient, fail to capture the richness of qualitative user feedback. Innovations include establishing a method that 1) works with qualitative user experience data without the need for explicit review scores, 2) focuses on implicit user satisfaction, and 3) provides scaled numerical sentiment analysis, offering a more nuanced understanding of user sentiment, instead of simply classifying sentiment as positive, neutral, or negative. Data is collected using the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of two software systems. PDT data was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader, a leading sentiment analysis tool. Each system was asked to evaluate the data in two ways, by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Numerical analysis is used to provide insights into the magnitude of sentiment to drive high quality decisions regarding product desirability. Each LLM is asked to provide its confidence (low, medium, high) in its sentiment score, along with an explanation of its score. All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding user sentiment. This study adds deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment.},
 author = {Weitl-Harms, Sherri and Hastings, John D. and Lum, Jonah},
 title = {Using LLMs to Establish Implicit User Sentiment of Software Desirability},
 keywords = {GPT;LLM;Machine Learning;Measurement;Numerical analysis;Product Desirability Toolkit;Reviews;Sentiment analysis;Software Desirability;Software systems;Technological innovation;Transfer learning;User experience},
 pages = {1645--1650},
 doi = {10.1109/ICMLA61862.2024.00254}
}


@article{Wu.2024,
 abstract = {Conversational recommendation systems (CRS) can actively discover users' preferences and perform recommendations during conversations. The majority of works on CRS tend to focus on a single conversation and dig it using knowledge graphs, language models, etc. However, they often overlook the abundant and rich preference information that exists in the user's historical conversations. Meanwhile, end-to-end generation of recommendation results may lead to a decrease in recommendation quality. In this work, we propose a personalized conversational recommendation system infused with historical interaction information. This framework leverages users' preferences extracted from their historical conversations and integrates them with the users' preferences in current conversations. We find that this contributes to higher accuracy in recommendations and fewer recommendation turns. Moreover, we improve the interactive pattern between the recommendation module and the dialogue generation module by utilizing the slot filling method. This enables the results inferred by the recommendation module to be integrated into the conversation naturally and accurately. Our experiments on the benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art methods in the evaluation of recommendations and dialogue generation.},
 author = {Wu, F. and Zhao, G. and Li, T. and Shen, J. and Qian, X.},
 year = {2024},
 title = {Improving Conversational Recommendation System Through Personalized Preference Modeling and Knowledge Graph},
 keywords = {Accuracy;Conversational recommendation system;dialogue generation;History;Knowledge graphs;Motion pictures;Oral communication;personalized recommendations;Recommender systems;Task analysis},
 pages = {8529--8540-8529--8540},
 volume = {36},
 number = {12},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 doi = {10.1109/TKDE.2024.3421580}
}


@inproceedings{Wu.,
 abstract = {The integration of Large Language Models (LLMs) into edge devices such as smartphones represents a significant leap in mobile technology, promising enhanced user experiences and novel functionalities. This paper presents a first look at LLM-powered smartphones, addressing four key aspects: the current market landscape, core functions enabled by integrated LLMs, potential security risks, and user perceptions. The findings reveal a rapidly evolving market with major manufacturers competing to integrate LLMs, innovative features that improve user interaction, significant security challenges, and mixed user perceptions that balance enthusiasm for new capabilities with privacy concerns. This study contributes to understanding LLM integration in mobile devices and its implications for users, manufacturers, and the broader technological landscape.},
 author = {Wu, Liangxuan and Zhao, Yanjie and Wang, Chao and Liu, Tianming and Wang, Haoyu},
 title = {A First Look at LLM-powered Smartphones},
 keywords = {Conferences;Ethics;Industries;Large language models;Privacy;Security;Smart phones;Software engineering;Technological innovation;User experience},
 pages = {208--217}
}


@inproceedings{Xia.,
 abstract = {This paper presents a novel design of a multi-agent system framework that applies large language models (LLMs) to automate the parametrization of simulation models in digital twins. This framework features specialized LLM agents tasked with observing, reasoning, decision-making, and summarizing, enabling them to dynamically interact with digital twin simulations to explore parametrization possibilities and determine feasible parameter settings to achieve an obj ective. The proposed approach enhances the usability of simulation model by infusing it with knowledge heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos and codes are available at a GitHub Repository: https://github.comlYuchenXia/LLMDrivenSimulation},
 author = {Xia, Yuchen and Dittler, Daniel and Jazdi, Nasser and Chen, Haonan and Weyrich, Michael},
 title = {LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins},
 keywords = {Cognitive load;Decision making;Digital Twin;Digital twins;Intelligent Automation;Large language models;Load modeling;Manufacturing automation;Multi-Agent System;Multi-agent systems;Simulation;Software development management;Usability;Visualization},
 pages = {1--4},
 doi = {10.1109/ETFA61755.2024.10710900}
}


@article{Xiang.2024,
 abstract = {In software maintenance, concise summaries of bug reports are crucial, significantly enhancing developer efficiency and ultimately improving software quality and user experience. Large language models (LLMs) have become the standard method for bug report summarization due to their powerful representation capabilities. However, LLM-based approaches face two primary challenges: accurately modeling the contextual relationships between various components within a bug report and the risk of overfitting when fine-tuning LLMs on datasets of limited size. To address these challenges, we propose a novel approach, SumLLaMA, which leverages contrastive learning pre-training and parameter-efficient fine-tuning. Contrastive learning pre-training is employed to construct contextual relations between components in a single bug report, enabling SumLLaMA to learn sequence-level representations. For parameter-efficient fine-tuning, we fine-tune a smaller adapter instead of the entire LLM, reducing the number of parameters trained to about 1/1500 of the original model, effectively mitigating the risk of overfitting. To evaluate the effectiveness of SumLLaMA, we compare it against five baseline models, including a state-of-the-art model, on a publicly available dataset. The experimental results show that SumLLaMA outperforms all baselines by up to 26.66, 17.10, and 24.01 points in ROUGE-1, ROUGE-2, and ROUGE-L metrics, respectively, achieving a state-of-the-art result for automated bug report summarization.},
 author = {Xiang, B. and Shao, Y.},
 year = {2024},
 title = {SumLLaMA: Efficient Contrastive Representations and Fine-Tuned Adapters for Bug Report Summarization},
 keywords = {Bug report summarization;Codes;Computer bugs;contrastive representation;efficient fine-tuning;Self-supervised learning;Semantics;Software maintenance;Task analysis;Training;Vectors},
 pages = {78562--78571-78562--78571},
 volume = {12},
 issn = {2169-3536},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2024.3397326}
}


@inproceedings{Xu.,
 abstract = {Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4{\%} accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27{\%} to 78{\%} under interactive prompt evolution.},
 author = {Xu, Y. and Feng, J. and Miao, W.},
 title = {Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models},
 keywords = {Accuracy;Formal Specification;Industries;Large language models;Logic;Natural language processing;Prompt engineering;Requirements engineering;Software quality;Software reliability;Training},
 pages = {204--215-204--215},
 doi = {10.1109/QRS62785.2024.00029}
}


@inproceedings{Yabaku.,
 abstract = {The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies.},
 author = {Yabaku, Mounika and Pombo, Nuno and Ouhbi, Sofia},
 title = {Exploring the Potential Use of Generative AI in Software Engineering Education},
 keywords = {AIDriven Educational Tools;Debugging;Education;Generative AI;Large language models (LLMs);Pedagogical Innovation;Requirements engineering;Software;Software development management;Software engineering;Software Engineering Education;Software measurement;Testing;Usability},
 pages = {1--7},
 doi = {10.1109/AICT61888.2024.10740416}
}


@inproceedings{Wang.,
 abstract = {With the rapid development of web technology, more and more software applications have become web-based in the past decades. To ensure software quality and user experience, various techniques have been proposed to automatically test web applications by interacting with their GUIs. To achieve high functional coverage, web GUI testing tools often need to generate high-quality text inputs and interact with the associated GUI elements (e.g., click submit buttons). However, developing a holistic approach that solves both subtasks is challenging because the web GUI context can be complicated and highly dynamic, which is hard to process programmatically. The recent development of large vision-language models (LVLM) provides new opportunities to handle these longstanding problems. We in this paper propose VETL, the first LVLM-driven end-to-end web testing technique. With LVLM's scene understanding capabilities, VETL can generate valid and meaningful text inputs focusing on the local context, while avoiding the need to extract precise textual attributes. The selection of associated GUI elements is formulated as a visual question answering problem, allowing LVLM to capture the logical connection between the input box and the relevant element based on visual instructions. Further, the GUI exploration is guided by a multi-armed bandit module employing a curiosity-oriented strategy. Experiments show that VETL is effective in exploring web state/action spaces and detecting bugs. Compared with WebExplor, the state-of-the-art web testing technique, VETL can discover 25{\%} more unique web actions on benchmark websites. Moreover, it can expose functional bugs in top-ranking commercial websites, which have been confirmed by the website maintainers. Our work makes the first attempt of leveraging LVLM in end-to-end GUI testing, demonstrating promising results of this research direction.},
 author = {Wang, Siyi and Wang, Sinan and Fan, Yujia and Li, Xiaolei and Liu, Yepang},
 title = {Leveraging Large Vision-Language Model for Better Automatic Web GUI Testing},
 keywords = {Automatic Web GUI Testing;Computer bugs;Focusing;Graphical user interfaces;Large Language Model;Large Vision-Language Model;Question answering (information retrieval);Software maintenance;Software quality;Space exploration;Testing;Text Input Generation;User experience;Visualization},
 pages = {125--137},
 doi = {10.1109/ICSME58944.2024.00022}
}


@inproceedings{Yang.,
 abstract = {The development of effective diagnostic methodolo-gies for software system failures is of paramount importance. Traditional methods, which rely on specialized terminology and intricate reasoning, require users to have a technical background, resulting in reduced flexibility and decreased user-friendliness. With the rise of generative large language models, optimizing human-computer interaction has become a critical area of focus. Additionally, the inherent intelligence and extensive knowledge of large language models make them both easy and effective to employ for fault diagnosis assistance. We introduce IFKG, an advanced tool for diagnosing software system failures. IFKG integrates generative large language models with knowledge graphs, employing natural language interactions to implement fault detection and deliver solutions. IFKG enables users to upload descriptive problems, retrieve pertinent information from the knowledge graph, and present diagnostic results in natural language. Our accuracy assessments across diverse software system failures indicate that the IFKG provides targeted and actionable recommendations, effectively assisting users in ad-dressing a range of software system issues. The tool is available on GitHub at https://github.com/mako-xxlIFKG, and the demo video can be found on YouTube: https://youtu.belDie2vgZm2hk.},
 author = {Yang, Xixuan and Jia, Tong and Li, Ying and Huang, Gang},
 title = {IFKG: An Intelligent Fault Diagnosis Tool with Knowledge Graph and Generative LLM},
 keywords = {Fault diagnosis;Human computer interaction;knowledge graph;Knowledge graphs;Large language models;LLM;Natural languages;Software development management;software fault diagnosis;Software systems;Terminology;Video on demand;Web sites},
 pages = {839--843},
 doi = {10.1109/SANER64311.2025.00088}
}


@inproceedings{Yu.,
 abstract = {This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs' capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.},
 author = {Yu, S. and Fang, C. and Ling, Y. and Wu, C. and Chen, Z.},
 title = {LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities},
 keywords = {Automation;Behavioral sciences;ChatGPT;Large Language Model;Mobile App Testing;Mobile applications;Software quality;Software reliability;Software testing;Test Generation;Test Migration;User interfaces},
 pages = {206--217-206--217},
 doi = {10.1109/QRS60937.2023.00029}
}


@inproceedings{Yuan.,
 abstract = {The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel's v0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google's Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.},
 author = {Yuan, Mingyue and Chen, Jieshan and Xing, Zhenchang and Quigley, Aaron and Luo, Yuyu and Luo, Tianqi and Mohammadi, Gelareh and Lu, Qinghua and Zhu, Liming},
 title = {DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models},
 keywords = {Codes;Design Guideline;Design methodology;Frontend Code Repair;Knowledge based systems;Large language models;Maintenance engineering;Software development management;Software engineering;System analysis and design;UI Design;Usability;User experience},
 pages = {2483--2494},
 doi = {10.1109/ICSE55347.2025.00109}
}


@inproceedings{Zaeifi.,
 abstract = {The proliferation of smartphone technology has generated unprecedented volumes of data, creating challenges in under-standing digital behavior patterns. We present a new computational system that integrates Large Language Models (LLMs) with conventional data processing techniques using a novel three-levelOur system employs LLMs for zero-shot learning capabilities to classify usage patterns, achieving 95{\%} accuracy in task classification through automated pattern identification. The system implements pattern verification reaching 98{\%} validation accuracy and utilizes automated validation that reduces data loss by 75{\%}. This hierarchical approach demonstrates consistent performance across diverse device types and usage scenarios while maintaining processing efficiency through automated prompt engineering and code generation.},
 author = {Zaeifi, Mehdi and Lin, Beiyu},
 title = {Smartphone Usage Data Cleaning Using LLM-Based Processing},
 keywords = {Accuracy;Big Data;Cleaning;Codes;Data processing;Large language models;Performance evaluation;Prompt engineering;Zero shot learning},
 pages = {8871--8873},
 doi = {10.1109/BigData62323.2024.10825647}
}


@inproceedings{Zhang.,
 abstract = {The Intelligent Park Energy Control Platform (IPECP) leverages Large Language Models (LLM) to advance smart park energy management, aligning with China's carbon peaking and neutrality goals. By integrating LLMs, digital twin technology, and hierarchical scheduling optimization, the IPECP processes complex, unstructured data to uncover energy usage patterns and market trends. This enables accurate predictions and optimized energy dispatch strategies, enhancing real-time monitoring and control. The platform's architecture includes a data layer for collection and processing, an intelligent analytic layer for deep insights, and a user interaction layer for personalized advice and support. Application results show a 15{\%} to 20{\%} reduction in monthly energy consumption and over 50{\%} reduction in operational carbon emissions, achieving zero emissions with carbon sinks. The IPECP also improves energy management efficiency through automated operations and maintenance. Future work will expand the platform's capabilities to include more renewable energy sources and develop advanced business models to promote wider adoption and support global climate efforts.},
 author = {Zhang, Dingsheng and Li, Zepeng and Liang, Xueying and Chen, Gang},
 title = {Research on the Intelligent Park Energy Control Platform Based on LLM},
 keywords = {Accuracy;Business;Carbon;Carbon dioxide;carbon management;Digital twins;Energy consumption;energy control;Energy management;energy optimization;LLM;Market research;Optimization;Real-time systems},
 pages = {41--45},
 doi = {10.1109/ICSCGE64239.2024.11064308}
}


@inproceedings{Zhang.b,
 abstract = {Large language models (LLMs) excel at general question-answering (Q{\&}A) but often fall short in specialized domains due to a lack of domain-specific knowledge. Commercial companies face the dual challenges of privacy protection and resource constraints when involving LLMs for fine-tuning. This paper propose a novel framework, Self-Evolution, designed to address these issues by leveraging lightweight open-source LLMs through multiple iterative fine-tuning rounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution employ a strategy that filters and reinforces the knowledge with higher value during the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat using 4,000 documents containing rich domain knowledge from China Mobile, achieving a performance score 174{\%} higher on domain-specific question-answering evaluations than Qwen1.5-7B-Chat and even 22{\%} higher than Qwen1.5-72B-Chat. Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days, and it improves the efficiency of locating alarms, fixing problems, and finding related reports, with an average efficiency improvement of over 18.6{\%}. In addition, we release Self-Evolution framework code in https://github.com/Zero-Pointer/Self-Evolution.},
 author = {Zhang, Shenglin and Zhu, Pengtian and Ma, Minghua and Wang, Jiagang and Sun, Yongqian and Li, Dongwen and Wang, Jingyu and Guo, Qianying and Hua, Xiaolei and Zhu, Lin and Pei, Dan},
 title = {Enhanced Fine-Tuning of Lightweight Domain-Specific Q{\&}A Model Based on Large Language Models},
 keywords = {Conferences;Data mining;domain alignment;Faces;Filters;Iterative methods;Large Language Model;Large language models;Maintenance;Privacy;Protection;question answering;Software reliability},
 pages = {61--66},
 doi = {10.1109/ISSREW63542.2024.00048}
}


@article{Zhang.2024,
 abstract = {Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of `noise'; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models `learn' inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.},
 author = {Zhang, Y. and Qiu, Z. and Stol, K. J. and Zhu, W. and Zhu, J. and Tian, Y. and Liu, H.},
 year = {2024},
 title = {Automatic Commit Message Generation: A Critical Review and Directions for Future Work},
 keywords = {benchmark;Chatbots;Codes;commit message generation;Commit-based software development;Information retrieval;Machine translation;Noise measurement;open collaboration;Software maintenance;Task analysis},
 pages = {816--835-816--835},
 volume = {50},
 number = {4},
 issn = {1939-3520},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2024.3364675}
}


@inproceedings{Zheng.,
 abstract = {Programming knowledge is a crucial aspect of computer science education, and unit testing is commonly employed to automatically assess programming assignments. Instructors and teaching assistants typically invest considerable efforts in writing unit tests, which may still be vulnerable to human oversight and mistakes. In this work, we explored the feasibility of using Large Language Models (LLMs) to automate the assessment of programming assignments. In particular, we proposed two approaches: the plain approach that uses GPT-4o-mini in a vanilla setting, and the augmented approach that integrates additional strategies such as tailored prompts with syntax and semantic constraints, and a feedback mechanism with information on test-effectiveness metrics. We evaluate the two approaches on six real-world programming assignments from an introductory-level programming course at our university. Compared to the plain approach, the augmented approach improves the usability and effectiveness of the generated unit tests, reducing 85 {\%} compilation errors while enhancing the statement coverage and mutation scores by 1.7 x and 2.1 x, respectively. In addition, the augmented approach also complements human-written tests by covering additional program behaviors. In a case study of 1296 students' submissions that pass human-written tests, the augmented approach successfully detected new bugs in 13 {\%} submissions, with an accuracy of 27 {\%}. These results not only demonstrate the potentials of LLMs in generating useful unit tests for programming assignments, but also highlight the strategies that can effectively enhance LLMs' capabilities to augment human-written tests, offering practical benefits for both educators and students.},
 author = {Zheng, Kaisheng and Shen, Yuanyang and Tao, Yida},
 title = {Automatic Unit Test Generation for Programming Assignments Using Large Language Models},
 keywords = {Computer bugs;Large language models;programming assignments;Programming profession;Reproducibility of results;Semantics;Syntactics;Test pattern generators;Testing;Unit test generation;Usability;Writing},
 pages = {242--252},
 doi = {10.1109/CSEET66350.2025.00031}
}


@inproceedings{You.,
 abstract = {This paper studies the practical method of multimodal data fusion technology in traffic. The practical method of multimodal data fusion technology based on LLM and attention mechanism is proposed, and the analysis model of multimodal data fusion technology is constructed, and the quantitative recursive analysis model is used to analyze the constructed data model to realize the feature extraction of constrained feature data. Then, we study the practical method of traffic multi-modal data fusion technology based on LLM and attention mechanism, optimize the number of neurons in the hidden layer neural network, and complete the practical analysis of traffic multi-modal data fusion technology. The simulation experiment results show that the proposed method in this paper can effectively improve the application efficiency and application effect of traffic multi-mode data fusion technology, and provide professional talents for the development of innovative LLM and attention mechanism.},
 author = {You, Boqian},
 title = {Research on Multimodal Data Fusion Technology based on LLM and Attention Mechanism},
 keywords = {Analytical models;attention mechanism;Attention mechanisms;data fusion technology;Data integration;Data models;Feature extraction;Human computer interaction;LLM;multimodal data;Neurons;Optimization;Problem-solving;Robots},
 pages = {1--4},
 doi = {10.1109/ICHORA65333.2025.11016994}
}


@inproceedings{Shuvo.,
 abstract = {The automobile service industry's explosive growth highlights the need for creative approaches to boost operational effectiveness and user experience. This study introduces a Hybrid Garage Assistance System, integrating Classical Machine Learning (ML) techniques with Generative AI to optimize garage service discovery and analysis. The system employs sophisticated data processing methods, including Term Frequency-Inverse Document Frequency (TF-IDF) vectorization and regex-based service detection, to extract actionable insights from unstructured garage data.Central to the system are machine learning models Random Forest (RF) and XGBoost (XGB) which achieve high precision and recall in classifying garage services. A hybrid search mechanism, combining cosine similarity with ML-driven predictions, ensures the delivery of highly personalized search results. To further refine decision-making, the system incorporates Generative AI models such as Perplexity for web-based research, Gemini for location-specific analysis, Mistral for email sending and GPT-4 for detailed service recommendations and dall-e for creating user specific parts images. These advanced tools provide users with comprehensive information that enables them to make well-informed decisions about garage services.Performance evaluation of the system is conducted using robust metrics, including precision, recall, F1-score, and system latency. Experimental results reveal a precision of 𝟖 𝟓 {\%}, recall of 𝟕 𝟎 . 𝟖 {\%}, and an F1-score of 𝟕 𝟕 . 𝟐 {\%}, demonstrating the efficacy of integrating classical ML with generative AI. The system's average latency of 5.9 seconds ensures a seamless and responsive user experience.This hybrid framework highlights the potential of blending classical ML and Large Language Models (LLMs) to enhance search and recommendation functionalities, offering a scalable and robust blueprint for future advancements in the automotive service sector. The system's Propose a Multi-Agent System With high accuracy, scalability, and reliability position it as a cutting-edge solution for users navigating the complexities of garage service selection.},
 author = {Shuvo, Mahfuzur Rahman and Rahman, Ashifur and Akuthota, Vishwanath and Paul, Tanay and Islam, Mahfujul and Ashraf, Md Sadi and Roy, Prosenjit and Reza, Md Tanzim},
 title = {A Multi-Agent Garage Service Search and Recommendation with Hybrid MLs and LLMs},
 keywords = {Garage;Generative AI;Hybrid power systems;LLMs;Measurement;MLs;MultiAgent;Multi-agent systems;Navigation;Radio frequency;Random forests;Reliability;Scalability;User experience},
 pages = {1--6},
 doi = {10.1109/ICCECE61355.2025.10940937}
}


@inproceedings{Wang.b,
 abstract = {The prevalent engagement with mobile apps underscores the importance of understanding their data practices. Transparency plays a crucial role in this context, ensuring users to be informed and give consent before any data access occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to inform users about detailed insights into apps' data access and sharing. This feature continues Apple's trend of privacy-focused innovations (following Privacy Nutrition Labels), and has been marketed as a big step forward in user privacy. However, its real-world impacts on user privacy and control remain unexamined. We thus proposed an end-to-end study involving systematic assessment of the App Privacy Report's real-world benefits and limitations, LLM-enabled and multi-technique synthesized enhancements, and comprehensive evaluation from both system and user perspectives. Through a structured focus group study with twelve everyday iOS users, we explored their experiences, understanding, and perceptions of the feature, suggesting its limited practical impact resulting from missing important details. We identified two primary user concerns: the clarity of data access purpose and domain description. In response, we proposed enhancements including a purpose inference framework and domain clarification pipeline. We demonstrated the effectiveness and benefits of such enhancements for mobile app users. This work provides practical insights that could help enhance user privacy transparency and discusses areas for future research.},
 author = {Wang, Liu and Wang, Dong and Pan, Shidong and Jiang, Zheng and Wang, Haoyu and Wang, Yi},
 title = {A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements},
 keywords = {Data privacy;Data transparency;Mobile applications;Pipelines;Privacy;Security;Stakeholders;Systematics;Technological innovation;Usability},
 pages = {4210--4228},
 doi = {10.1109/SP61157.2025.00223}
}


@article{Voria.2025,
 abstract = {Stakeholders' conversations requirements elicitation meetings hold valuable insights into system and client needs. However, manually extracting requirements is time-consuming, labor-intensive, and prone to errors and biases. While current state-of-the-art methods assist in summarizing stakeholder conversations and classifying requirements based on their nature, there is a noticeable lack of approaches capable of both identifying requirements within these conversations and generating corresponding system requirements. These approaches would assist requirement identification, reducing engineers' workload, time, and effort. They would also enhance accuracy and consistency in documentation, providing a reliable foundation for further analysis. To address this gap, this paper introduces RECOVER (Requirements EliCitation frOm conVERsations), a novel conversational requirements engineering approach that leverages natural language processing and large language models (LLMs) to support practitioners in automatically extracting system requirements from stakeholder interactions by analyzing individual conversation turns. The approach is evaluated using a mixed-method research design that combines statistical performance analysis with a user study involving requirements engineers, targeting two levels of granularity. First, at the conversation turn level, the evaluation measures RECOVER's accuracy in identifying requirements-relevant dialogue and the quality of generated requirements in terms of correctness, completeness, and actionability. Second, at the entire conversation level, the evaluation assesses the overall usefulness and effectiveness of RECOVER in synthesizing comprehensive system requirements from full stakeholder discussions. Empirical evaluation of RECOVER shows promising performance, with generated requirements demonstrating satisfactory correctness, completeness, and actionability. The results also highlight the potential of automating requirements elicitation from conversations as an aid that enhances efficiency while maintaining human oversight.},
 author = {Voria, Gianmario and Casillo, Francesco and Gravino, Carmine and Catolino, Gemma and Palomba, Fabio},
 year = {2025},
 title = {RECOVER: Toward Requirements Generation From Stakeholders' Conversations},
 keywords = {Accuracy;automated software engineering;Conversational requirements engineering;Data mining;Documentation;Focusing;Large language models;Machine Learning;Natural language processing;Oral communication;Requirements engineering;Software engineering;Stakeholders},
 pages = {1912--1933},
 volume = {51},
 number = {6},
 issn = {1939-3520},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2025.3572056}
}


@inproceedings{Sinha.,
 abstract = {There is a growing need to develop more robust, automated methods to check for vulnerabilities in code and software. Large language models, due to their ability to engage with a wide variety of scenarios, provide a personalized framework for users to safeguard their devices. This paper focuses on using a LLaMa3.2 3B model as its base, pretrained on the DiverseVul dataset consisting of vulnerable code snippets in programming languages such as C, C++, and Python. Using QLoRa for PEFT-based fine-tuning, the model achieved an Accuracy of 93.75{\%}, outperforming other models. The CLI application provides a seamless user experience by pinpointing user directories for vulnerability scanning and classification. Despite its promising results, the study identifies limitations, including challenges with obfuscated code, computational resource constraints, and generalization issues. This research contributes to advancing real-time detection capabilities and scalable solutions for cybersecurity.},
 author = {Sinha, Nishchay and Trivedi, Raghav and Mittapalli, Sanjit and Parihar, Anuj and Selvanambi, Ramani},
 title = {Targeting and Automating Recoveries from Cybersecurity Vulnerabilities Using Large Language Models},
 keywords = {Codes;Computational modeling;Computer security;Cybersecurity;Deep Learning;Generative AI;Large language models;Large Language Models (LLM);Maintenance engineering;Pipelines;Python;Real-time systems;Software;Threat Intelligence;User experience},
 pages = {1369--1373},
 doi = {10.1109/IC3ECSBHI63591.2025.10990905}
}


@inproceedings{Sladic.,
 abstract = {Honeypots are essential tools in cybersecurity for early detection, threat intelligence gathering, and analysis of attacker's behavior. However, most of them lack the required realism to engage and fool human attackers long-term. Being easy to distinguish honeypots strongly hinders their effectiveness. This can happen because they are too deterministic, lack adaptability, or lack deepness. This work introduces shelLM, a dynamic and realistic software honeypot based on Large Language Models that generates Linux-like shell output. We designed and implemented shelLM using cloud-based LLMs. We evaluated if shelLM can generate output as expected from a real Linux shell. The evaluation was done by asking cybersecurity researchers to use the honeypot and give feedback if each answer from the honeypot was the expected one from a Linux shell. Results indicate that shelLM can create credible and dynamic answers capable of addressing the limitations of current honeypots. ShelLM reached a TNR of 0.90, convincing humans it was consistent with a real Linux shell. The source code and prompts for replicating the experiments have been publicly available.},
 author = {Sladi{\'c}, M. and Valeros, V. and Catania, C. and Garcia, S.},
 title = {LLM in the Shell: Generative Honeypots},
 keywords = {Computer security;honeypots;Large language models;Linux;shelLM;Software;Source coding},
 pages = {430--435-430--435},
 doi = {10.1109/EuroSPW61312.2024.00054}
}


@inproceedings{Strand.,
 abstract = {The rapid evolution of digital sports media necessitates sophisticated information retrieval systems that can efficiently parse extensive multimodal datasets. This paper demonstrates SoccerRAG, an innovative framework designed to harness the power of Retrieval Augmented Generation (RAG) and Large Language Models (LLMs) to extract soccer-related information through natural language queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic querying and automatic data validation, enhancing user interaction and accessibility to sports archives. We present a novel interactive user interface (UI) based on the Chainlit framework which wraps around the core functionality, and enable users to interact with the SoccerRAG framework in a chatbot-like visual manner.},
 author = {Strand, Aleksander Theo and Gautam, Sushant and Midoglu, Cise and Halvorsen, P{\aa}l},
 title = {Demo: Soccer Information Retrieval Via Natural Queries using SoccerRAG},
 keywords = {association football;Data mining;Databases;Feature extraction;Information retrieval;Large language models;Natural language processing;Natural languages;Pipelines;Retrieval augmented generation;Sports;Structured Query Language;UI;Visualization},
 pages = {1--5},
 doi = {10.1109/CBMI62980.2024.10859233}
}


@inproceedings{Sunico.,
 abstract = {Amid the highly competitive job market, creating an effective resume is vital but often difficult, particularly for students from underprivileged backgrounds with limited career development support. To mitigate this, we introduce a novel resume building application that employs the Large Language Model (LLM) to aid students in composing their first resumes. The application comprises three modules: Resume Generation, Resume Assessment, and User I/O. The Resume Generation module utilizes prompt engineering to produce resume bullet points, while the Resume Assessment module evaluates these bullet points for potential enhancements. The User I/O module simplifies user interaction by accepting free-style plain English as input and displaying the generated bullet points as suggestions. We have developed a prototype application that demonstrates the effectiveness of these functionalities while emphasizing ease-of-use. We have also confirmed that the content generated for resumes adheres to the benchmarks of high-quality standards. As future work, we aim to carry out usability testing with real students to further evaluate the application's utility in educational environments.},
 author = {Sunico, R. J. and Pachchigar, S. and Kumar, V. and Shah, I. and Wang, J. and Song, I.},
 title = {Resume Building Application based on LLM (Large Language Model)},
 keywords = {Buildings;Engineering profession;LLM (Large Language Model);Prompt engineering;resume building;Resumes;Social networking (online);Standards;Usability;User interfaces},
 pages = {486--492-486--492},
 doi = {10.1109/ICCCIS60361.2023.10425602}
}


@inproceedings{Swamy.,
 abstract = {AI PDF is a smart and simple to use program for conversing with documents. Many people use it for interaction with documents for work, study and life management purposes. AI PDF is a tool of analysis chatbot which enables precise extraction of insight from uploaded PDF and other documents into it. AI PDF as another AI tool for interacting with files. Yes, you can pose the document questions to help you summarize the document, learn more information, and overall get a better understanding of the document without a necessity to read it. This software is a perfect fit for students studying new skills, theories, projects, and other stuff. You can save lots of time of reading entire PDFs by just AI summarizing main points for you. Users can operate the interface and upload the documents easily due to its user-friendly nature. This leaves you able to add PDFs respectively and start chatting to which is weird at first but gets used to quickly so one can easily see where the sourcing is utilized in a document which is good for proofing. The present AI PDF works with LLMA (Large Language Model Algorithm) AI would quickly search for information and give you a succinct summary. Thus, without spending time on reaching the very end of the document, you are able to grasp its main idea in general, this issue can be solved even though you scanning pdfs.},
 author = {Swamy, S. Sriramana and Alanssari, Ali Ihsan and Adel, Abual-Hass and Al-Hussein, Rouaida Kadhim A. and Manipreeth, Macha and Nithin, Vislavath},
 title = {AI PDF's using LLMA Model: Time Saving Document Analysis},
 keywords = {Adaptation models;Artificial intelligence;Databases;Google Auth;Lang Chain;Large language models;LLMA Model;Portable document format;PostgreSQL Database;Software;Stripe;Text analysis;Training;User interfaces;Vector Database;Vectors},
 pages = {1--5},
 doi = {10.1109/ARIIA63345.2024.11051535}
}


@inproceedings{Tanaka.,
 abstract = {The superior functionality and versatility of generative AI have raised expectations for the improvement of human society and concerns about the ethical and social risks associated with the use of generative AI. Many previous studies have presented risk issues as concerns associated with the use of generative AI, but since most of these concerns are from the user's perspective, they are difficult to lead to specific countermeasures. In this study, the risk issues presented by the previous studies were broken down into more detailed elements, and risk factors and impacts were identified. In this way, we presented information that leads to countermeasure proposals for generative AI risks.CCS CONCEPTS• General and reference$\rightarrow$Evaluation; Surveys and overviews, • Human-centered computing$\rightarrow$HCI theory, concepts and models; • Social and professional topics$\rightarrow$Computing / technology policy.},
 author = {Tanaka, H. and Ide, M. and Yajima, J. and Onodera, S. and Munakata, K. and Yoshioka, N.},
 title = {Taxonomy of Generative AI Applications for Risk Assessment},
 keywords = {Atmospheric modeling;Computational modeling;Ethics;Generative AI;language models;responsible AI;responsible innovation;risk assessment;Risk management;Surveys;Taxonomy;technology risks},
 pages = {288--289-288--289}
}


@inproceedings{Tao.,
 abstract = {As Large Language Models (LLMs) advance in natural language processing, there is growing interest in leveraging their capabilities to simplify software interactions. In this paper, we propose a novel framework that integrates LLMs for both classifying natural language inputs into corresponding API calls and automating the creation of sample datasets tailored to specific API functions. By classifying natural language commands, our work allows users to invoke complex software functionalities through simple inputs, improving interaction efficiency and lowering the barrier to software utilization. Our dataset generation approach also enables the efficient and systematic evaluation of different LLMs in classifying API calls, offering a practical tool for developers or business owners to assess the suitability of LLMs for customized API management. We conduct experiments on several prominent LLMs using generated sample datasets for various API functions. The results show that GPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B performs much worse at 0.759. These findings highlight the potential of LLMs to transform API management and validate the effectiveness of our framework in guiding model testing and selection across diverse applications.},
 author = {Tao, Chunliang and Fan, Xiaojing and Yang, Yahe},
 title = {Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation},
 keywords = {Accuracy;API Classification;API Management;Business;Dataset Generation;Large language models;Large language models (LLMs);Natural language processing;Natural Language Processing (NLP);Software;Synthetic data;Systematics;Testing;Transforms;Translation},
 pages = {628--634},
 doi = {10.1109/CAIT64506.2024.10962957}
}


@inproceedings{Wahba.,
 abstract = {This paper presents and explores the development of digital human twin through mobile app integrated with chat-bot and aimed to deliver dynamic and realistic user interactions. By utilizing advanced technologies such as reinforcement learning, lip-sync animation, and facial expression modeling. This project aims to develop a personal virtual assistant using various technologies. The system integrate various deep learning models such as 3D avatar generation, speech recognition, clone voice from the user, Large Language Model (LLM), 3D avatar after uploading images from the user, and engages in lifelike human conversation. In the evaluation of the system there's a potential to improve user experience. Moreover, the paper also includes a review of related datasets, highlighting the capabilities and challenges of creating a realistic virtual avatar.},
 author = {Wahba, Karim Ahmed and {Amr Ahmed}, Khaled and Kamel, Martina Raafat and Fathy, Marwan and Abdelfatah, Pr. Khaled Hussien and Hatem, Sarah},
 title = {Creating a Digital Human Twin: Cloning Voice, Face, and Attitude},
 keywords = {Deep Learning;Large language models;Mobile applications;Reinforcement learning;Solid modeling;Speech recognition;Three-dimensional displays;Training;User experience;Virtual assistants},
 pages = {199--205},
 doi = {10.1109/MIUCC62295.2024.10783609}
}


@inproceedings{Tao.b,
 abstract = {Many human-computer interactions and automated processes are dependent on data. We need to correctly store data and retrieve data to support these processes. Large Language Models (LLMs) have made significant progress in comprehension and reasoning. However, we have found that their ability to handle professional data is weak, which significantly limits their applications in automated processes and professional applications. These scenarios often involve a large number of similar data entries, which are challenging for LLMs such as ChatGPT-4 Omni. In this paper, we propose a Rational Intelligence Model that comprehends human experts' knowledge of data structure and process requirements of data, automatically extracts data from conversations with end users, and effectively stores the data and retrieves it for supporting the interaction and process. Experiments show that ChatGPT-4o can achieve 0{\%} error in simple queries, 2.6{\%} errors when data entries are out of order, and 38{\%} errors when the queries are reasonably complex. However, with our proposed Rational Intelligence Model (RIM), we can achieve 0{\%} error rate in all tests. RIM fundamentally changes software engineering and expert system development approaches. Instead of having a software engineer understand expert knowledge of data processing, this is now achieved by RIM, which means it is much more flexible, lower in cost, and requires much less development time.},
 author = {Tao, XueHong and Miao, Yuan and Wang, Guanhua},
 title = {Rational Intelligence Model: Overcoming Data Handling Limitations in LLMs},
 keywords = {Accuracy;ChatGPT;Cognition;Costs;Data models;Data processing;Error analysis;expert system;Large Language Model;Large language models;Process Automation;Rational Intelligence;Software;Software engineering;Solid modeling;SQL},
 pages = {519--523},
 doi = {10.1109/ICARCV63323.2024.10821608}
}


@inproceedings{ValenzuelaToledo.,
 abstract = {GitHub Actions (GA) has become the de facto tool that developers use to automate software workflows, seamlessly building, testing, and deploying code. Yet when GA fails, it disrupts development, causing delays and driving up costs. Diagnosing failures becomes especially challenging because error logs are often long, complex and unstructured. Given these difficulties, this study explores the potential of large language models (LLMs) to generate correct, clear, concise, and actionable contextual descriptions (or summaries) for GA failures, focusing on developers' perceptions of their feasibility and usefulness. Our results show that over 80 {\%} of developers rated LLM explanations positively in terms of correctness for simpler/small logs. Overall, our findings suggest that LLMs can feasibly assist developers in understanding common GA errors, thus, potentially reducing manual analysis. However, we also found that improved reasoning abilities are needed to support more complex CI/CD scenarios. For instance, less experienced developers tend to be more positive on the described context, while seasoned developers prefer concise summaries. Overall, our work offers key insights for researchers enhancing LLM reasoning, particularly in adapting explanations to user expertise.},
 author = {Valenzuela-Toledo, Pablo and Wu, Chuyue and Hern{\'a}ndez, Sandro and Boll, Alexander and Machacek, Roman and Panichella, Sebastiano and Kehrer, Timo},
 title = {Explaining GitHub Actions Failures with Large Language Models: Challenges, Insights, and Limitations},
 keywords = {CI/CD;Cognition;Focusing;GitHub Action Run Failure Explanation;GitHub Actions;Large language models;Manuals;Productivity;Software;Software development management;Stability analysis;Testing;Usability},
 pages = {286--297},
 doi = {10.1109/ICPC66645.2025.00037}
}


@inproceedings{Vijayvargiya.,
 abstract = {Code readability strongly influences code compre-hension and, to some degree, code quality. Unreadable code makes software maintenance more challenging and is prone to more bugs. To improve the readability, using good identifier names is crucial. Existing studies on automatic identifier re-naming have not considered aspects such as the code context. Additionally, prior research has done little to address the typical challenges inherent in the identifier renaming task. In this paper, we propose a new approach for renaming identifiers in source code by fine-tuning a transformer model. Through the use of perplexity as an evaluation metric, our results demonstrate a significant decrease in the perplexity values for the fine-tuned approach compared to the baseline, reducing them from 363 to 36. To further validate our method, we conduct a developers' survey to gauge the suitability of the generated identifiers, comparing original identifiers with identifiers generated with our approach as well as two state-of-the-art large language models, GPT-4 Turbo and Gemini Pro. Our approach generates better identifier names than the original names and exhibits competitive performance with state-of-the-art commercial large language models. The proposed method carries significant implications for software developers, tool vendors, and researchers. Software developers may use our proposed approach to generate better variable names, increasing the clarity and readability of the software. Researchers in the field may use and build upon the proposed approach for variable renaming.},
 author = {Vijayvargiya, Sanidhya and Saad, Mootez and Sharma, Tushar},
 title = {Enhancing Identifier Naming Through Multi-Mask Fine-Tuning of Language Models of Code},
 keywords = {Codes;Large language models;Predictive models;Software development management;Software maintenance;Source coding;Surveys;Training;Transformers;Usability},
 pages = {71--82},
 doi = {10.1109/SCAM63643.2024.00017}
}


@article{Villagran.2024,
 abstract = {This article presents a controlled case study focused on implementing and using generative artificial intelligence, specifically large language models (LLMs), in physiotherapy education to assist instructors with formulating effective technology-mediated feedback for students. It outlines how these advanced technologies have been integrated into an existing feedback-oriented platform to guide instructors in providing feedback inputs and establish a reference framework for future innovations in practical skills training for health professions education. Specifically, the proposed solution uses LLMs to automatically evaluate feedback inputs made by instructors based on predefined and literature-based quality criteria and generates actionable textual explanations for reformulation. In addition, if the instructor requires, the tool supports summary generation for large sets of text inputs to achieve better student reception and understanding. The case study describes how these features were integrated into the feedback-oriented platform, how their effectiveness was evaluated in a controlled setting with documented feedback inputs, and the results of its implementation with real users through cognitive walkthroughs. Initial results indicate that this innovative implementation holds great potential to enhance learning and performance in physiotherapy education and has the potential to expand to other health disciplines where the development of procedural skills is critical, offering a valuable tool to assess and improve feedback based on quality standards for effective feedback processes. The cognitive walkthroughs allowed us to determine participants' usability decisions in the face of these new features and to evaluate the perceived usefulness, how this would integrate into their workload, and their opinion regarding the potential for the future within this teaching strategy. This article concludes with a discussion of the implications of these findings for practice and future research directions in this developing field.},
 author = {Villagr{\'a}n, I. and Hern{\'a}ndez, R. and Schuit, G. and Neyem, A. and Fuentes-Cimma, J. and Miranda, C. and Hilliger, I. and Dur{\'a}n, V. and Escalona, G. and Varas, J.},
 year = {2024},
 title = {Implementing Artificial Intelligence in Physiotherapy Education: A Case Study on the Use of Large Language Models (LLM) to Enhance Feedback},
 keywords = {Artificial intelligence;Feedback;generative artificial intelligence (AI);health science education;Large language models;Large language models (LLMs);Logic gates;procedural skills;Reviews;Task analysis;technology-enhanced learning;Training;Tutorials},
 pages = {2025--2036-2025--2036},
 volume = {17},
 journal = {IEEE Transactions on Learning Technologies},
 doi = {10.1109/TLT.2024.3450210}
}


@inproceedings{Virvou.,
 abstract = {The recent launch of ChatGPT by OpenAI has created a profound global impact, initiating deep questions among educators about how it might affect education, syllabi and teaching methods. Currently, the full scope of potential benefits and risks associated with ChatGPT in education remains unclear, given that its impact surpasses the level of preparation educators and institutions may have had for such a pre-trained generative AI tool. While Artificial Intelligence in Education has long been a subject of research, with a particular focus on developing Intelligent Tutoring Systems, the emergence of ChatGPT marks a distinctive advancement in this field. Unlike dedicated Intelligent Tutoring Systems, ChatGPT is readily available to a diverse spectrum of educational stakeholders, including teachers, students, schools, universities, and educational institutions. Scholars have initiated assessments of ChatGPT's effectiveness across various educational disciplines, even though ChatGPT was not explicitly designed for educational purposes. However, the widespread accessibility of ChatGPT, coupled with its extensive knowledge base, necessitates the development of comprehensive evaluation frameworks. In this paper, we introduce a holistic evaluation framework tailored for ChatGPT. This framework takes into account both soft and hard skills, and it is designed to seamlessly incorporate ChatGPT into Intelligent Tutoring Systems, making it suitable for a wide range of educational fields. By establishing a connection between ITS and ChatGPT, as they are both AI tools, we can benefit from the substantial background work achieved by previous research in ITSs to evaluate the educational influence of ChatGPT.},
 author = {Virvou, M. and Tsihrintzis, G. A.},
 title = {Is ChatGPT Beneficial to Education? A Holistic Evaluation Framework Based on Intelligent Tutoring Systems},
 keywords = {AI in Education;Chatbots;ChatGPT;Cognition;Education;Educational Evaluation Frameworks;educational software;e-learning;Ethics;Generative AI;Intelligent Tutoring Systems;Knowledge based systems;Large language models;Stakeholders},
 pages = {1--8-1--8},
 doi = {10.1109/IISA59645.2023.10345949}
}


@inproceedings{Virvou.b,
 abstract = {This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts},
 author = {Virvou, M. and Tsihrintzis, G. A.},
 title = {Pre-made Empowering Artificial Intelligence and ChatGPT: The Growing Importance of Human AI-Experts},
 keywords = {AI-Empowered Software Engineering;Artificial intelligence;Chatbots;ChatGPT;e-learning Requirements Engineering in AI;Ethics;Generative AI;Human-centered AI;Industries;Intelligent Information Systems;Large language models (LLMs);Medical services;Requirements engineering;Responsible Artificial Intelligence;Software;User interfaces},
 pages = {1--8-1--8},
 doi = {10.1109/IISA59645.2023.10345880}
}


@inproceedings{Vito.,
 abstract = {As blockchain technology continues to evolve, the need for accessible solutions for developing smart contracts has grown, especially for non-technical users. This paper addresses practitioners' challenges in generating Solidity smart contracts from natural language requirements within the AstraKode Blockchain no-code platform (AKB). Our goal is to lower the barrier of entry into smart contract development, making it more accessible to users with limited technical expertise. We propose three methods, i.e., Naive Generation, Augmented Generation, and Enhanced Generation, each utilizing large language models to streamline the code generation process. These methods cater to different user needs, from rapid prototyping to handling complex business scenarios, improving accessibility and usability within AKB. We demonstrate their practical relevance, potential, and limitations in addressing real-world challenges in smart contract development through empirical evaluations and practitioner feedback. Thanks to collaboration with academia and effective knowledge transfer, these methods provide innovative solutions to the challenges of smart contract generation. Furthermore, they have been integrated into AKB to enhance user services, ultimately promoting the development and deployment of secure and efficient smart contracts in the industry.},
 author = {de Vito, Gabriele and D'Amici, Damiano and Izzo, Fabiano and Ferrucci, Filomena and {Di Nucci}, Dario},
 title = {LLM-Based Generation of Solidity Smart Contracts from System Requirements in Natural Language: The AstraKode Case},
 keywords = {Blockchains;Code Generation;Codes;Industries;Knowledge transfer;Large Language Model;Large language models;Natural languages;No-Code Platform;Rapid prototyping;Smart contracts;Software;Usability},
 pages = {170--180},
 doi = {10.1109/SANER64311.2025.00024}
}


@inproceedings{Vito.b,
 abstract = {This paper introduces AGORA, an innovative approach that leverages Large Language Models to automate the definition of acceptance test cases from use cases. AGORA consists of two phases that exploit prompt engineering to 1) identify test cases for specific use cases and 2) generate detailed acceptance tests cases. AGORA was evaluated through a controlled experiment involving industry professionals, comparing the effectiveness and efficiency of the proposed approach with the manual method. The results showed that AGORA can generate acceptance test cases with a quality comparable to that obtained manually but improving the process efficiency by over 90{\%} in a fraction of the time. Furthermore, user feedback indicated high satisfaction with using the proposed approach. These findings underscore the potential of AGORA as a tool to enhance the efficiency and quality of the software testing process.},
 author = {de Vito, Gabriele and Vassallo, Gabriele and Palomba, Fabio and Ferrucci, Filomena},
 title = {AGORA: An Approach for Generating Acceptance Test Cases from Use Cases},
 keywords = {automated software engineering;Collaboration;Guidelines;Industries;Large language models;Natural languages;Prompt engineering;Refining;Software development management;Software engineering;Software testing;User Acceptance Testing;Writing},
 pages = {126--133},
 doi = {10.1109/SEAA64295.2024.00027}
}


@inproceedings{Usha.,
 abstract = {One of the difficult task for many users on SQL is to write the SQL Query due to its syntax and structure. If a person needs to query a database, they should know everything about how data is distributed and what the internal dependencies are. For this reason, it is not easy for everyone to access data in database without proper knowledge. This paper presents a novel application that leverages generative AI and natural language processing (NLP) to enable users to interact with databases using natural language queries. Built on the Gemini API, the application translates user queries into SQL queries, simplifying database interactions for non-technical users. The Python-based backend, SQLite database management, and Streamlit frontend provide a comprehensive solution for database querying and analysis. This approach democratizes data retrieval and analysis, offering automated insights and visualizations to users of all skill levels. The app also features automated data analysis, which boosts insight generation for users of all skill levels. Further, the traditional ways of querying SQL generally require specialized knowledge and are only accessible to those with technical backgrounds. When the application takes charge of the query construction process and offers data as UI elements, it can invigorate users to have a higher degree of insight into what their data actually is and explore it even more efficiently. The Python software stack combines Python for backend processing, SQLite for database management and a web-based frontend for user interaction to provide an all-encompassing database querying and analysis solution.},
 author = {Usha, V. and Abhinash, Nalagarla Chiru and Chowdary, Sakhamuri Nitin and Sathya, V. and Reddy, Eeda Ramakrishna and {Sathiya Priya}, S.},
 title = {Enhanced Database Interaction Using Large Language Models for Improved Data Retrieval and Analysis},
 keywords = {Data Retrieval;Databases;Deep Learning;Distributed databases;Gemini API;Generative AI;Google LLM;Internet of Things;Large Language Model;Large language models;Machine Learning;Natural language processing;Prompt engineering;Python;SQLQuery Generation;Structured Query Language;Syntactics;Text to SQL Generation;User experience},
 pages = {1302--1306},
 doi = {10.1109/ICoICI62503.2024.10696623}
}


@inproceedings{Mahmud.b,
 abstract = {One of the most important tasks related to managing bug reports is localizing the fault so that a fix can be applied. As such, prior work has aimed to automate this task of bug localization by formulating it as an information retrieval problem, where potentially buggy files are retrieved and ranked according to their textual similarity with a given bug report. However, there is often a notable semantic gap between the information contained in bug reports and identifiers or natural language contained within source code files. For user-facing software, there is currently a key source of information that could aid in bug localization, but has not been thoroughly investigated - information from the graphical user interface (GUI). In this paper, we investigate the hypothesis that, for end user-facing applications, connecting information in a bug report with information from the GUI, and using this to aid in retrieving potentially buggy files, can improve upon existing techniques for text retrieval-based bug localization. To examine this phenomenon, we conduct a comprehensive empirical study that augments four baseline text-retrieval techniques for bug localization with GUI interaction information from a reproduction scenario to (i) filter out potentially irrelevant files, (ii) boost potentially relevant files, and (iii) reformulate text-retrieval queries. To carry out our study, we source the current largest dataset of fully-localized and reproducible real bugs for Android apps, with corresponding bug reports, consisting of 80 bug reports from 39 popular open-source apps. Our results illustrate that augmenting traditional techniques with GUI information leads to a marked increase in effectiveness across multiple metrics, including a relative increase in Hits@10 of 13-18{\%}. Additionally, through further analysis, we find that our studied augmentations largely complement existing techniques, pushing additional buggy files into the top-10 results while generally preserving top ranked files from the baseline techniques.},
 author = {Mahmud, J. and de Silva, N. and Khan, S. A. and Mostafavi, S. H. and Mansur, S. H. and Chaparro, O. and Marcus, A. and Moran, K.},
 title = {On Using GUI Interaction Data to Improve Text Retrieval-Based Bug Localization},
 keywords = {Bug Localization;Computer bugs;GUI;Location awareness;Measurement;Mobile Apps;Natural language processing;Natural languages;Semantics;Software;Source coding},
 pages = {466--478-466--478},
 doi = {10.1145/3597503.3608139}
}


@inproceedings{Maeda.,
 abstract = {Designing service robots capable of tidying up in unfamiliar and dynamic human environments presents a significant challenge. Such robots must not only recognize and manipulate a wide range of objects but also align their actions with tidying up rules, which may vary greatly from one individual to another. To address these challenges, we propose a comprehensive software framework that integrates Large Language Model (LLM) and Vision-Language Models (VLMs) for service robots. Our framework enables robots to learn human-specific tidying up rules through interaction and observation, and to identify and handle previously unseen objects and receptacles. This adaptive framework offers a unified solution for recognizing, learning, and acting upon diverse and dynamic human environments. We evaluate our framework using both a text-based benchmark dataset to assess tidying up rule learning and a simulated environment to demonstrate practical tidying up performance. In the evaluation using the text-based benchmark dataset, our framework selects appropriate receptacles for unseen objects with high accuracy (87.4{\%}), including unseen receptacle categories. The simulation evaluation confirms the effectiveness of our framework in realistic environments and scenarios. This research advances the field of service robotics by presenting an integrated software solution that leverages LLM and VLMs for more personalized and adaptable robot behavior in real-world tasks.},
 author = {Maeda, Ryuichi and Baselizadeh, Adel and Watanabe, Shin and Kurazume, Ryo and Torresen, Jim},
 title = {Adaptive Tidying Robots: Learning from Interaction and Observation},
 keywords = {Accuracy;Adaptation models;Benchmark testing;Large language models;Robot learning;Search problems;Service robots;Software;System integration;User experience},
 pages = {185--192},
 doi = {10.1109/SII59315.2025.10871003}
}


@inproceedings{Lohar.,
 abstract = {Log file analysis has a critical role in monitoring and maintaining software systems, yet the manual inspection of logs becomes increasingly impractical with the growing volume of data. This survey paper explores recent advancements in automated log file analysis, with a particular focus on the integration of AI techniques., which includes ML models and NLP. The study identifies key challenges in traditional methods, such as the need for human interpretation., difficulty in detecting new errors, and issues in backtracking within continuous data streams. Moreover, we examine state-of-the- art AI approaches., like LLaMA 2, to streamline log analysis by automating error detection., summarization, and anomaly identification. Research deficiencies are identified, notably the necessity for advanced methodologies to manage variety of log formats, automated model optimization, and ongoing learning processes. This comprehensive review endeavors to provide a thorough examination of the current landscape, encompassing perspectives on potential outcomes and prospective trajectories in the domain of AI -enhanced log file analysis. In addition to providing insights into prospective solutions and future directions in AI-driven log file analysis, this study attempts to give an in-depth analysis of the current situation.},
 author = {Lohar, Prerna and Baraskar, Trupti},
 title = {Automated AI Tool for Log File Analysis},
 keywords = {Artificial intelligence;Error Detection;LLM;Log Analysis;Log Parsing;Manuals;Mobile computing;Monitoring;Optimization;Reviews;Software systems;Streams;Surveys;Trajectory},
 pages = {1762--1766},
 doi = {10.1109/ICMCSI64620.2025.10883511}
}


@inproceedings{Charoenthanakitkul.,
 abstract = {This study presents a comprehensive framework for malware detection that integrates traditional machine learning algorithms with advanced large language models (LLMs) to enhance both classification accuracy and automated report generation. The proposed approach leverages three widely used machine learning models-Random Forest, XGBoost, and LightGBM-to classify different types of malwares based on extracted features from the EMBER dataset, a well-known benchmark dataset for malware analysis. Each of these models was trained and evaluated to determine their effectiveness in identifying malicious software with high accuracy. Among the tested algorithms, LightGBM exhibited the best performance, achieving an impressive classification accuracy of 83{\%}. XGBoost followed closely with 81{\%}, while Random Forest achieved an accuracy of 79{\%}. To further enhance the usability of the system for cybersecurity analysts, the study incorporates a large language model, specifically LLaMA, which is accessed via an API. This model is employed to generate detailed, human-readable reports that provide insights into detected malware threats. By automating the reporting process, the framework aims to reduce the manual effort required for malware analysis, allowing security professionals to focus on high-priority threats. Future research will focus on improving classification performance through advanced feature engineering techniques and exploring deep learning-based neural network architectures to further enhance the accuracy and efficiency of malware detection and analysis.},
 author = {Charoenthanakitkul, Aticha and Viboonsang, Pranodnard and Kosolsombat, Somkiat},
 title = {Optimizing Malware Detection with Random Forest, XGBoost, LightGBM, and LLM-Reporting},
 keywords = {Accuracy;Analytical models;Computational modeling;Computer security;Cybersecurity;Feature extraction;Large language models;LightGBM;LLM;Machine learning algorithms;Malware;Malware Detection;Random Forest;Random forests;Training;XGBoost},
 pages = {1--5},
 doi = {10.1109/ICCI64209.2025.10987272}
}


@inproceedings{Chaudhary.,
 abstract = {This paper presents our experience developing a Llama-based chatbot for question answering about continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Our chatbot is designed to handle the specificities of CI/CD documents at Ericsson, employing a retrieval-augmented generation (RAG) model to enhance accuracy and relevance. Our empirical evaluation of the chatbot on industrial CI/CD-related questions indicates that an ensemble retriever, combining BM25 and embedding retrievers, yields the best performance. When evaluated against a ground truth of 72 CI/CD questions and answers at Ericsson, our most accurate chatbot configuration provides fully correct answers for 61.11{\%} of the questions, partially correct answers for 26.39{\%}, and incorrect answers for 12.50{\%}. Through an error analysis of the partially correct and incorrect answers, we discuss the underlying causes of inaccuracies and provide insights for further refinement. We also reflect on lessons learned and suggest future directions for further improving our chatbot's accuracy.},
 author = {Chaudhary, Daksh and Vadlamani, Sri Lakshmi and Thomas, Dimple and Nejati, Shiva and Sabetzadeh, Mehrdad},
 title = {Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson},
 keywords = {Accuracy;Chatbot-Enabled Software Engineering;Chatbots;Continuous integration;Continuous Integration and Continuous Delivery (CI/CD);Feedback loop;Large language models (LLMs);Production;Prototypes;Question answering (information retrieval);Retrieval-Augmented Generation (RAG);Software maintenance;Telecommunications;Usability},
 pages = {707--718},
 doi = {10.1109/ICSME58944.2024.00075}
}


@inproceedings{Chen.,
 abstract = {This study addresses the growing demand for effective childcare support amid increasing challenges dual-income households face. We propose a mobile application designed to alleviate parental burdens by transforming routine tasks, such as dressing, tidying, and meal times, into engaging activities for children. Our goal is to foster children's independence and self-reliance by making these tasks enjoyable through game-like interfaces, thereby reducing parental stress and workload. The application will leverage cutting-edge artificial intelligence to balance automated processes with human input, ensuring a user-friendly experience for parents and children. Through this dual approach, the application aims to enhance family quality of life by facilitating efficient, enjoyable childcare experiences. Our research explores the innovative integration of AI within childcare solutions, highlighting its strengths and compensating for its limitations to achieve a balanced, effective platform.},
 author = {Chen, Sinan and Kihara, Taiju and Abe, Mako and Abe, Shiryuu},
 title = {Design and Development of a Web Application for Childcare Assistance Using AI-Code Generation},
 keywords = {AI-code generation;assistive technology;childcare;Image recognition;Large language models;Mobile applications;Python;Security;Software;Stress;Training;Transfer learning;Usability;Web application},
 pages = {361--365},
 doi = {10.1109/ICCC62609.2024.10942261}
}


@inproceedings{Chen.b,
 abstract = {The aim of this study was to develop a mobile application for psychotherapy with insomnia patients using the ChatGLM-LoRA model, fine-tuned by Low-Rank Adaptation, and validated in a clinical trial.The dataset used to train the model was a collection of 764 dialogues related to sleep disorders. The corpus was randomly divided into three subsets: training, validation, and test sets. The hyperparameters used in this study to train the model were 450 epochs, betas ranging from 0.9 to 0.95, weight decay rate 5e-4, maximum learning rate 1e-5, and AdamW optimizer. Based on the test results of the above hyperparameters, the four metrics of BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L of the model reached 0.0340, 0.0451, and 0.0163; 0.2773, 0.3075, and 0.1986; 0.0592, 0.0735, and 0.0261; 0.2112, 0.2336, and 0.1500 for the training, validation, and test sets.These results indicate the technical feasibility and potential clinical utility of using an advanced language model-based application for psychotherapeutic intervention in insomnia.},
 author = {Chen, Y. and Pan, S. and Xia, Y. and Ren, K. and Zhang, L. and Mo, Z. and Chen, J. and Zhang, M. and Li, H. and Shuai, J. and Xia, Q. and Yu, R.},
 title = {A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy},
 keywords = {Adaptation models;application (APP);Distance measurement;Large Language Model (LLM);Large language models;Low-Rank Adaptation (LoRA);Mechatronics;Medical treatment;Random access memory;sleep disorders;Training},
 pages = {360--367-360--367},
 doi = {10.1109/CIS-RAM61939.2024.10673360}
}


@inproceedings{Chopade.,
 abstract = {The goal of the project is to automate the conversion of pseudocode into visually appealing flowcharts, facilitating the transition from human-readable algorithm descriptions to machine-executable code in software. This study presents a new approach that uses the OpenAI GPT-3.5 Turbo model for automatic flowchart generation. Addressing the lack of tools that understand pseudocode, the method interprets pseudocode, generates executable code fragments, and creates aesthetically pleasing flowcharts. This article reviews the literature and highlights a gap in automated flow charts. The selected methodology uses state-of-theart language models, with a special focus on natural language processing (NLP). The study aims to improve the understanding of algorithms by visually representing complex algorithms through automatic flowchart generation.Examples 1. Sorting rhythm: Consider a scenario where a complex sorting algorithm is described in pseudocode. The flowchart generator interprets the pseudocode, generates the corresponding executable code, and converts it into a complete flowchart. This visual representation helps developers understand algorithm logic, encourages discussion, and improves collaboration.2. Recursive algorithms and tree structures: The project extends the application to handle recursive algorithms and tree structures. For example, when a flowchart generator is provided with recursive pseudocode to traverse a binary tree, it smoothly transforms the abstract description into a detailed flowchart. This feature increases the versatility of the toolbox and offers a wide range of algorithmic models.3. Optimization Algorithm Performance: The flowchart generator not only helps to understand but also to improve algorithms by identifying potential bottlenecks and optimizing the generated code. Critical decision points and loops in the flowchart visually highlight key areas and provide developers with valuable information to make informed optimization decisions.},
 author = {Chopade, A. and Shingde, V. and Chavare, A. and Bhagwat, T.},
 title = {Code Insight - Flowchart Generator},
 keywords = {algorithm design;algorithm efficiency;Codes;collaborative development;flowchart;Flowcharts;Generators;GPT-3.5 turbo;Natural language processing;pseudocode interpretation;serialization;Software algorithms;Transforms;tree structures;Visualization},
 pages = {1--6-1--6},
 doi = {10.1109/IC457434.2024.10486526}
}


@inproceedings{Choudhuri.,
 abstract = {Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity-variations in users' cognitive styles-that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice? We developed a theoretically grounded statistical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools in their work. We surveyed software developers (\mathrm{N}{=}{2}38) at two major global tech organizations: GitHub Inc. and Microsoft; and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools in their work. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience.},
 author = {Choudhuri, Rudrajit and Trinkenreich, Bianca and Pandita, Rahul and Kalliamvakou, Eirini and Steinmacher, Igor and Gerosa, Marco and Sanchez, Christopher and Sarma, Anita},
 title = {What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards Genai},
 keywords = {Behavioral Intentions;Cognitive Styles;Generative AI;LLM;Maintenance;Mathematical models;PLS-SEM;Productivity;Refining;Software;Software development management;Software engineering;trust;Usability;User experience},
 pages = {1691--1703},
 doi = {10.1109/ICSE55347.2025.00087}
}


@inproceedings{Chu.,
 abstract = {An AI agent is an intelligent framework that can perceive, plan and decompose, perform actions, and reflect. The multi-agent mode is a distributed computing mode that collaborates to complete different tasks in a group of AI intelligent agent environments. After discussion, inspection, reasoning, and evaluation, accurate conclusions are drawn. Multi AI intelligent agents play a crucial role in enhancing the performance of large models. Research and optimization of multi-agent architecture provide more perspectives and insights for system architecture design. In network planning, there are various application scenarios for reviewing planning template documents. This paper studies a multi-agent architecture based on open-source large language models, which is applied to planning document templates and data review to enhance the efficiency of processing and analyzing.},
 author = {Chu, Weiyan and Yin, Sitan and Huang, Lei and Lin, Ling and Wang, Xiaodong and Zhang, Zhi and Li, Hongwu},
 title = {Verify-Agent: Large Language Model Multi-Agent for Intelligent Verification},
 keywords = {Computer architecture;framework;Intelligent agents;intelligent verification;Large language models;multi-agent;Multi-agent systems;Optimization;paradigm;performance;Planning;Reviews;Systems architecture;Ubiquitous computing;Usability;verify-agent},
 pages = {374--379},
 doi = {10.1109/IUCC65928.2024.00072}
}


@inproceedings{Chaplia.,
 abstract = {This paper presents a cloud-based system for microservice code analysis that utilizes LLM agents as a foundation. The system is designed to receive the code updates, analyze the code, and provide results to the user. Users can obtain information about the microservice, code summaries, code reviews, reliability checks, and improvement recommendations. The history of changes allows for tracking the evolution of microservices and identifying the issues when they appear. The prototype of the proposed system was implemented and tested. The results show the systems' usability and provide valuable insights. This research underscores the potential of combining LLM agents with cloud technologies, offering a scalable microservice solution that paves the way for future innovations in automated code analysis and software engineering.},
 author = {Chaplia, Oleh and Klym, Halyna},
 title = {Cloud-Based System for Source Code Analysis of Microservices with LLM Agents},
 keywords = {Artificial intelligence;Cloud computing;Codes;llm agents;Microservice architectures;microservices;Software architecture;Software reliability;Source coding;Standards;Technological innovation;Testing;Text summarization;Usability},
 pages = {1--4},
 doi = {10.1109/CSIT65290.2024.10982613}
}


@inproceedings{Corazza.,
 abstract = {When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct--vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model--a mathematical abstraction of the software system--which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we ``fill in the blanks'' using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.},
 author = {Corazza, Jan and Gavran, Ivan and Moreira, Gabriela and Neider, Daniel},
 title = {Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs},
 keywords = {Blockchains;Code Generation;Codes;Formal Methods;Large language models;Maintenance engineering;Mathematical models;Model Synthesis;Model-Based Techniques;Smart contracts;Software;Software Auditing;Syntactics;Trustless services;User experience},
 pages = {542--552},
 doi = {10.1109/ICST62969.2025.10989026}
}


@inproceedings{Ding.,
 abstract = {Over the course of the past several years, Natural Language Processing (NLP) and machine learning researchers have grown more interested in the subject of Intelligent Tutoring Systems (ITS).The primary purpose of ITS is to facilitate language learning by facilitating voice interaction via speech recognition and speech synthesis technologies and evaluating learners' oral expression using language models. In this study, we examine the history of ITS's evolution as well as its present research state and potential future possibilities. First, a brief overview of ITS's past development is provided, followed by an examination of its primary application domains. Second, a discussion of some of the most important technologies utilized by ITS is presented. These technologies include voice recognition, speech synthesis, language modeling, conversation management, and learning management. In the final part of this article, we will go over some of the next developments in ITS, such as cross-language learning, multimodal interaction, and tailored learning.},
 author = {Ding, Y.},
 title = {Development of Intelligent Tutors Based on Dialogue Systems},
 keywords = {Customer services;dialogue systems;Education;Foreign language;Machine Learning;model;Natural language processing;Oral communication;Speech recognition;technology;User experience},
 pages = {927--930-927--930},
 doi = {10.1109/SSAIC61213.2024.00187}
}


@inproceedings{Djajadi.,
 abstract = {Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.},
 author = {Djajadi, Natanael and Deljouyi, Amirhossein and Zaidman, Andy},
 title = {Using Large Language Models to Generate Concise and Understandable Test Case Summaries},
 keywords = {Automated Test Generation;Documentation;Large language models;Readability;Software;Software testing;Test pattern generators;Unit Testing},
 pages = {322--326},
 doi = {10.1109/ICPC66645.2025.00040}
}


@inproceedings{Dong.,
 abstract = {The escalating frequency and increasing complexity of cyber attacks underscore the importance of Cyber Threat Intelligence (CTI). Tactics, Techniques, and Procedures (TTPs), as advanced CTI capable of characterizing adversarial behaviors and intentions, have garnered increased attention. However, TTPs are predominantly found embedded within unstructured natural language texts of threat reports. The accurate extraction and standardization of TTPs pose significant challenges. Existing methods exhibit limitations in terms of accuracy, generalizability, and interpretability. This paper presents a pipeline for automatically extracting TTPs from threat reports and providing rationales using large language models. To support this approach, we have developed three datasets using advanced commercial LLMs for data synthesis. These datasets are made publicly available to facilitate further research. Experimental results demonstrate the superior performance of our proposed approach, achieving an F1-score of 97.15{\%} and accuracies of 79.22{\%} and 92.97{\%} in the respective tasks. These results surpass state-of-the-art methods by 15.39{\%}, 12.87{\%}, and 27.57{\%}, respectively. To the best of our knowledge, this paper is the first to simultaneously extract TTPs while providing the underlying rationales for the extraction. This novel approach significantly improves the usability of the results by providing a richer context for threats.},
 author = {Dong, Fangming and Jiang, Zhengwei and Ma, Chunyan and He, Qiying and Yang, Peian and Yao, Yepeng and Wang, Jian},
 title = {From Threat Report to ATT{\&}CK: Automated Extraction and Reasoning of TTPs Using Large Language Models},
 keywords = {Accuracy;Cyber threat intelligence;Data mining;Federated learning;Large Language Model;Large language models;MITRE ATT{\&}CK;Natural languages;Pipelines;Standardization;Synthetic data;TTP Extraction;Usability},
 pages = {860--865},
 doi = {10.1109/CSCWD64889.2025.11033281}
}


@inproceedings{Du.,
 abstract = {Generation-based conversational recommender systems (CRSs) are tailored to individual user needs, employing cutting-edge text generation techniques for generating contextually relevant and fluent responses. Nevertheless, current CRSs, including advanced general language models like ChatGPT, often overlook the subtle differences in users' linguistic styles and the evolving nature of their interests, which leads to a tendency for uniform responses at the onset of dialogues and unnecessarily prolonged interaction cycles. To overcome these challenges, our approach leverages user historical data and information from similar user profiles to more accurately capture the user linguistic feature within dialogue subtasks. We introduce an innovative multi-factor cross-attention mechanism, specifically designed for the efficient integration of diverse quantitative relationships. In the recommendation subtask, our strategy involves the intuitive representation of user interest migration trajectories and the stereotype. This not only allows for a more granular understanding of the trajectory of user interests but also alleviates data sparsity for new users. Our proposed framework, User Linguistic Style Awareness and Interest-Driven CRSs (LSAID), has undergone extensive testing on well-established CRS datasets such as ReDial and INSPIRED. The empirical results underscore LSAID's effectiveness, showcasing its state-of-the-art performance. Our code will be released at http://github.com/zhizhaixingchen/LSAID.},
 author = {Du, Hongkai and Li, Haozhou and Peng, Qinke and Fu, Laiyi},
 title = {User Linguistic Style Awareness and Interest-Driven Conversational Recommender Systems},
 keywords = {Chatbots;Codes;conversational recommender systems;Digital twins;interest-driven;Large Language Model;Large language models;Linguistics;prompt learning;Recommender systems;Testing;Trajectory;user personality},
 pages = {215--220},
 doi = {10.1109/DTPI61353.2024.10778861}
}


@inproceedings{DurgaprasadK.V.V.B..,
 abstract = {The development of artificial intelligence (AI) agents represents a significant technological advancement characterized by autonomous software programs capable of executing diverse tasks with human-like intelligence. This study investigates the creation of AI agents that integrate state-of-the-art capabilities for decision-making and natural language interaction. Drawing on the fundamental concepts of AI agents, this research endeavors to enhance their functionality by seamlessly incorporating real-time Internet data into decision-making processes and implementing contextually aware conversation systems. The proposed AI agents were designed as independent entities capable of learning from data, making decisions, and executing actions without the need for direct human intervention. By leveraging language models (LLMs), these agents process vast amounts of data and continuously adapt and improve their performance over time. The integration of real-time Internet data enables agents to adjust their decision-making processes dynamically, ensuring prompt and well-informed responses to changing information. Contextually aware conversation systems empower these agents to engage in natural and personalized interactions, thereby fostering heightened user satisfaction and engagement. An intriguing aspect of AI agents is their capacity to learn and adapt to new situations without explicit programming in each unique scenario. Through continuous learning and interaction with the environment, AI agents have evolved to handle complex tasks efficiently and accurately. They can analyze patterns in large datasets, providing valuable insights and predictions across various industries, including healthcare, finance, and transportation. This research underscores a commitment to innovation, addressing existing limitations in AI technology and exploring its potential to advance artificial intelligence in practical business applications. By bridging the gap between traditional AI capabilities and emerging business needs, this research aims to contribute significantly to the development and implementation of intelligent AI agents in real-world scenarios.},
 author = {{Durgaprasad, K. V. V. B.} and Abozibid, Hassan Khalid and {Nasri Hawas}, Jawdat and {Sai Krishna}, Pusala Bhuvan and {Bhavya Sri}, Kanumuri and Reddy, Gali Pranay},
 title = {AI Agents and Conversation System},
 keywords = {AI Agents;Artificial intelligence;Automation;Business;Decision making;Generative AI;Industries;Internet;Large language models (LLMs);Natural languages;Oral communication;Pattern Recognition;Real-time systems;Transfer learning;Transformer Architecture;Transportation},
 pages = {1--7},
 doi = {10.1109/ARIIA63345.2024.11051848}
}


@inproceedings{Duvvuru.,
 abstract = {Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose Autosimtest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that Autosimtest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.},
 author = {Duvvuru, Venkata Sai Aswath and Zhang, Bohan and Vierhauser, Michael and Agrawal, Ankit},
 title = {LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems},
 keywords = {ai for se;Autonomous aerial vehicles;Manuals;Planning;Simulation;Simulation Testing;Software engineering;sUAS;Surveillance;Testing;Urban areas;Usability;Wind},
 pages = {385--397},
 doi = {10.1109/ICSE55347.2025.00223}
}


@inproceedings{Ellahi.,
 abstract = {In the ever-growing field of Generative Artificial Intelligence, quick engineering has become a revolutionary method in the field of NLP for big language versions. This technique entails the manipulation of input queries to improve the quality and relation of the textual outputs of these models. New findings point to the fact that through the enhancement of the aspect of prompt engineering it could indeed be a way of significantly boosting LLM performance through a change of the input query format. This paper highlights the practical use of the prompt engineering concept to Tamil-based LLMs with an explicit purpose of developing a well-structured process capable of generating accurate and contextually relevant conversational responses. Thus, with subtle changes to how prompts are offered, this research aims to enhance the efficacy of Tamil LLMs that demand trivial data entry. The research uses the Query Transformation Module (QTM), which is an elaborate method developed to systematically change the input prompts into three forms of inquiries. Therefore, each format is built with objectives and key points in mind to enhance the understanding of the models as well as the results. To measure the effectiveness of this strategy, the QTM was employed with leading Tamil LLMs SKT GPT-2 and Tamil ChatGPT. We employed four different query methods in our experiments: all the initially employed unaltered request and three modified requests according to the QTM. The evaluation was made using Google SSA to fit the evaluation criteria of naturalness and specificity for the sentences generated by the models. Our experimental findings show that overall, there is a reasonable improvement in the performance by an average of 11 percent. Identified that there is an increase of 46{\%} in the quality of generated sentences when using the transformed queries as compared to the unmodified prompts. This improvement also depicts how the QTM has an effectiveness of enhancing the performance of Tamil LLMs, resulting into its usability as a tool in the corresponding field of prompt engineering.},
 author = {Ellahi, Ehsan and {Krishna Priya}, R. and Talha, Muhammad and Hariprabhu, M. and Aragani, Venu Madhav and Tiwari, Mohit},
 title = {Optimizing the Performance of Generative Artificial Intelligence ,Recent Approaches to Engineering Large Language Models},
 keywords = {Adaptation models;AI;AI chatbots;Chatbots;Generative AI;Internet;Large language models;LLMs;Measurement;performance assessment of LLMs;Prompt engineering;QTM;Real-time systems;Tamil AI models;Training;Training data;Usability},
 pages = {1--6},
 doi = {10.1109/ICACCM61117.2024.11059049}
}


@inproceedings{Dhyani.,
 abstract = {Our study provides an improvement on the creation of Application Programming Interfaces (APIs) usage documentation using the efficiency and power of Generative AI. APIs play an important role in software integration and software maintenance but the process of API documentation creation has been traditional and did not evolve with time, this paper employs Generative AI to enhance the accuracy, speed, and scale of API documentation generation. The automated API documentation generator is created using natural language processing applied through a large language model (TinyPixel/Llama-2-7B-bf16-sharded model). Training data was created by applying web scraping on various large tech companies' documentation web pages to get a good quality and industry-standard documentation dataset. It was further diversified and increased using the GPT model to handle a wide range of API scenarios. The fine-tuning greatly enhanced the TinyPixel/Llama-2-7B-bf16-sharded model's efficiency and quality of output which is proven by the reduced response time and the accuracy of documentation generated. Our study's comparative study confirms the effectiveness of the approach used. Our study's conclusion offers a comprehensive approach that should improve software development processes and pave the way for additional developments in API documentation.},
 author = {Dhyani, P. and Nautiyal, S. and Negi, A. and Dhyani, S. and Chaudhary, P.},
 title = {Automated API Docs Generator using Generative AI},
 keywords = {API Documentation;Documentation;fine-tuning;Generative AI;Generators;Industries;Large language models;Natural language processing;Training data;User experience;Web pages;Web Scraping},
 pages = {1--6-1--6},
 doi = {10.1109/SCEECS61402.2024.10482119}
}


@article{Fakhoury.2024,
 abstract = {Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97{\%} in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.},
 author = {Fakhoury, S. and Naik, A. and Sakkas, G. and Chakraborty, S. and Lahiri, S. K.},
 year = {2024},
 title = {LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation},
 keywords = {Accuracy;Artificial intelligence;Benchmark testing;Code Generation;Codes;Cognitive load;Human factors;Intent disambiguation;LLMs;Natural languages;Python;Task analysis;Test Generation},
 pages = {2254--2268-2254--2268},
 volume = {50},
 number = {9},
 issn = {1939-3520},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2024.3428972}
}


@inproceedings{Chaaben.,
 abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
 author = {Chaaben, M. B. and Burgue{\~n}o, L. and Sahraoui, H.},
 title = {Towards using Few-Shot Prompt Learning for Automating Model Completion},
 keywords = {domain modeling;few-shot learning;language models;model completion;Natural languages;prompt learning;Semantics;Software engineering;Task analysis;Usability},
 pages = {7--12-7--12},
 doi = {10.1109/ICSE-NIER58687.2023.00008}
}


@inproceedings{Bouzenia.,
 abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
 author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
 title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
 keywords = {ai for se;Autonomous agents;Computer bugs;Large language models;llm agents;Maintenance engineering;Pricing;program repair;Reliability;Software;Software engineering;Translation;User experience},
 pages = {2188--2200},
 doi = {10.1109/ICSE55347.2025.00157}
}


@inproceedings{Aberkane.,
 abstract = {Large Language Models (LLMs) have significantly impacted various industries, offering enhanced processes and decision-making capabilities. Among these models, ChatGPT, an intelligent conversational agent, has found applications in multiple fields, including software development. This study explores how ChatGPT can benefit requirements engineering (RE), specifically in facilitating compliance with the General Data Protection Regulation (GDPR) principle of data protection by design and default. In particular, our research objective is to evaluate ChatGPT's ability to support professionals in assessing GDPR compliance within user stories, thus enhancing requirement quality. We obtain insights into ChatGPT's strengths and limitations in this context through an experiment and survey pilot study with experts. The experiment's outcome suggests that respondents generally agreed with ChatGPT's evaluation of user stories on GDPR compliance, agreeing with almost 73{\%} of ChatGPT's evaluations. Furthermore, the survey results showed reservations regarding recommending ChatGPT to peers or colleagues, suggesting a cautious stance within the professional community. This study provides insights into ChatGPT's utility as an assistive tool for GDPR compliance in RE, presenting a starting point to address GDPR challenges in RE using LLMs.},
 author = {Aberkane, Abdel-Jaouad and Broucke, Seppe vanden and Poels, Geert and Georgiadis, Georgios},
 title = {Leveraging ChatGPT for GDPR Compliance in Requirements Engineering: A Pilot Study},
 keywords = {Chatbots;data protection by design and by default;Decision making;General Data Protection Regulation;Industries;Large language models;Requirements engineering;Security;Software development management;Surveys},
 pages = {34--41},
 doi = {10.1109/SpaCCS63173.2024.00012}
}


@inproceedings{AbuArqoub.,
 abstract = {This article presents a comprehensive study on designing and implementing a dashboard for enhanced data visualization and query support at the University of Petra. The system leverages retrieval-augmented generation (RAG) and large language models (LLMs) to support diverse document types, including curricula, course descriptions, and program outcomes, alongside intended learning outcome (ILO)-related files. Our implementation demonstrates significant improvements in data accessibility and query response times while maintaining high accuracy in information retrieval and visualization. Through extensive evaluation, we show that this innovative approach transforms data management processes in higher education by enabling natural language interactions with educational data systems, building upon established business intelligence frameworks while introducing advanced AI capabilities.},
 author = {Abu-Arqoub, Mohammad and {Alkarim Banna}, Abed and El-Khalili, Nuha and {Al-Shaikh Hasan}, Mohammad},
 title = {Design and Implementation of a Comprehensive RAG-Driven Dashboard Within the ILO System for Data Visualization and Query Support: A Case Study of the University of Petra},
 keywords = {Business intelligence;Data systems;Data visualization;Educational technology;ILO System;Information retrieval;Large language models;Learning Analytics;Natural languages;RAG;Retrieval augmented generation;Time factors;Transforms},
 pages = {1--5},
 doi = {10.1109/ICCIAA65327.2025.11013578}
}


@inproceedings{Abukadah.,
 abstract = {Efficiently navigating through mobile applications to accomplish specific tasks can be time-consuming and challenging, particularly for users who are unfamiliar with the app or faced with intricate menu structures. Simplifying access to a particular screen is a shared user priority, especially for individuals with diverse needs, including those with specific accessibility requirements. This underscores the exploration of innovative solutions to streamline the navigation process. This work addresses the challenge of mapping natural language intents to user interfaces, with a specific focus on the context of mobile applications. The primary objective of this work is to provide users with a more intuitive and efficient method for accessing desired screens in mobile applications by expressing their intentions in natural language. Existing approaches to this task have relied heavily on qualitative human studies for evaluating the performance. Moreover, widely used pre-trained vision-language models, such as Contrastive Language-Image Pretraining (CLIP), struggle to generalize effectively to the unique visual characteristics of user interfaces. Acknowledging the limitations, we introduce a novel approach that harnesses the power of the pre-trained vision-language models. Specifically, we investigate whether fine-tuning pre-trained vision-language models on mobile screens can address the challenges posed by the intricate nature of mobile application interfaces. Our approach involves the utilization of state-of-the-art pre-trained text and image encoders and employing a supervised fine-tuning process, where pre-trained models are adapted to the specific needs of mobile screen interactions. Moreover, a shared embedding space facilitates the alignment of embeddings of both text and image modalities, fostering a cohesive understanding between the natural language intents and visual features of user interface elements. We conduct extensive experimental evaluations using the Screen2Word dataset. Through systematic analysis and established metrics, we examine the models' ability to accurately map diverse linguistic intents to specific user interfaces. Our analysis demonstrates that fine-tuning yields substantial improvements over the zero-shot performance of the pre-trained vision-language models.},
 author = {Abukadah, H. and Fereidouni, M. and Siddique, A. B.},
 title = {Mapping Natural Language Intents to User Interfaces through Vision-Language Models},
 keywords = {Adaptation models;Computational modeling;Measurement;Natural language processing;Navigation;pre-trained vision-language models;Systematics;user interface navigation;User interfaces;Visualization},
 pages = {237--244-237--244},
 doi = {10.1109/ICSC59802.2024.00045}
}


@inproceedings{AlarioHoyos.,
 abstract = {Students frequently rely on chatbots powered by generative Artificial Intelligence (GenAI), such as ChatGPT, Copilot, Gemini, and Claude, to assist with a wide range of academic tasks. However, these chatbots are not specifically designed for the context of particular courses, which can lead to responses that are sometimes inaccurate or insufficiently relevant. This paper introduces a chatbot specifically designed to support first-year engineering students in a Java programming course. Developed using the Retrieval-Augmented Generation (RAG) technique, the chatbot draws on course-specific resources such as videos, quizzes, programming exercises, and other materials, while using OpenAI's Large Language Models (LLMs) GPT-4 and GPT-3.5 for information analysis and response generation. The data collected, consisting of logs from 1,059 messages sent by students to the chatbot and 30 responses to a survey, indicate that students primarily used the chatbot to clarify concepts and explain code snippets. Moreover, most of the students reported that the responses provided by the chatbot were well suited to the Java programming course.},
 author = {Alario-Hoyos, Carlos and Kemcha, Rebiha and Kloos, Carlos Delgado and Callejo, Patricia and Est{\'e}vez-Ayres, Iria and Sant{\'i}n-Crist{\'o}bal, David and Cruz-Argudo, Francisco and L{\'o}pez-S{\'a}nchez, Jos{\'e} Luis},
 title = {Tailoring Your Code Companion: Leveraging LLMs and RAG to Develop a Chatbot to Support Students in a Programming Course},
 keywords = {and Programming Course;Chatbots;Codes;Generative Artificial Intelligence (GenAI);Information analysis;Java;Large language models;Large language models (LLMs);Learning (artificial intelligence);programming;Retrieval augmented generation;Retrieval-Augmented Generation (RAG);Surveys;Videos},
 pages = {1--8},
 doi = {10.1109/TALE62452.2024.10834365}
}


@inproceedings{Alian.,
 abstract = {End-to-end (E2E) testing is essential for ensuring web application quality. However, manual test creation is time-consuming, and current test generation techniques produce incoherent tests. In this paper, we present Autoe2e,a novel approach that leverages Large Language Models (LLMs) to automate the generation of semantically meaningful feature-driven E2E test cases for web applications. Autoe2eintelligently infers potential features within a web application and translates them into executable test scenarios. Furthermore, we address a critical gap in the research community by introducing E2EBENCH, a new benchmark for automatically assessing the feature coverage of E2E test suites. Our evaluation on E2EBENCH demonstrates that Autoe2eachieves an average feature coverage of 79{\%}, outperforming the best baseline by 558 {\%}, highlighting its effectiveness in generating high-quality, comprehensive test cases.},
 author = {Alian, Parsa and Nashid, Noor and Shahbandeh, Mobina and Shabani, Taha and Mesbah, Ali},
 title = {Feature-Driven End-to-End Test Generation},
 keywords = {Benchmark testing;End-to-End Testing;Feature Inference;Large language models;Manuals;Software engineering;Test pattern generators;Translation},
 pages = {450--462},
 doi = {10.1109/ICSE55347.2025.00141}
}


@article{Almatrafi.2025,
 abstract = {Code duplication, commonly known as code cloning, is a persistent challenge in software development. While reusing code fragments boosts productivity, excessive cloning poses challenges to maintenance and elevates the risk of bugs. Therefore, integrating code clone detection into the development process is crucial. The extensive code-related knowledge inherent in Large Language Models (LLMs) renders them high-potential candidates for addressing diverse software engineering challenges. However, the effectiveness of LLMs in the specific task of code clone detection requires precise evaluation. This paper proposes an innovative methodology leveraging few-shot instruction-tuned GPT-3.5 Turbo and GPT-4 to detect code clones across all types, focusing on complex clones (Type-3 and Type-4). Unlike conventional approaches confined to specific language pairs or tasks, our method employs versatile language models, showcases generalization strengths for semantic understanding, and leverages instruction tuning with few-shot inference for task-specific adaptability in code clone detection. A conversational dataset was crafted from BigCloneBench for instruction tuning, enhancing task alignment and performance. This study evaluates the proficiency of LLMs in identifying code clones, analyzing the impact of instruction tuning, and assessing the efficiency across various clone types. Experimental results demonstrate these models achieving competitive performance against existing tools for overall and complex clone detection. Integration into an Integrated Development Environment (IDE) enables real-time detection and automated refactoring, bridging the gap between theoretical advancements and practical usability. This work highlights the potential of generalized LLMs setting a new standard in a field traditionally dominated by specialized tools and demonstrates their adaptability for complex challenges in code analysis and maintainability.},
 author = {Almatrafi, Afnan A. and Eassa, Fathy A. and Sharaf, Sanaa A.},
 year = {2025},
 title = {Code Clone Detection Techniques Based on Large Language Models},
 keywords = {Adaptation models;Clone detection;Cloning;code duplication;Codes;fine-tuning;instruction tuning;Large language models;Natural language processing;Real-time systems;Software engineering;Syntactics;Training;Transformers;Tuning},
 pages = {46136--46146},
 volume = {13},
 issn = {2169-3536},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2025.3549780}
}


@inproceedings{AshenShanuka.,
 abstract = {This research evaluates the capabilities of Large Language Models (LLMs) in generating CRUD applications using Python Flask framework, focusing on code quality, security, and UI design. The study analyzes five prominent LLMs: Claude 3.5 Sonnet, Gemini, GitHub Copilot, GPT-4, and Perplexity, through automated static code review tools including Code Factor, Codacy, and Code Scene. The evaluation reveals consistently high performance across models, with GitHub Copilot and Gemini achieving superior code health metrics (9.5-10.0) and 100{\%} green code ratings. While all models maintained Grade A ratings in Code Factor, common security vulnerabilities, particularly in Flask debug mode configuration, persisted across implementations. UI evaluation through professional interviews indicated that while generated interfaces were functionally complete, they lagged behind human-designed UIs in aesthetic appeal. The research demonstrates that LLMs can automate 60-90{\%} of CRUD development tasks, though human intervention remains essential for environment setup and security configuration. The findings suggest a shift toward human-AI hybrid development approaches, where entry-level developers can effectively manage LLM-generated code while focusing on strategic implementation decisions.},
 author = {{Ashen Shanuka}, K. A. and Wijayanayake, Janaka and Vidanage, Kaneeka},
 title = {Analyzing the impact of prompt engineering on efficiency, code quality, and security in CRUD application development},
 keywords = {Code quality;Codes;Collaboration;crud operations;Focusing;Large language models;Measurement;programming;Prompt engineering;Security;Software development management;Testing;User experience},
 pages = {1--6},
 doi = {10.1109/ICARC64760.2025.10963005}
}


@inproceedings{Bucaioni.,
 abstract = {The integration of large language models into software systems is transforming capabilities such as natural language understanding, decision-making, and autonomous task execution. However, the absence of a commonly accepted software reference architecture hinders systematic reasoning about their design and quality attributes. This gap makes it challenging to address critical concerns like privacy, security, modularity, and interoperability, which are increasingly important as these systems grow in complexity and societal impact. In this paper, we describe our emerging results for a preliminary functional reference architecture as a conceptual framework to address these challenges and guide the design, evaluation, and evolution of large language model-integrated systems. We identify key architectural concerns for these systems, informed by current research and practice. We then evaluate how the architecture addresses these concerns and validate its applicability using three open-source large language model-integrated systems in computer vision, text processing, and coding.},
 author = {Bucaioni, Alessio and Weyssow, Martin and He, Junda and Lyu, Yunbo and Lo, David},
 title = {A Functional Software Reference Architecture for LLM-Integrated Systems},
 keywords = {Computational modeling;Computer architecture;functional reference architecture;Large language models;LLMs;Natural language processing;Privacy;Security;Software architecture;Software reference architecture;Software systems;Systematics;Text processing},
 pages = {1--5},
 doi = {10.1109/ICSA-C65153.2025.00006}
}


@inproceedings{Babushkin.,
 abstract = {This paper explores methods for employee feedback (EF) collection and introduces a new framework to improve the process. Current feedback collection process often suffers from slow workflow, bias, and poor Artificial Intelligence (AI) integration. Large Language Models (LLM) combined with human oversight may provide a more agile, objective, and user-friendly way for collecting and analyzing feedback. Existing EF tools were evaluated based on criteria such as AI capabilities, usability, and costs. Building on these findings, an open-source and self-hosted EF tool was developed. The tool integrates AI at every stage of the process and offers real-time AI assistance in writing, summarizing, and interpreting feedback. Pilot testing in two tech companies demonstrated user satisfaction.},
 author = {Babushkin, Konstantin and Timofeev, Nikolai and Filianin, Ivan},
 title = {Research on AI-Powered Employee Feedback Tools},
 keywords = {artificial intelligence integration;Companies;Costs;employee feedback;employee performance evaluation;feedback collection;HR tool;Industries;Large Language Model;Large language models;Performance evaluation;Real-time systems;Surveys;Testing;Usability;Writing},
 pages = {127--132},
 doi = {10.1109/SmartIndustryCon65166.2025.10986092}
}


@inproceedings{Baravkar.,
 abstract = {Service registry, a key component of the service-oriented architecture (SOA), aids software developers in discovering services that meet specific functionality requirements. Recent years have witnessed the transition from the traditional service registries to its successor, the Service Marketplaces, which involves deeper engagement in the SOA software lifecycle and offers additional features, such as service request delegation and monitoring of services' Quality of Service (QoS). However, by analyzing developers' questions posted on online Q{\&}A forums, we found that many developers struggle with such transition, leading to development inefficiencies and even security vulnera-bilities. This paper presents the first empirical study aimed at uncovering the issues developers face with marketplaces, particularly those arising from the transition. Through a meticulous process of manually labeling and analyzing developers' questions, we develop a taxonomy of these issues, summarize the impacts caused by the transition, and provide actionable suggestions to App developers, service providers, and marketplaces. Utilizing the labeled questions and our insights, we fine-tune a Large Language Model (LLM) for providing answers to similar questions raised by developers and helping service providers and marketplaces extract useful information from these questions, such as service outages and key leakages. Our evaluation of the model's performance in answering and extracting pertinent information from a set of real-world questions demonstrates its effectiveness: it accurately classified 85 {\%} of the queries and successfully identified 88 {\%} of service names and 77 {\%} of key leakages. As the first empirical study in this domain, this work not only aids developers in navigating the transition more effectively but also sheds light on the under explored issue of service registry evolution, offering valuable insights for researchers.},
 author = {Baravkar, S. and Zhang, C. and Hassan, F. and Cheng, L. and Song, Z.},
 title = {Decoding and Answering Developers' Questions About Web Services Managed by Marketplaces},
 keywords = {Data mining;Large Language Model;Large language models;Navigation;Q{\&}A forums;Quality of service;Service Marketplace;Service-oriented architecture;Software;Taxonomy},
 pages = {194--203-194--203},
 doi = {10.1109/SSE62657.2024.00037}
}


@inproceedings{Baresi.,
 abstract = {Recent breakthroughs in Artificial Intelligence (AI) obfuscate the boundaries between digital, physical, and social spaces, a trend expected to continue in the foreseeable future. Traditionally, software engineering has prioritized technical aspects, focusing on functional correctness and reliability while often neglecting broader societal implications. With the rise of software agents enabled by Large Language Models (LLMs) and capable of emulating human intelligence and perception, there is a growing recognition of the need for addressing socio-critical issues. Unlike technical challenges, these issues cannot be resolved through traditional, deterministic approaches due to their subjective nature and dependence on evolving factors such as culture and demographics. This paper dives into this problem and advocates the need for revising existing engineering principles and methodologies. We propose a conceptual framework for quality assurance where AI is not only the driver of socio-critical systems but also a fundamental tool in their engineering process. Such framework encapsulates pre-production and runtime workflows where LLM-based agents, so-called artificial doppelg{\"a}ngers, continuously assess and refine socio-critical systems ensuring their alignment with established societal standards.CCS CONCEPTS• Software and its engineering $\rightarrow$ Extra-functional properties; Software verification and validation; • Computing methodologies $\rightarrow$ Artificial intelligence.},
 author = {Baresi, Luciano and Camilli, Matteo and Dolci, Tommaso and Quattrocchi, Giovanni},
 title = {A Conceptual Framework for Quality Assurance of LLM-based Socio-critical Systems},
 keywords = {AI-enabled agents;Computational modeling;Focusing;Human intelligence;Large language models;Market research;Quality assurance;Reliability engineering;Runtime;Software agents;Software reliability},
 pages = {2314--2318}
}


@inproceedings{Bento.,
 abstract = {Water, being an essential resource for life and sustainable development, faces significant challenges such as scarcity due to factors such as climate change and population growth. Given this panorama, the need arises for innovative solutions to guarantee universal access to water. In this context, an Internet of Things (IoT) system has been developed in collaboration with OpenAI technology during classrooms, aimed at automatically capturing rainwater using sensors and actuators. The system proposes to optimize the management of water resources, eliminating dependence on operators and efficiently taking advantage of rainfall. The IoT system integrates various sensors to collect relevant environmental data, which is sent and stored in the cloud through Internet connections. These environmental parameters are analyzed to determine the optimal time for rainwater harvesting and to identify possible uses of the collected water. A comparison between the language models and the microcontroller data is carried out to evaluate the knowledge-based decision-making capability of artificial intelligence. As a result, a rigorous evaluation was obtained for decision making and the identification of the most appropriate OpenAI language model for this project. The rainwater harvesting system based on IoT and OpenAI technology represents a significant step and contributes towards sustainable water management, taking advantage of technological innovation to address crucial challenges in the availability of water resources.},
 author = {Bento, A. C. and Santos-Res{\'e}ndiz, C. De Los and Segura-Prado, L. E. and {La Rosa}, R. Garza-de and de {Jesus Hilario-Cruz}, R. and Gonz{\'a}lez-V{\'a}zquez, S.},
 title = {Experimental Results with the Use of OpenAI with IoT Classes for Water Analysis in Rainfall},
 keywords = {Artificial intelligence;Decision making;higher education;Internet of Things;IoT;OpenAI;professional education;Rain;rainfall collection;Sensor systems;Sensors;Sociology;Technological innovation},
 pages = {60--65-60--65},
 doi = {10.1109/icSmartGrid61824.2024.10578189}
}


@inproceedings{Bhatia.,
 abstract = {Multiple-choice questions quizzes provide an interactive and engaging way to assess students' understanding and reinforce learning, but their manual creation can be time-consuming and labor-intensive for educators. The emergence of Large Language Models (LLMs) offers a transformative solution by enabling automated MCQ generation, correction, and personalized feedback. LLMs, with their advanced natural language understanding and generation capabilities, facilitate the creation of diverse and adaptive MCQ quizzes that cater to various learning objectives and difficulty levels. This paper introduces Quiz Master, an AI-powered platform for automated MCQ quiz generation, correction, and feedback, developed using open-source tools such as Meta Llama, Ollama, LangChain, and Streamlit. Quiz Master delivers an engaging and personalized learning experience, allowing students to actively participate in quizzes while receiving instant, adaptive feedback. By optimizing quiz creation and reducing educator workload, Quiz Master demonstrates the potential of LLMs to enhance educational practices, making learning more enjoyable, interactive, and effective.},
 author = {Bhatia, Gaurav and Alhajri, Raya},
 title = {Accelerate Learning with AI Powered Quiz Master Using Llama LLM},
 keywords = {Adaptation models;Adaptive systems;AI;Data models;Education;Ethics;Human computer interaction;LangChain;Large language models;Llama3.2;LLM;Manuals;Natural language processing;Ollama;Streamlit;Technological innovation},
 pages = {1--6},
 doi = {10.1109/AI2E64943.2025.10983252}
}


@inproceedings{Bock.,
 abstract = {Performance is key to the success and adoption of software systems. In video games, performance is commonly highlighted as one of the top quality concerns raised by players. To check the performance of their systems, development teams tend to rely on profiling and monitoring tools, which observe program executions to identify regressions. The usage of static analysis tools for this purpose has been so far limited. Lately, the success of Large Language Models in many code analytics tools led to attempts to leverage them in static performance analysis. These studies showed promising results in predicting runtime and regressions on large public datasets. In this paper, we evaluate the usability of such models in practice, and particularly in the domain of video games. We train a state-of-the-art neural network on the Code4Bench dataset to predict runtime regressions for programming competition programs, then evaluate its ability to generalize to new domains. Our results show that these models achieve great results (e.g. 95.73{\%} accuracy for performance comparison) on the original domain for programs solving in-sample programming tasks, yet fail to generalize to out-of-sample tasks. Furthermore, we show that transfer techniques such as domain adversarial adaptation and model fine-tuning are not sufficient to transfer these models to the target industrial domain of AAA games.},
 author = {B{\"o}ck, M. and Habchi, S. and Nayrolles, M. and Cito, J.},
 title = {Performance Prediction From Source Code Is Task and Domain Specific},
 keywords = {Adaptation models;Codes;Deep Learning;Defect Prediction;Repository Mining;Runtime;Software performance;Source coding;Static analysis;Training data;Video games},
 pages = {35--42-35--42},
 doi = {10.1109/ICPC58990.2023.00015}
}


@inproceedings{Bodepudi.,
 abstract = {Self-management is an art of managing your thoughts, feelings and actions that helps you achieve goals in life. In today's expeditious world individual's often struggle to maintain equilibrium between their personal life and work. The proposed web application uses NLP to evaluate the user behavior and mood and prepare a customized routine that balances productivity and rest by dynamically scheduling user's tasks. Current NLP based systems for assessing behavior and mood lack the capacity for thorough and flexible scheduling. It also involves libraries like NLTK. TensorFlow is used which is a machine learning model used to personalize suggestions. The web application analyses journals entered by user using NLP and detect user's emotions. The application gives AI generated prompts for completing their journal. When user enters their daily tasks, this application analyses the previous works of user and plan a daily schedule for them. The application also motivates them to complete their tasks by AI-generated positive affirmations through email. It analyses user's answers of questionnaires designed for stress assessment. This model can take audio, video journaling. The web application integrates with calendar events by using google calendar API. It uses google analytics for mood tracking and analytics. The functionality of the application would be supported by strong frontend and backend architectures that make use of ReactJS for frontend and Node.js, MySQL, Firebase, for backend. The proposed app can improve sentiment analysis and it is a unique combination of different features and is expected to be 2{\%} higher than any existing models.},
 author = {Bodepudi, N. R. and Boddeti, K. Sabareesh and Palle, A. Reddy and Bhukya, L. and {Bolla, S. D. T. Reddy} and Sreedevi, C.},
 title = {SoulEase : AI Enhanced Personal Journal - Text Based Emotion Detection},
 keywords = {AI-generated affirmations;Deep Learning;Dynamic scheduling;Dynamic task scheduling;Emotion recognition;Mood;Mood analytics;Natural Language Processing (NLP);Productivity;Schedules;Sentiment analysis;stress assessment},
 pages = {1155--1162-1155--1162},
 doi = {10.1109/ICCPCT61902.2024.10673121}
}


@inproceedings{Borg.,
 abstract = {In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions.},
 author = {Borg, Markus},
 title = {Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring},
 keywords = {AI refactoring;Calibration;Codes;Collaboration;Conferences;Human factors;Industries;Large language models;Security;Software;software maintainability;Testing;trust},
 pages = {37--41},
 doi = {10.1109/IDE66625.2025.00012}
}


@inproceedings{Bappon.,
 abstract = {Inline comments in the source code facilitate easy comprehension, reusability, and enhanced readability. However, code snippets in answers on Q{\&}A sites like Stack Overflow (SO) often lack comments because answerers volunteer their time and often skip comments or explanations due to time constraints. Existing studies show that these online code examples are difficult to read and understand, making it difficult for developers (espe-cially novices) to use them correctly and leading to misuse. Given these challenges, we introduced AUTOGENICS, a tool designed to integrate with SO to generate effective inline comments for code snippets in SO answers exploiting large language models (LLMs). Our contributions are threefold. First, we randomly select 400 answer code snippets (200 Python + 200 Java) from SO and gener-ate inline comments for them using LLMs (e.g., Gemini). We then manually evaluate these comments' effectiveness using four key metrics: accuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate promising effectiveness in generating inline comments for SO answer code snippets. Second, we surveyed 14 active SO users to perceive the effectiveness of these inline comments. The survey results are consistent with our previous manual evaluation. However, according to our evaluation, LLMs-generated comments are less effective for shorter code snippets and sometimes produce noisy comments. Third, to address the gaps, we introduced AUTOGENICS that extracts additional context from question texts and generates context-aware inline comments. It also optimizes comments by removing noise (e.g., comments in import statements and variable declarations). We evaluate the effectiveness of AUTOGENICS-generated comments using the same four metrics that outperform those of standard LLMs. AUTOGENICS might (a) enhance code comprehension with context-aware inline comments, (b) save time, and improve developers' ability to learn and reuse code more accurately.},
 author = {Bappon, Suborno Deb and Mondal, Saikat and Roy, Banani},
 title = {AUTOGENICS: Automated Generation of Context-Aware Inline Comments for Code Snippets on Programming Q{\&}A Sites Using LLM},
 keywords = {Codes;Inline comments;Java;Large Lan-guage Models;Noise;Noise measurement;programming;Python;Source coding;Stack overflow;Standards;Surveys;Tool Support;Usability;User Study},
 pages = {24--35},
 doi = {10.1109/SCAM63643.2024.00013}
}


@inproceedings{Fakhoury.,
 abstract = {We introduce a novel workflow, TICODER, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TICODER partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TICODER at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TICODER through a user study with 15 participants.},
 author = {Fakhoury, S. and Naik, A. and Sakkas, G. and Chakraborty, S. and Musuvathi, M. and Lahiri, S. K.},
 title = {Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation},
 keywords = {Accuracy;Codes;LLM4Code;Natural languages;Software engineering;User intent formulation;User Study},
 pages = {390--391-390--391},
 doi = {10.1145/3639478.3643525}
}


@inproceedings{Fan.,
 abstract = {This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.},
 author = {Fan, A. and Gokkaya, B. and Harman, M. and Lyubarskiy, M. and Sengupta, S. and Yoo, S. and Zhang, J. M.},
 title = {Large Language Models for Software Engineering: Survey and Open Problems},
 keywords = {Automated Program Repair;Documentation generation;Generative AI;Genetic Improvement;Human-Computer Interaction;Large language models;Maintenance engineering;Refactoring;Reliability engineering;Requirements engineering;Search Based Software Engineering (SBSE);Software;Software Analytics;Software engineering;Software Engineering Education;Software Maintenance and Evolution;Software Processes;Software reliability;Software testing;Surveys;Testing},
 pages = {31--53-31--53},
 doi = {10.1109/ICSE-FoSE59343.2023.00008}
}


@inproceedings{Ferdousi.,
 abstract = {Texture image generation has been studied for various applications, including gaming and entertainment. However, context-specific realistic texture generation for industrial applications, such as generating defect textures on railway components, remains unexplored. A mobile-friendly, LLM-based tool that generates fine-grained defect characteristics offers a solution to the challenge of understanding the impact of defects from actual occurrences. We introduce TextureMeDefect, an innovative tool leveraging an LLM- based AI-Inferencing engine. The tool allows users to create realistic defect textures interactively on images of railway components taken with smartphones or tablets. We conducted a multifaceted evaluation to assess the relevance of the generated texture, time, and cost in using this tool on iOS and Android platforms. We also analyzed the software usability score (SUS) across three scenarios. TextureMeDefect outperformed traditional image generation tools by generating meaningful textures faster, showcasing the potential of AI-driven mobile applications on consumer-grade devices.},
 author = {Ferdousi, Rahatara and Hossain, M. Anwar and {El Saddik}, Abdulmotaleb},
 title = {TextureMeDefect: LLM-based Synthetic Railway Defect Texture on Mobile Devices},
 keywords = {AI;AI-tool;Costs;Defect;Electric potential;Engines;Entertainment industry;GPT;Image synthesis;LLM;Mobile applications;Mobile Device;Rail transportation;Railway;Smart phones;Software;Texture;Usability},
 pages = {1--6},
 doi = {10.1109/ICCE63647.2025.10930185}
}


@inproceedings{Joldea.,
 abstract = {The era of large language models (LLMs) brings forth a new wave of automation to many fields of activity. In this work we employ the AI advancements catalyzed by these LLMs to create a smart university assistant. A chatbot that comes to assist university enrolled students and staff on administrative, legislative and public interest topics. To this end, we develop a platform that combines Large Language Models, Retrieval Augmented Generation, Speech-to-Text and Text-to-Speech technologies to automate accessibility to university-related information. We start from openly available models and resources, adapt and finetune them to our target - Romanian question answering with information retrieval - and then release our solutions publicly at https://github.com/Andrei481/RomanianChatbot.},
 author = {Joldea, Andrei-Răzvan and Cernăzanu-Glăvan, Diana and S{\^a}rbu, Vlad and Bulzan, Andrei-{\c{S}}tefan},
 title = {Multimodal AI for Romanian University Support: An LLM, RAG and Voice Approach},
 keywords = {Assistant;Chatbot;Chatbots;Computational modeling;Domain-specific fine-tuning;Informatics;Information retrieval;Large language models;LLM;Question answering (information retrieval);RAG;Retrieval augmented generation;Romanian;Software development management;Speech to text;STT;Text to speech;TTS},
 pages = {000189--000194},
 doi = {10.1109/SACI66288.2025.11030141}
}


@article{Joosten.2024,
 abstract = {Traditionally, ideating new product innovations is primarily the responsibility of marketers, engineers, and designers. However, a rapidly growing interest lies in leveraging generative artificial intelligence (AI) to brainstorm new product and service ideas. This study conducts a comparative analysis of ideas generated by human professionals and an AI system. The results of a blind expert evaluation show that AI-generated ideas score significantly higher in novelty and customer benefit, while their feasibility scores are similar to those of human ideas. Overall, AI-generated ideas comprise the majority of the top-performing ideas, while human-generated ideas scored lower than expected. The executive's emotional and cognitive reactions were measured during the evaluation to check for potential biases and showed no differences between the idea groups. These findings suggest that, under certain circumstances, companies can benefit from integrating generative AI into their traditional idea-generation processes.},
 author = {Joosten, J. and Bilgram, V. and Hahn, A. and Totzek, D.},
 year = {2024},
 title = {Comparing the Ideation Quality of Humans With Generative Artificial Intelligence},
 keywords = {AI-augmented innovation;Artificial intelligence;Artificial Intelligence (AI);Chatbots;ChatGPT;Companies;Creativity;Generative AI;idea generation;innovation;Large language models (LLMs);Task analysis;Technological innovation},
 pages = {153--164-153--164},
 volume = {52},
 number = {2},
 journal = {IEEE Engineering Management Review},
 doi = {10.1109/EMR.2024.3353338}
}


@inproceedings{KailashVarma.,
 abstract = {The current college library system is hindered by inefficiencies in book searching, availability updates, identity verification, and checkout processes, resulting in delays and errors that negatively impact student experiences. This research paper presents an AI-based librarian system designed to address these challenges through automation and digital transformation. Leveraging a Large Language Model (LLM), the proposed system facilitates personalized interactions between students and library resources. Upon student verification, the AI librarian provides real-time information on book availability, location, and tailored recommendations, significantly reducing the time spent on manual searches. The streamlined checkout process allows for automated book issuance and instant confirmation notifications, minimizing human error and enhancing record- keeping. This innovative solution not only improves operational efficiency but also enriches the user experience by offering a user-friendly interface and timely assistance. Future enhancements, such as voice integration and mobile application support, are suggested to further modernize library services. This research underscores the potential of AI technologies to revolutionize library management and improve service delivery in academic settings.},
 author = {{Kailash Varma}, Nadimpalli Madana and Aryan, Adla and Dhanush, Pagilla and Manikanta, Rangisetti and Chandhu, Nalla and Arora, Gagan Deep},
 title = {Developing an AI-Based Library Assistant: Enhancing Book Retrieval with Natural Language Processing and Machine Learning},
 keywords = {AI-based librarian;Automation;Knowledge engineering;Large Language Model (LLM);Large language models;Libraries;library automation;library management system;Manuals;Mobile applications;Natural language processing;personalized recommendations;Real-time systems;student identification;Usability;User experience},
 pages = {136--140},
 doi = {10.1109/CICTN64563.2025.10932399}
}


@inproceedings{Kang.,
 abstract = {As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.},
 author = {Kang, Long and Ai, Jun and Lu, Minyan},
 title = {Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model},
 keywords = {Accuracy;Data models;Human computer interaction;Large language models;LLM;Manuals;Measurement;Software systems;Software Test;Software testing;Test Case;Training;Tuning},
 pages = {132--140},
 doi = {10.1109/DSA63982.2024.00027}
}


@inproceedings{Kannadasan.,
 abstract = {The rapid evolution that continuously occurs in web applications makes personalization and intuitiveness within the experiences of users all the more important. The following study deals with the development of an LLM architecture customized for contextually aware interactions within web environments. Using these kinds of datasets, obtained from real user interactions, together with behavioral analytics and contextual metadata coming from a variety of sources such as the Microsoft COCO dataset and Amazon Customer Reviews, we train LLMs to understand user needs through explicit interaction patterns or contextual cues. To this end, we integrate state-of-the-art natural language processing methodologies with contextual embedding strategies that empower models to dynamically adapt responses, enhance user engagement, and smoothen workflows. We perform extensive experimentation and user testing, demonstrating large improvements in user satisfaction and efficiency of interaction over traditional static models. Furthermore, we discuss a number of challenges related to data privacy, model scalability, and real-time processing that need to be resolved to make such custom LLMs practical in a wide range of web application settings. This work underlines how context-aware architectures can bring revolutionary changes to LLMs for user interactions, offering more responsive, intelligent, and user-centric web applications.},
 author = {Kannadasan, Tamilarasan},
 title = {Custom LLM Architectures for Context-Aware User Interactions in Web Applications},
 keywords = {Adaptation models;Computational modeling;Computer architecture;Context modeling;Context-Aware;Data privacy;Large language models;Real-World Datasets;Reviews;Scalability;Service-oriented architecture;Time factors;Transforms;User Interaction;Web Applications},
 pages = {727--732},
 doi = {10.1109/ICACRS62842.2024.10841706}
}


@inproceedings{Kannadasan.b,
 abstract = {As real-time applications become increasingly integral to full-stack development, the demand for efficient and scalable Large Language Model (LLM) architectures has surged. This study explores optimization strategies for LLMs tailored to meet the stringent performance and latency requirements of real-time environments. We investigate architectural enhancements, including model pruning, quantization, and parallel processing techniques, to reduce computational overhead without compromising accuracy. To validate the proposed approach, we employ the Common Crawl Corpus dataset as a comprehensive case study, leveraging its extensive and diverse textual data to simulate real-world application scenarios. Our experiments demonstrate significant improvements in response times and resource utilization, enabling seamless integration of LLMs into full-stack frameworks. Additionally, we address challenges related to data handling and model adaptability, ensuring that optimized architectures maintain robustness across dynamic workloads. The findings highlight the potential of LLM optimizations to bridge the gap between advanced natural language processing capabilities and the immediate demands of real-time application development. This work provides a foundational framework for developers aiming to harness the power of LLMs in full-stack projects, paving way for more responsive and intelligent web and software solutions.},
 author = {Kannadasan, Tamilarasan},
 title = {Optimizing LLM Architectures for Real-Time Applications in Full-Stack Development},
 keywords = {Accuracy;Adaptation models;Architecture Optimization;Computational modeling;Full-Stack Development;Large language models;Optimization;Public Dataset;Quantization (signal);Real-Time Applications;Real-time systems;Resource management;Throughput;User experience},
 pages = {721--726},
 doi = {10.1109/ICACRS62842.2024.10841540}
}


@inproceedings{Karli.,
 abstract = {Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming--moving from the conventional process of user specifying programming logic to an iterative, collaborative process in which the user specifies desired program outcomes while LLM produces detailed specifications. We introduce a novel integrated development system, Alchemist, that leverages LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, aiming to reduce the required knowledge for developing robot applications. We present a detailed examination of our system design and provide an exploratory study involving true end-users to assess capabilities, usability, and limitations of our system. Through the design, development, and evaluation of our system, we derive a set of lessons learned from the use of LLMs in robot programming. We discuss how LLMs may be the next frontier for democratizing end-user development of robot applications.CCS CONCEPTS• Human-centered computing; • Computer systems organization $\rightarrow$ Robotics;},
 author = {Karli, U. B. and Chen, J. T. and Antony, V. N. and Huang, C. M.},
 title = {Alchemist: LLM-Aided End-User Development of Robot Applications},
 keywords = {Code Generation;end-user development;Human-robot interaction;Large language models;Logic;Natural languages;Organizations;robot programming;Robots;Usability},
 pages = {361--370-361--370}
}


@inproceedings{Jiang.,
 abstract = {Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at MICROSOFT. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management. Building upon these valuable insights, we introduce XPERT, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, XPERT generates customized KQL queries tailored to new incidents. Furthermore, XPERT incorporates a novel performance metric called XCORE, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of XPERT, demonstrating its effectiveness in offline settings. Notably, we deploy XPERT in the real production environment of a large-scale incident management system in MICROSOFT, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and XPERT stands as a pioneering DSL query recommendation framework designed for incident management.},
 author = {Jiang, Y. and Zhang, C. and He, S. and Yang, Z. and Ma, M. and Qin, S. and Kang, Y. and Dang, Y. and Rajmohan, S. and Lin, Q. and Zhang, D.},
 title = {XPERT: Empowering Incident Management with Query Recommendations via Large Language Models},
 keywords = {Incident management;Knowledge engineering;Large Language Model;Machine Learning;Measurement;Production;Query Generation;Software reliability;User experience;Writing},
 pages = {1121--1133-1121--1133},
 doi = {10.1145/3597503.3639081}
}


@inproceedings{Khan.,
 abstract = {With the increasing prevalence of chronic diseases among the elderly population in Bangladesh, there is a critical need for essential solutions to address remote healthcare challenges. This research explores the integration of mobile health (mHealth) technology with wearable Internet of Things (IoT) devices to enhance remote healthcare services. Through a systematic four-step approach encompassing literature review for problem statement identification, needs assessment through quantitative analysis of research questions (RQs), app design and development utilizing User-Centered Design (UCD) and Rapid Iterative Testing and Evaluation (RITE) method, and usability evaluation through User Experience Questionnaire (UEQ), and mHealth App Usability Questionnaire (MAUQ), ShasthoBondhu, a Flutter-based cross-platform smartphone app, offers features such as automated vital monitoring and alerting, remote accessibility of account, location sharing based on user's vitals, convenient doctor appointments, instant emergency calls and real-time interaction with a virtual medical assistant powered by Large Language Model (LLM) tools. In addition, the app interfaces in bilingual format (Bangla and English) for seamless independent interactions. The assessment results, based on the 8-item UEQ-Sand 6-item MAUQ, showcased remarkable outcomes: UEQ's pragmatic and hedonic qualities surpassed benchmarks with means of 2.35 and 1.80, respectively, while MAUQ achieved an impressive 4.3 out of 5 on the rating scale. Henceforth, ShasthoBondhu charts the path forward, offering a scalable, user-centered solution with the fusion of mHealth technology, wearable IoT gadgets, and AI innovations to transform remote healthcare services in Bangladesh.},
 author = {Khan, Farhan and Tulon, Tanima Ahamed and Masrur, Noor and Hasan, Md. Jihanul and Badrul, Tasnuba and Islam, Ashraful},
 title = {Development and Evaluation of ShasthoBondhu: mHealth App for Guiding Emergency Remote Healthcare with Wearable IoT and AI Fusion},
 keywords = {Biomedical monitoring;emergency;Google Fit;healthcare;Internet of Things;IoT;MAUQ;Medical services;mHealth;Statistical analysis;Systematics;Technological innovation;Transforms;UCD;UEQ;Usability;User centered design;User experience;wearable},
 pages = {01--10},
 doi = {10.1109/ASET60340.2024.10708738}
}


@inproceedings{Koualty.,
 abstract = {This paper showcases the development and deployment of a generative AI-based chatbot, using cutting-edge natural language processing techniques to deliver an interactive and informative user experience. Accessible through web browsers on various operating systems, the chatbot aims to maximize accessibility. The chatbot demonstrates the capabilities of the ChatGPT model from OpenAI, seamlessly integrating with APIs and adhering to GDPR regulations to ensure reliability and privacy compliance. Comprehensive evaluations were conducted to optimize the chatbot's performance and development efforts. The results validate the effectiveness of the approach, with user acceptance testing providing valuable feedback for further improvements. The results show that the proposed conversational model which was trained and worked based on ChatGPT has a superior performance to traditional conversational models, and could work optimally by applying any language without any additional training.},
 author = {Koualty, Rand and Chou, Nien-Ying and Alabdallah, Suleiman},
 title = {Generative AI Agents, Build a Multilingual ChatGPT-based Customer Service Chatbot},
 keywords = {Chatbot;Chatbots;ChatGPT;Customer Service;Generative AI agents;Large language models;LLMs;Multilingual;Operating systems;Privacy;Regulation;Reliability;Testing;Training;User experience},
 pages = {5--10},
 doi = {10.1109/FLLM63129.2024.10852436}
}


@article{Lee.2025,
 abstract = {The increasing demand for programming education and growing class sizes require immediate and personalized feedback. However, integrating Large Language Models (LLMs) like ChatGPT in introductory programming courses raises concerns about AI-assisted cheating. In large-scale settings, faulty code submissions may lead LLMs to overanalyze, causing unnecessary token consumption. This paper proposes a GPT-4o-based code review system that provides accurate feedback while reducing token usage and preventing AI-assisted cheating. Unlike general-purpose LLM tools for professionals, the system is pedagogically designed for primary and secondary students by focusing on review necessity and learner-friendly feedback. The system features a Code Review Module (CRM) that reduces token usage via a Review Necessity Chain (RNC), and Code Correctness Check Module (CCM) combining test case validation with LLM-based assessment. To prevent AI-assisted cheating, the system provides automated feedback on submitted code without prompting and revealing correct answers, which are accessed only through the ``Ask Code Tutor'' button. In usability test, the system detected up to 42.86{\%} more errors than a conventional online judge. BERTScore analysis showed that over 80{\%} of the system-generated reviews were semantically aligned with human feedback. A performance comparison with state-of-the-art systems demonstrated a blocking success rate of 86{\%}, with a comparable review omission rate. These results indicate that the system provides more accurate feedback than conventional automated code reviews, while achieving token efficiency and supporting self-directed learning through educational feedback. Thus, it can serve as a practical solution for scalable programming education in primary and secondary classes.},
 author = {Lee, Dong-Kyu and Joe, Inwhee},
 year = {2025},
 title = {A GPT-Based Code Review System With Accurate Feedback for Programming Education},
 keywords = {Accuracy;Automation;Chatbots;Codes;Education;GPT-4o;LangChain;Large language models (LLMs);learner-friendly code reviews;programming education;Programming profession;Proposals;Reviews;Training;Usability},
 pages = {105724--105737},
 volume = {13},
 issn = {2169-3536},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2025.3581139}
}


@inproceedings{Li.,
 abstract = {In a trip planning service, travelers first set a starting point, a destination, and a sequential list of specific points of interest types (e.g., museums, restaurants, and parks). Based on this information, the service searches the spatial database to customize the best travel itinerary for the tourist. However, in previous studies, planners only considered the time factor when designing the optimal route, and failed to fully consider the quality of each point of interest. In this study, we specifically leveraged the capabilities of large language model to parse and respond to complex travel-related user queries. To apply large language model to route planning, we fine-tuned the model to understand geotagging and user travel preferences. We have introduced a novel graph search algorithm combined with large language model output, which optimizes the route search process to provide optimal travel recommendations by taking into account various factors such as distance length, budget constraints, and popularity of tourist attractions. In addition, we have integrated real-time traffic data and historical travel data to further improve the prediction accuracy and application usefulness of the model. In the experimental validation phase, we designed a series of benchmarks to compare the performance of the system with traditional algorithms and other machine learning-based route planning methods. Experimental results show that our model has a significant improvement compared with traditional methods in improving the speed and accuracy of path selection.},
 author = {Li, Bohang and Zhang, Kai and Sun, Yiping and Zou, Jianke},
 title = {Research on Travel Route Planning Optimization based on Large Language Model},
 keywords = {Accuracy;Graph search algorithm;Heuristic algorithms;Large Language Model;Large language models;Optimization;Planning;Prediction algorithms;Predictive models;Real-time systems;Route optimization;Spatial databases;Time factors;Travel route planning},
 pages = {352--357},
 doi = {10.1109/DOCS63458.2024.10704489}
}


@article{Li.2025,
 abstract = {The communication and computation integrated network architecture ((Com)2Net) simplifies the deployment of applications such as distributed machine learning. However, the management and operation of the (Com)2Net pose significant burdens and consume a substantial amount of time due to its large-scale and complexity nature. Intent-based networking technology and Large Language Models (LLMs) are beneficial for the automation configuration and management of (Com)2Nets. In this article, we introduce a network automation configuration and management architecture. It implements network automation configuration based on user intents in natural language, reducing the network management complexity. The network management agent, based on a LLM, is proposed to translate user intents in natural language into machine instructions. Finally, we present two use cases and real-world evaluations to validate the performance of this architecture. The demo and code are open available on: https://github.com/Lizonghang/KlonetAI.},
 author = {Li, Qing and Xiong, Yanxu and Li, Zonghang and Ma, Chongxi and Yu, Hongfang and Sun, Gang and Luo, Long and Zhang, Zhaofeng},
 year = {2025},
 title = {KlonetAI: Automating (Com)2Nets Management With Human Language Intents},
 keywords = {Agent-based modeling;Automation;Complex networks;Configuration management;Graphics processing units;Knowledge based systems;Large language models;Machine Learning;Natural language processing;Performance evaluation;Servers;Software defined networking;Telecommunication network management;Virtualization},
 pages = {12--19},
 volume = {39},
 number = {3},
 issn = {1558-156X},
 journal = {IEEE Network},
 doi = {10.1109/MNET.2024.3507801}
}


@inproceedings{Li.b,
 abstract = {This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.},
 author = {Li, Youwei and Li, Yangyang and Yang, Yangzhao},
 title = {Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model},
 keywords = {Adaptation models;Agent;App Automation Testing;Automation;Deep Learning;Digital twins;Large Language Model;Large language models;Logic;Mobile applications;Mobile handsets;Natural languages;Testing},
 pages = {609--614},
 doi = {10.1109/DTPI61353.2024.10778901}
}


@inproceedings{Lima.,
 abstract = {Analyzing mobile app reviews is essential for identifying trends and issue patterns that affect user experience and app reputation in app stores. A risk matrix provides a straightforward, intuitive method to prioritize software maintenance actions to mitigate negative ratings. However, manually constructing a risk matrix is time-consuming, and stakeholders often struggle to understand the context of risks due to varied descriptions and the sheer volume of reviews. Therefore, machine learning-based methods are needed to extract risks and classify their priority effectively. While existing studies have automated risk matrix generation in software development, they have not explored app reviews or utilized Large Language Models (LLMs) in a scalable architecture. To address this gap, we present iRisk (scalable microservice for classifying issue Risks), a tool for generating a risk matrix based on crowdsourced app reviews using LLM. We present i-LLAMA, a fine-tuned version of LLaMA 3, optimized to detect and prioritize app-related issues using a risk analysis dataset of reviews categorized by severity and likelihood of occurrence. This dataset is also publicly available. Our contributions include the open-source resources to support the software maintenance and evolution industry, fine-tuning of LLaMA 3, and a scalable microservice architecture to handle large volumes of data. The iRisk can manage app issues and risks and provide an automated dashboard and visualizations for decision-making, monitoring, and risk mitigation. The tool is available on GitHub11https://github.com/vitormesaque/iRisk, and a presentation about the tool can be found in this video22https://irisk.mappidea.com.},
 author = {de Lima, Vitor Mesaque Alves and Barbosa, Jacson Rodrigues and Marcacini, Ricardo Marcodes},
 title = {iRisk: A Scalable Microservice for Classifying Issue Risks Based on Crowdsourced App Reviews},
 keywords = {App Reviews;Computer architecture;Issue Prioritization;Large Language Model;Microservice architectures;Monitoring;Opinion Mining;Reviews;Risk analysis;Risk Matrix;Risk mitigation;Software development management;Software maintenance;Stakeholders;User experience},
 pages = {858--862},
 doi = {10.1109/ICSME58944.2024.00091}
}


@article{Liu.2024,
 abstract = {Student question generation (SQG) is an effective strategy for improving reading comprehension. It helps students improve their understanding of reading materials, metacognitively monitor their comprehension, and self-correct comprehension gaps. Internet technologies have been used to facilitate SQG process through intensive peer support. However, the availability, level of task commitment, and capabilities of student peers have emerged as significant concerns, particularly in light of the global pandemic and the subsequent postpandemic era. Thus, this article presents a student-artificial intelligence (AI) cocreation tool called CoAsker for supporting question generation. Following recent human-computer interaction (HCI) research in human-AI collaborative writing, CoAsker first allows students to provide question clues and answers and then uses a state-of-the-art pretrained language model, T5-PEGASUS, to generate questions. Finally, the student can use this AI question directly or perform reflection by comparing his or her questions with the AI question. An empirical study was conducted to examine the quality of AI questions and the effect of this tool on student engagement and reading comprehension. The results of the study show that students using this tool (treatment) were more engaged in generating low-level cognitive questions and performed better in acquiring knowledge than those using a traditional online question generation tool (control). These results indicate that student-AI question cocreation is beneficial to SQG training and educational assessment for reading comprehension, such as repeated practices.},
 author = {Liu, M. and Zhang, J. and Nyagoga, L. M. and Liu, L.},
 year = {2024},
 title = {Student-AI Question Cocreation for Enhancing Reading Comprehension},
 keywords = {Artificial intelligence;Authoring systems;Computational modeling;Educational technology;Federated learning;Human computer interaction;Internet;Natural language processing;Task analysis;Writing},
 pages = {815--826-815--826},
 volume = {17},
 journal = {IEEE Transactions on Learning Technologies},
 doi = {10.1109/TLT.2023.3333439}
}


@inproceedings{Kim.,
 abstract = {Effective user experience (UX) evaluation requires personalized assessment methods that adapt to individual user characteristics and real-time context. This study introduces the User Experience Questionnaire generation workflow using multiple LLMs (UEQ-mLLM), a system that generates tailored questionnaires based on user data collected through a multimodal interactive dashboard. By leveraging user information and states, UEQ-mLLM generates questionnaires that enhance the accuracy and depth of UX evaluations. Comparative analysis against a single LLM-based approach using the G-Eval framework demonstrated a significant performance improvement of 20.62{\%} for UEQ-mLLM. This work highlights the potential of utilizing multiple LLMs to generate effective UX questionnaires and contributes to the advancement of user-centered design methodologies.},
 author = {Kim, Yeonwoo and Lee, Junhyeok and Han, Ju Hyuk and Kim, Minjae and Lee, Howook and Lee, Won Hee},
 title = {Agentic LLM Workflows for Personalized User Experience Questionnaire Generation},
 keywords = {Accuracy;Agentic Workflow;Data models;Gaze tracking;Large language models;Physiology;Real-time systems;Training data;User centered design;User experience;User Experience Questionnaire},
 pages = {1--4},
 doi = {10.1109/ICCE-Asia63397.2024.10773955}
}


@inproceedings{Jacobs.,
 abstract = {This paper presents the use of Retrieval Augmented Generation (RAG) to improve the feedback generated by Large Language Models for programming tasks. For this purpose, corresponding lecture recordings were transcribed and made available to the Large Language Model GPT-4 as external knowledge source together with timestamps as metainformation by using RAG. The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture. In an exercise platform developed to solve programming problems for an introductory programming lecture, students can request feedback on their solutions generated by GPT-4. For this task GPT-4 receives the students' code solution, the compiler output, the result of unit tests and the relevant passages from the lecture notes available through the use of RAG as additional context. The feedback generated by GPT-4 should guide students to solve problems independently and link to the lecture content, using the time stamps of the transcript as meta-information. In this way, the corresponding lecture videos can be viewed immediately at the corresponding positions. For the evaluation, students worked with the tool in a workshop and decided for each feedback whether it should be extended by RAG or not. First results based on a questionnaire and the collected usage data show that the use of RAG can improve feedback generation and is preferred by students in some situations. Due to the slower speed of feedback generation, the benefits are situation dependent.},
 author = {Jacobs, S. and Jaschke, S.},
 title = {Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation},
 keywords = {Codes;Conferences;Feedback;GPT-4;Large language models;programming education;Programming profession;Recording;Retrieval augmented generation;Videos},
 pages = {1--5-1--5},
 doi = {10.1109/CSEET62301.2024.10663001}
}


@inproceedings{Jacob.,
 abstract = {Queries in PDFs can be time-consuming and labor-intensive because of the unstructured nature of the PDF document type and the need for accurate and relevant search results. By applying cutting-edge algorithms for natural language processing to examine PDF documents and extract relevant data, LangChain solves these difficulties. It makes use of an easy search interface, adjustable filters, and efficient indexing and retrieval mechanisms to enhance the search experience. To efficiently retrieve relevant information from PDF documents, users can annotate critical portions, store queries, and create bookmarks with LangChain. The characteristics of LangChain improve overall productivity and greatly simplify PDF querying. Semantic search, driven by the latest Transformer language models, represents a significant evolution in information retrieval systems. This research work explores the capabilities of semantic search to efficiently retrieve documents from large collections in response to natural language queries. Unlike traditional keyword-based approaches, semantic search connects the power of Transformer models to discern meaning, providing users with more contextually relevant and accurate results within seconds. This technology not only enhances the user experience by delivering superior matches from document collections but also lays the foundation for tackling more intricate tasks, like text summarization and question-answering. The research investigates the impact of semantic search on information retrieval efficiency and accuracy, comparing its performance with conventional methods. The findings presented herein not only showcase the immediate benefits of semantic search but also open paths for future research and development in natural language processing and its applications.},
 author = {Jacob, T. Prem and Bizotto, B. L. S. and Sathiyanarayanan, M.},
 title = {Constructing the ChatGPT for PDF Files with Langchain - AI},
 keywords = {Chatbots;ChatGPT;Chroma DB;Deep Learning;Information retrieval;LangChain;OpenAI;Pinecone;Portable document format;Probability density function;Redis;Semantic search;Transformers;User experience;Vector Embeddings},
 pages = {835--839-835--839},
 doi = {10.1109/ICICT60155.2024.10544643}
}


@inproceedings{Izadi.,
 abstract = {Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outper-formed the other models across all programming languages, high-lighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the models' predictions, we found that 66.3{\%} of failures were due to models' limitations, 24.4{\%} occurred due to inappropriate model usage in a development context, and 9.3{\%} were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.},
 author = {Izadi, M. and Katzy, J. and {van Dam}, T. and Otten, M. and Popescu, R. M. and {van Deursen}, A.},
 title = {Language Models for Code Completion: A Practical Evaluation},
 keywords = {Analytical models;Automatic Code Completion;CodeGPT;Codes;Data models;evaluation;ide;InCoder;language models;Open Source;Predictive models;Training;Training data;Transformers;UniXcoder},
 pages = {956--968-956--968},
 doi = {10.1145/3597503.3639138}
}


@inproceedings{Fontana.,
 abstract = {Intent-based networking has attracted interest in the academic research for enhancing network management operations with user-oriented features. One of the main challenge in this field is the acquisition of the user intents and subsequently the relative translation into policies for the automatic management of the network. Concerning this task, the primary technique employed is relying on Graphical User Interfaces (GUI)s. In addition, the use of Natural Language Processing techniques has been extensively adopted for improved user experience. Recently, some preliminary studies have shown that using Large Language Models (LLMs) for this purpose leads to achieve interesting results. However, based on a comprehensive analysis of the state of the art, it has emerged that the works utilizing the LLMs do not fully exploit all the capabilities these tools could potentially offer. For this reason, the doctoral work aims to address the following challenges: enhancing user experience through the utilization of intelligent chatbots, improving the correct understanding of user intents and ensuring the translation of user intentions into a coherent set of network configurations, which are generated automatically.},
 author = {Fontana, M. and Martini, B. and Sciarrone, F.},
 title = {Exploring Large Language Models in Intent Acquisition and Translation},
 keywords = {Accuracy;Chatbots;Deep Learning;Design methodology;Large language models;Medical services;User experience},
 pages = {231--234-231--234},
 doi = {10.1109/NetSoft60951.2024.10588924}
}


@inproceedings{Franzosi.,
 abstract = {Graphical User Interface (GUI) based testing is a commonly used practice in industry. Although valuable and, in many cases, necessary, it is associated with challenges such as high cost and requirements on both technical and domain expertise. Augmented testing, a novel approach to GUI test automation, aims to mitigate these challenges by allowing users to record and render test cases and test data directly on the GUI of the system under test (SUT). In this context, Scout is an augmented testing tool that captures system states and transitions during manual interaction with the SUT, storing them in a test model that is visually represented in the form of state trees and reports. While this representation provides basic overview of a test suite, e.g. about its size and number of scenarios, it is limited in terms of analysis depth, interpretability, and reproducibility. In particular, without human state labeling, it is challenging to produce meaningful and easily understandable test reports. To address this limitation, we present a novel solution and a demonstrator, integrated into Scout, which leverages large language models (LLMs) to enrich the model-based test case representation by automatically labeling and describing states and describing transitions. We conducted two experiments to evaluate the impact of the solution. First, we compared LLM-enhanced reports with expert-generated reports using embedding distance evaluation metrics. Second, we assessed the usability and perceived value of the enhanced reports through an industrial survey. The results of the study indicate that the plugin can improve readability, actionability, and interpretability of test reports. This work contributes to the automation of GUI testing by reducing the need for manual intervention, e.g. labeling, and technical expertise, e.g. to understand test case models. Although the solution is studied in the context of augmented testing, we argue for the solution's generalizability to related test automation techniques. In addition, we argue that this approach enables actionable insights and lays the groundwork for further research into autonomous testing based on Generative AI.},
 author = {Franzosi, Diogo Buarque and Al{\'e}groth, Emil and Isaac, Maycel},
 title = {LLM-Based Labelling of Recorded Automated GUI-Based Test Cases},
 keywords = {Automation;Graphical user interfaces;Labeling;Manuals;Software development management;Software testing;Surveys;Testing;Transforms;Usability},
 pages = {453--463},
 doi = {10.1109/ICST62969.2025.10988984}
}


@inproceedings{Galimzyanov.,
 abstract = {This paper introduces the human-curated Pandas-PlotBench dataset, designed to evaluate language models' effectiveness as assistants in visual data exploration. Our benchmark focuses on generating code for visualizing tabular data---such as a Pandas DataFrame---based on natural language instructions, complementing current evaluation tools and expanding their scope. The dataset includes 175 unique tasks. Our experiments assess several leading Large Language Models (LLMs) across three visualization libraries: Matplotlib, Seaborn, and Plotly. We show that the shortening of tasks has a minimal effect on plotting capabilities, allowing for the user interface that accommodates concise user input without sacrificing functionality or accuracy. Another of our findings reveals that while LLMs perform well with popular libraries like Matplotlib and Seaborn, challenges persist with Plotly, highlighting areas for improvement. We hope that the modular design of our benchmark will broaden the current studies on generating visualizations. Our dataset and benchmark code is available online: https://huggingface. co/datasets/JetBrains-Research/PandasPlotBench; https://github.com/JetBrains-Research/PandasPlotBench.},
 author = {Galimzyanov, Timur and Titov, Sergey and Golubev, Yaroslav and Bogomolov, Egor},
 title = {Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code},
 keywords = {Benchmark testing;Codes;Data models;Data visualization;Large language models;Libraries;Natural languages;Software;User experience;Visualization},
 pages = {503--507},
 doi = {10.1109/MSR66628.2025.00083}
}


@inproceedings{Gallagher.,
 abstract = {Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in re-producibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.},
 author = {Gallagher, S. K. and Ratchford, J. and Brooks, T. and Brown, B. and Heim, E. and McMillan, S. and Nichols, W. R. and Rallapalli, S. and Smith, C. and VanHoudnos, N. and Winski, N. and Mellinger, A. O.},
 title = {Assessing LLMs for High Stakes Applications},
 keywords = {HCI;Large language models;Measurement;metrics;Reliability;scaling;Security;Software engineering;TEVV;trust;Tuning},
 pages = {103--105-103--105},
 doi = {10.1145/3639477.3639720}
}


@article{Ge.2024,
 abstract = {The deployment of large language models (LLMs) brings challenges to intelligent systems because its capability of integrating large-scale training data facilitates contextual reasoning. This paper envisions a revolution of the LLM based (Artificial) Intelligent Operating Systems (IOS, or AIOS) to support the core of automated vehicles. We explain the structure of this LLM-OS and discuss the resulting benefits and implementation difficulties.},
 author = {Ge, J. and Chang, C. and Zhang, J. and Li, L. and Na, X. and Lin, Y. and Wang, F. Y.},
 year = {2024},
 title = {LLM-Based Operating Systems for Automated Vehicles: A New Perspective},
 keywords = {Automated vehicles;Cognition;Hardware;LLM;Memory management;operating system;Operating systems;Safety;Software;Task analysis},
 pages = {4563--4567-4563--4567},
 volume = {9},
 number = {4},
 journal = {IEEE Transactions on Intelligent Vehicles},
 doi = {10.1109/TIV.2024.3399813}
}


@inproceedings{Gokkaya.,
 abstract = {Open-source software is critical for modern digital infrastructure, yet security vulnerabilities remain a significant concern as attackers exploit unpatched systems. Large Language Models (LLMs) have shown promise in vulnerability detection, but their ability to detect security patches solely based on code changes remains underexplored. This capability is crucial for identifying security patches when commit messages lack explicit security labels. This study evaluates six LLMs, e.g., GPT-4o, Claude 3.5 Haiku, and DeepSeek V3, using various prompting approaches to assess their capacity to distinguish between security and non-security patches and analyze their effectiveness in classifying security patches into their corresponding CWE categories. The results show that LLMs can detect security patches, but performance varies across models and prompting strategies. DeepSeek V3 (Chain-of-Thought) and GPT-4o (Zero-Shot) demonstrate the most consistent performance across all evaluation metrics, each achieving over 70{\%} in accuracy, precision, recall, and F1-score. New prompting techniques have also led to notable improvements in certain areas, particularly in precision. However, CWE classification remains a major challenge, with most models misclassifying over 70{\%} of security patches. Even the best-performing model, Claude Haiku 3.5 (Few-Shot), achieves only 31.1{\%} accuracy, with memory-related vulnerabilities like out-of-bounds write and use-after-free being the most frequently misclassified. These findings highlight the potential of LLMs in security patch detection but emphasize the need for improved CWE classification. Source code available at: https://github.com/betulgkkaya/LLMs{\_}Patch{\_}Detection.git.},
 author = {Gokkaya, Betul},
 title = {Leveraging Large Language Models for Security Patch Detection and CWE Classification},
 keywords = {Accuracy;CWE Classification;Human computer interaction;Large language models;LLMs;Measurement;Open source software;Optimization;Prompt engineering;Robots;Security;Security Patch Detection;Silent Fix Identification;Source coding},
 pages = {1--10},
 doi = {10.1109/ICHORA65333.2025.11017081}
}


@inproceedings{Gu.,
 abstract = {Empathetic dialogue systems can recognize users' emotions and provide appropriate responses, which are crucial for enhancing the user experience. However, existing empathetic dialogue systems often fall short in understanding some complex implicit emotions. To address this problem, we propose a multi-level knowledge-enhanced prompting approach to achieve more effective empathetic dialogue generation effect. We first acquire topic words and emotional keywords as low-level emotional knowledge. Next, we retrieve dialogue samples that are most similar in topic and emotional attributes, forming mid-level emotional knowledge. Subsequently, we guide a large language model (LLM) to generate high-level comprehensive emotional knowledge based on the information from the previous two levels and the dialogue context. Finally, based on the emotional knowledge, we further guide LLM to generate empathetic responses. The research results indicate that our multi-level knowledge-enhanced prompting approach outperforms other baselines.},
 author = {Gu, Z. and Zhu, Q. and He, H. and Yu, Z. and Lan, T. and Yuan, S.},
 title = {Multi-Level Knowledge-Enhanced Prompting for Empathetic Dialogue Generation},
 keywords = {Artificial intelligence;Computational modeling;Emotion recognition;empathetic dialogue system;Federated learning;Knowledge based systems;knowledge-enhanced prompting;Large Language Model;Large language models;Task analysis;User experience},
 pages = {3170--3175-3170--3175},
 doi = {10.1109/CSCWD61410.2024.10580095}
}


@inproceedings{Gupta.,
 abstract = {`My Assistant,' a speech-enabled virtual assistant created to promote smooth human-machine interaction, is presented in this research study. By utilizing cutting-edge speech recognition and natural language processing technologies, My Assistant can translate spoken words into text, carry out user commands, and provide contextually appropriate answers. The study highlights the significance of interacting with other systems and APIs while discussing the fundamental methods of voice recognition, task execution, and answer generation [1]. Metrics for assessing speech recognition precision and user contentment are suggested, accompanied with comparisons with current personal assistants. The results show how effective and promising My Assistant is for raising user convenience and productivity [2].},
 author = {Gupta, A. D. and Danishan and Kumar, A. and Chaudhary, I. and Yasir, A. M. and Kumar, N.},
 title = {My Assistant SRSTC: Speech Recognition and Speech to Text Conversion},
 keywords = {Acoustic modeling;Deep Learning;Human computer interaction;Measurement;Natural language processing;Natural language processing NLP;Productivity;Response generation;Semantic parsing;Speech recognition;Text Recognition;Text-to-speech (TTS);Virtual assistant;Virtual assistants},
 pages = {394--400-394--400},
 doi = {10.1109/IC3SE62002.2024.10593324}
}


@inproceedings{Haji.,
 abstract = {Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28 {\%} of the tests generated by Copilot are passing tests; 54.72 {\%} of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45 {\%} of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.},
 author = {Haji, K. E. and Brandt, C. and Zaidman, A.},
 title = {Using GitHub Copilot for Test Generation in Python: An Empirical Study},
 keywords = {Codes;Runtime;Software;Source coding;Syntactics;Test pattern generators;Writing},
 pages = {45--55-45--55}
}


@article{Haldar.2025,
 abstract = {Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students' responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.},
 author = {Haldar, Susmita and Pierce, Mary and {Fernando Capretz}, Luiz},
 year = {2025},
 title = {Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning},
 keywords = {Accuracy;Capstone project;Chatbots;ChatGPT;Education;Generative AI;Industries;Large language models;Microsoft Copilot;Sentiment analysis;Software engineering;Software testing;software testing education;Systematic literature review},
 pages = {46070--46090},
 volume = {13},
 issn = {2169-3536},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2025.3545882}
}


@inproceedings{Hoffmann.,
 abstract = {Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13-15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.},
 author = {Hoffmann, J. and Frister, D.},
 title = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
 keywords = {Codes;Computational modeling;Data models;Large language models;Machine Learning;Mobile Testing;Software;Software testing;Training data;Transformers},
 pages = {76--77-76--77}
}


@inproceedings{Hu.,
 abstract = {In recent years, mini-programs have rapidly gained popularity and are widely used in payment, travel, shopping and other fields, greatly enhancing the convenience of users' lives. However, these services usually require access to sensitive personal information such as phone numbers, location information, ID numbers and other permissions. In the process of using them, users may frequently encounter permission requests, and sometimes even be forced to authorize them, leading to poor usage experience or even falling into an endless-loop authorization cycle that is difficult to exit. Unfortunately, most of the existing studies are fragmented and only deal with individual issues of personal information usage in mini-programs, lacking a comprehensive analysis of how permission requests affect user experience. To address this problem, this paper proposes an automated tool called ELDetector that automatically traverses mini-program pages through dynamic analysis and detects authorization endless-loops with the assistance of the Large Language Model (LLM). We find that authorization endless-loops of mini-programs are mainly classified into two categories: single-page endless-loop and multipage endless-loop, based on the number of pages caught in the loop. We evaluated ELDetector on 97 popular mini-programs with an accuracy of 79.4{\%} in detecting the authorization endless-loop problem, of which 15 mini-programs have been fixed by the developers. In addition, with the help of the Large Language Model (LLM), ELDetector is 54{\%} faster than the traditional monkey test in identifying authorization endless-loop entrance points.},
 author = {Hu, Nan and Fan, Ming and Lei, Jingyi and He, Jiaying and Hou, Zhe},
 title = {ELDetector: An Automated Approach Detecting Endless-loop in Mini Programs},
 keywords = {Accuracy;Authorization;automated GUI testing;Collaboration;endless-loop;Foundation models;Graphical user interfaces;Large Language Model;Large language models;Measurement;mini program;Object recognition;Software engineering;User experience},
 pages = {104--112},
 doi = {10.1109/Forge66646.2025.00019}
}


@inproceedings{Hu.b,
 abstract = {In industrial practice, many bugs in commercial mobile apps manifest as self-conflicts of data presented in the GUI (Graphical User Interface). Such data inconsistency bugs can bring confusion to the users and deteriorate user experiences. They are a major target of industrial testing practice. However, due to the complication and diversity of GUI implementation and data presentation (e.g., the ways to present the data in natural language), detecting data inconsistency bugs is a very challenging task. It still largely relies on manual efforts. To reduce such human efforts, we proposed Au-toConsis, an automated data inconsistency testing tool we designed for Meituan. one of the largest E-commerce providers with over 600 million transacting users. AutoConsis can automatically analyze GUI pages via a multi-modal deep-learning model and extract target data from textual phrases leveraging LLMs (Large Language Models). With these extracted data, their inconsistencies can then be detected. We evaluate the design of AutoConsis via a set of ablation experiments. Moreover, we demonstrate the effectiveness of AutoConsis when applying it to real-world commercial mobile apps with eight representative cases.},
 author = {Hu, Y. and Jin, H. and Wang, X. and Gu, T. and Guo, S. and Chen, C. and Zhou, Y.},
 title = {AutoConsis: Automatic GUI-driven Data Inconsistency Detection of Mobile Apps},
 keywords = {Automatic Testing;Computer bugs;Data mining;Data models;Functional Bug;In-context Learning;Mobile applications;Mobile Apps;Software;Target recognition;Task analysis},
 pages = {137--146-137--146},
 doi = {10.1145/3639477.3639748}
}


@inproceedings{Huang.,
 abstract = {Traditional UX development methodologies focus on developing ``one size fits all'' solutions and lack the flexibility to cater to diverse user needs. In response, a growing interest has arisen in developing more dynamic UX frameworks. However, existing approaches often cannot personalise user experiences and adapt to user feedback in realtime. Therefore, my research introduces a novel approach of combining Large Language Models and personas, to address these limitations. The research is structured around three areas: (1) a critical review of existing adaptive UX practices and the potential for their automation; (2) an investigation into the role and effectiveness of personas in enhancing UX adaptability; and (3) the proposal of a theoretical framework that leverages LLM capabilities to create more dynamic and responsive UX designs and guidelines.},
 author = {Huang, Y.},
 title = {Generating User Experience Based on Personas with AI Assistants},
 keywords = {Automation;Guidelines;Proposals;Reviews;Software engineering;Standards;User experience},
 pages = {181--183-181--183},
 doi = {10.1145/3639478.3639810}
}


@inproceedings{Huq.,
 abstract = {Testing for accessibility is a significant step when developing software, as it ensures that all users, including those with disabilities, can effectively engage with web and mobile applications. While automated tools exist to detect accessibility issues in software, none are as comprehensive and effective as the process of user testing, where testers with various disabilities evaluate the application for accessibility and usability issues. However, user testing is not popular with software developers as it requires conducting lengthy interviews with users and later parsing through large recordings to derive the issues to fix. In this paper, we explore how large language models (LLMs) like GPT 4.0, which have shown promising results in context comprehension and semantic text generation, can mitigate this issue and streamline the user testing process. Our solution, called Reca11, takes in auto-generated transcripts from user testing video recordings and extracts the accessibility and usability issues mentioned by the tester. Our systematic prompt engineering determines the optimal configuration of input, instruction, context and demonstrations for best results. We evaluate Reca11's effectiveness on 36 user testing sessions across three applications. Based on the findings, we investigate the strengths and weaknesses of using LLMs in this space.},
 author = {Huq, Syed Fatiul and Tafreshipour, Mahan and Kalcevich, Kate and Malek, Sam},
 title = {Automated Generation of Accessibility Test Reports from Recorded User Transcripts},
 keywords = {crowd-sourced software testing;Large language models;Reproducibility of results;Semantics;Software;software accessibility;Software engineering;Software testing;Systematics;Testing;Usability;Video recording},
 pages = {204--216},
 doi = {10.1109/ICSE55347.2025.00043}
}


@inproceedings{Israilidis.,
 abstract = {This paper investigates the impact of large language model (LLM) AI tools, such as ChatGPT and Copilot, on software development education, focusing on usability, efficiency, and effectiveness in real-world scenarios. The research employs a quantitative approach, utilizing a survey of 50 software developers with varying levels of experience. Preliminary findings suggest that AI tools have a positive influence on expediting coding tasks and automating text generation, particularly in the early stages of product development. Challenges related to customization, accuracy, and transparency, as well as concerns about their potential impacts on employment, personal privacy, and ethical boundaries, have been identified. Pointers and initial recommendations for transitioning to AI-enhanced teaching and optimizing interactions between learners and generative AI practices are provided.},
 author = {Israilidis, J. and Chen, W. Y. and Tsakalerou, M.},
 title = {Software Development and Education: Transitioning Towards AI Enhanced Teaching},
 keywords = {AI tools;Education;Ethics;Focusing;Generative AI;Privacy;Product development;Software;Software development;Surveys},
 pages = {1--6-1--6},
 doi = {10.1109/EDUCON60312.2024.10578564}
}


@inproceedings{Ivanov.,
 abstract = {In today's digitally connected world, the emergence of conversational artificial intelligence powered by generative language models has ushered in a new era of human-computer interaction. Chatbots using these technologies are increasingly being used in a variety of scientific as well as social domains. These intelligent conversational agents, powered by advances in generative language models, offer a wide range of applications from customer support and healthcare to software development and education. This paper discusses the development of a microservice that works as an interface to ChatGPT through the GPT API. The goal is to facilitate the integration of next generation chatbots to distributed architecture services. Access to the microservice is implemented using an Advanced Message Queuing Protocol (AMQP) message broker. To conduct the experiments, a microservice was developed that provides a REST interface to the proposed microservice for clients that do not support the AMQP protocol.},
 author = {Ivanov, R. and Velkova, V.},
 title = {Microservice-Based Interface to ChatGPT},
 keywords = {AMQP;Chatbots;GPT API;Microservice architectures;microservices;MSA;Museums;Programming profession;Protocols;Reliability;Robots},
 pages = {1--5-1--5},
 doi = {10.1109/AQTR61889.2024.10554146}
}


@inproceedings{Zhu.,
 abstract = {The collaborative interaction between power systems and the Internet of Things (IoT) is strengthening, with IoT devices facilitating real-time monitoring and governance of the power grid, pushing power systems to the next frontier: the smart grid. Nevertheless, the reliance of power systems on a vast array of IoT devices, each with its unique API, makes the development of a unified smart grid software solution extremely complex. Most existing research focuses on accuracy in recommendations, thus neglecting users' needs for functional diversity. To address this issue, we propose a diversified API recommendation approach that suggests a variety of functional APIs from power system sensors to users. We start by converting API labels into feature vectors based on a pre-trained language model and calculate the similarity between APIs through clustering. Subsequently, we construct an API graph to model the functional similarity relationships between APIs. Finally, we generate a minimum weighted tree to obtain a combination of APIs that meets the requirements and ensures diversity. We demonstrate the advantages of our proposed method in terms of diversity through a case study.},
 author = {Zhu, Minhao and Gu, Huanhuan and Che, Xun and Chen, Jingfei and Zhao, Qian and Liu, Fan and Zheng, Yu},
 title = {A Novel Diversified API Recommendation for Power System Sensors},
 keywords = {Accuracy;api recommendation;Data mining;industrial Internet of Things;Internet of Things;IoT sensor;Large language models;power system;Power systems;recommendation system;Recommender systems;Sensor systems;Smart grids;Soft sensors;Vectors},
 pages = {17--22},
 doi = {10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00027}
}


@inproceedings{Zimmermann.,
 abstract = {This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.},
 author = {Zimmermann, D. and Koziolek, A.},
 title = {GUI-Based Software Testing: An Automated Approach Using GPT-4 and Selenium WebDriver},
 keywords = {Calculators;Codes;Conferences;Deep Learning;GPT-4;language models;Selenium;Software testing;Test Automation;Training data;UI Testing},
 pages = {171--174-171--174},
 doi = {10.1109/ASEW60602.2023.00028}
}


