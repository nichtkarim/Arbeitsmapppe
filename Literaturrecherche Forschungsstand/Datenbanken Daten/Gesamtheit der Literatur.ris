TY  - GEN
AU  - Zhou, Y.
AU  - Wang, E.
AU  - Yang, W.
AU  - Ge, W.
AU  - Yang, S.
AU  - Zhang, Y.
AU  - Qu, W.
AU  - Xie, W.
T1  - XSShield: Defending Against Stored XSS Attacks Using LLM-Based Semantic Understanding
JO  - Applied Sciences (Switzerland)
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
AD  - College of Computer Science and Technology, National University of Defense Technology, No.137 Yanwachi Street, Changsha, 410073, China
Y1  - 2025
VL  - 15
IS  - 3348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000837418&doi=10.3390%2fapp15063348&partnerID=40&md5=e3d108cac2208e0bea7766301e2ebaf0
M3  - https://doi.org/10.3390/app15063348
KW  - LLM
KW  - prompt learning
KW  - stored XSS
KW  - Computer software selection and evaluation
KW  - Java programming language
KW  - Network security
KW  - Asynchrony
KW  - Cross Site Scripting Attacks
KW  - Prompt learning
KW  - Security threats
KW  - Semantics understanding
KW  - Stored XSS
KW  - Temporal and spatial
KW  - WEB application
KW  - Web applications
KW  - Semantics
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: E. Wang; College of Computer Science and Technology, National University of Defense Technology, Changsha, No.137 Yanwachi Street, 410073, China; email: wangenze18@nudt.edu.cn
N2  - Cross-site scripting attacks represent one of the major security threats facing web applications, with Stored XSS attacks becoming the predominant form. Compared to reflected XSS, stored XSS attack payloads exhibit temporal and spatial asynchrony between injection and execution, rendering traditional browserside defenses based on request-response differential analysis ineffective. This paper presents XSShield, the first detection framework that leverages a Large Language Model to understand JavaScript semantics to defend against Stored XSS attacks. Through a Prompt Optimizer based on gradient descent and UCB-R selection algorithms, and a Data Adaptor based on program dependence graphs, the framework achieves real-time and fine-grained code processing. Experimental evaluation shows that XSShield achieves 93% accuracy and an F1 score of 0.9266 on the GPT-4 model, improving accuracy by an average of 88.8% compared to existing solutions. The processing time, excluding model communication overhead, averages only 0.205 s, demonstrating practical deployability without significantly impacting user experience. © 2025 by the authors.
ER  -
TY  - JOUR
AU  - Sottilare, R. A.
AU  - Schwarz, J.
T1  - 7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15812 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007848472&partnerID=40&md5=e5128df3756c8517fa6afc67fe6c9eda
U1  - 03029743 (ISSN); 978-303192966-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332889
N2  - The proceedings contain 40 papers. The special focus in this conference is on Adaptive Instructional Systems. The topics include: Evaluating Adaptive Training for Nautical Rules of the Road; from Standardization to Personalization: Leveraging Learner Profiles to Tailor Education; an Adaptive Simulated Startup Financial Modeling Mentor Using a Large Language Model to Address Shortages in Skilled Advisors: Architecture and Design Considerations; integrating Adaptive Interventions into Learning Engineering Workflows; leveraging Deterministic Algorithms to Personalize Education and Enhance Student Success: The Story of an Engineered Learning Experience; curriculum Sequencing as a Generalised Travelling Salesperson Problem: A Novel Perspective on Learning Path Generation; evaluation of Difficulty-Based Adaptive Training Strategies on Simulator Flight Training Performance; designing an Adaptive Mobile Application for Learning Programming Among Computing Students from Marginalized Backgrounds; architecture for a Large Scale Learning Ecosystem; automated Response Generation Using Language Models: An Approach to Enhancing User Interaction; Metacognition in HCI: Designing Systems for Planning and Flexibility; stress and Performance: Understanding the Effect of Appraisal and Coping on Performance in Complex Tasks; exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty; macrocognition in Simulation Based Training: A Practical Application of Learning Engineering in a Complex Training Environment; public Safety Personnel Readiness Prediction: A Hybrid Model of Neurophysiological and Psychometric Data; understanding the Complexity of Music Improvisation: Leveraging Cognitive Models to Inform Adaptive Instruction Design; advancing Cognitive State Monitoring: Diagnosing Cognitive Control States Under Varying Automation Reliability Level; the Weaponization of Critical Thinking: Understanding the Dynamics of Human Memory.
ER  -
TY  - JOUR
AU  - Qazi, S.
AU  - Kadri, M. B.
AU  - Naveed, M.
AU  - Khawaja, B. A.
AU  - Khan, S. Z.
AU  - Alam, M. M.
AU  - Su’ud, M. M.
T1  - AI-Driven Learning Management Systems: Modern Developments, Challenges and Future Trends during the Age of ChatGPT
JO  - Computers, Materials and Continua
Y1  - 2024
VL  - 80
IS  - 2
SP  - 3289–3314
EP  - 3289–3314
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201391014&doi=10.32604%2fcmc.2024.048893&partnerID=40&md5=d978573cd61bce15c518b96e496e77b6
M3  - https://doi.org/10.32604/cmc.2024.048893
KW  - artificial intelligence (AI)
KW  - chatbots
KW  - ChatGPT
KW  - convolutional neural networks
KW  - Internet of Things (IoT)
KW  - Learning management systems
KW  - natural language processing
KW  - online education
KW  - Application Layer
KW  - Convolutional neural networks
KW  - Network layers
KW  - Network security
KW  - Artificial intelligence
KW  - Chatbots
KW  - Convolutional neural network
KW  - Internet of thing
KW  - Language processing
KW  - Learning management system
KW  - Natural language processing
KW  - Natural languages
KW  - On-line education
KW  - Quality of service
U1  - 15462218 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 16; Correspondence Address: S. Qazi; Department of Computer Science, Faculty of Engineering, Science and Technology, IQRA University, Karachi, 75500, Pakistan; email: sameer.hashmat@iqra.edu.pk; M. Naveed; Department of Computer Science, Faculty of Engineering, Science and Technology, IQRA University, Karachi, 75500, Pakistan; email: shmnaveed@hotmail.com
N2  - COVID-19 pandemic restrictions limited all social activities to curtail the spread of the virus. The foremost and most prime sector among those affected were schools, colleges, and universities. The education system of entire nations had shifted to online education during this time. Many shortcomings of Learning Management Systems (LMSs) were detected to support education in an online mode that spawned the research in Artificial Intelligence (AI) based tools that are being developed by the research community to improve the effectiveness of LMSs. This paper presents a detailed survey of the different enhancements to LMSs, which are led by key advances in the area of AI to enhance the real-time and non-real-time user experience. The AI-based enhancements proposed to the LMSs start from the Application layer and Presentation layer in the form of flipped classroom models for the efficient learning environment and appropriately designed UI/UX for efficient utilization of LMS utilities and resources, including AI-based chatbots. Session layer enhancements are also required, such as AI-based online proctoring and user authentication using Biometrics. These extend to the Transport layer to support real-time and rate adaptive encrypted video transmission for user security/privacy and satisfactory working of AI-algorithms. It also needs the support of the Networking layer for IP-based geolocation features, the Virtual Private Network (VPN) feature, and the support of Software-Defined Networks (SDN) for optimum Quality of Service (QoS). Finally, in addition to these, non-real-time user experience is enhanced by other AI-based enhancements such as Plagiarism detection algorithms and Data Analytics. © 2024 Tech Science Press. All rights reserved.
ER  -
TY  - JOUR
AU  - Win, K. T.
AU  - Ali, R.
AU  - Karapanos, E.
AU  - Papadopoulos, G. A.
AU  - Oyibo, K.
AU  - Vlahu-Gjorgievska, E.
T1  - 20th International Conference on Persuasive Technology, PERSUASIVE 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15711 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008943439&partnerID=40&md5=c8488713fec13137e2eaddad16f955b9
U1  - 03029743 (ISSN); 978-303194958-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 20th International Conference on Persuasive Technology, PERSUASIVE 2025; Conference date: 5 May 2025 through 7 May 2025; Conference code: 333519
N2  - The proceedings contain 23 papers. The special focus in this conference is on Persuasive Technology. The topics include: Personalized Social Proof for Persuasive Human-Robot Interaction; personalized Digital Interventions for Behavior Change: Insights from the MoM App Study; exploring the Potential and Limitations of Large Language Models to Control the Behavior of Embodied Persuasive Agents; insights into the Design of Ethical and Trustworthy Persuasive Technologies; effect of Competitive and Cooperative Learning Contexts in Controversial Information Search: Preliminary Results; the Heuristic Evaluation of Manipulative Interfaces; digital Persuasion: Understanding the Impact of Online Influencers on Public Opinion; lifeLink: The Design and Evaluation of an mHealth App for Caregivers Supporting Individuals with Suicidality; bridging Research and Practice in Persuasive Mobile Stress Management Apps: A 21-Year Comparative Analysis and Novel Design Framework; designing Behavior Change Support Systems for Recovery from Addictions: Mapping Software Features with Counseling Strategies; investigation of the Eye Donor Aust App’s Persuasiveness; MyHealthCore: Towards a Community-Engaged HIV Prevention Persuasive mHealth App for Black Communities in Canada; health Risk Management Using Persuasive Technology: A Scoping Review; evaluation of an Emotion-Aware Persuasive Framework Based on Peripheral Interaction for Reducing Physical Strain in Office Environments; on People’s Susceptibility to Persuasive Techniques in Social Engineering: Is It About the Technique or Their Readiness to Be Persuaded?; AMRageddon V1: The Design and Usability Evaluation of a Digital Escape Room Game for Antimicrobial Resistance Education Through Persuasive Technology; petBuddy: An Examination of Augmented Reality Mobile Health Game for Promoting Physical Activity; the Motivational Appeal of Persuasive Strategies in a Healthy Eating Behaviour Change Game; non-binary People are Harder to Persuade: Evidence and Insights.
ER  -
TY  - CONF
AU  - Donato, B.
T1  - Rethinking Software Development Considering Collaboration with AI Assistants
PB  - IEEE Computer Society
AD  - University of Milano, Bicocca, Italy
Y1  - 2025
SP  - 154–156
EP  - 154–156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498477&doi=10.1109%2fICSE-Companion66252.2025.00045&partnerID=40&md5=e358399ebbc93af95250861f44b1c716
M3  - https://doi.org/10.1109/ICSE-Companion66252.2025.00045
KW  - AI Assistants
KW  - HCI
KW  - LLMs
KW  - Artificial intelligence
KW  - Computer programming
KW  - Computer software selection and evaluation
KW  - Groupware
KW  - Software quality
KW  - AI assistant
KW  - Development practices
KW  - LLM
KW  - PhD thesis
KW  - Software Quality
KW  - Software design
U1  - 02705257 (ISSN); 979-833153683-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: B. Donato; University of Milano, Bicocca, Italy; email: benedetta.donato@unimib.it; Conference name: 47th IEEE/ACM International Conference on Software Engineering, ICSE-Companion 2025; Conference date: 27 April 2025 through 3 May 2025; Conference code: 209532; CODEN: PCSED
N2  - The integration of AI into software development is transforming coding practices, with tools like GitHub Copilot marking the beginning of AI-assisted development. However, the interaction between developers and AI assistants is not yet well understood, presenting both opportunities and challenges. This PhD thesis addresses three core objectives: (1) improving the effectiveness of developer-AI interactions, (2) redefining development practices to incorporate AI as an essential collaborator, and (3) exploring collaborative dynamics within mixed teams of developers and AI agents. The aim is to establish methodologies that can boost productivity and software quality, paving the way for seamless human-AI collaboration in development. © 2025 IEEE.
ER  -
TY  - JOUR
AU  - Lim, Z.-S.
AU  - Yulastri, A.
AU  - Ho, S.-B.
AU  - Tan, C.-H.
T1  - Enhancing Travel Planning Efficiency with a Comprehensive TripEase GenAI Mechanism
JO  - International Journal on Advanced Science, Engineering and Information Technology
Y1  - 2024
VL  - 14
IS  - 6
SP  - 2090–2097
EP  - 2090–2097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214292298&doi=10.18517%2fijaseit.14.6.11985&partnerID=40&md5=ee3d9c5b0be4b03d420da6667365a6de
M3  - https://doi.org/10.18517/ijaseit.14.6.11985
KW  - efficiency enhancement
KW  - Generative artificial intelligence
KW  - personalized journey
KW  - travel planning
U1  - 20885334 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: S.-B. Ho; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia; email: sbho@mmu.edu.my
N2  - This paper highlights the trip-easiness generative AI mechanism to simplify domestic and international travel planning through a comprehensive services range based on the users’ locations, including itinerary management and car rental. This research focuses on building TripEase GenAI Mechanism with generative artificial intelligence (AI) for a fully-fledged AI-based trip planner web application. This mechanism would function independently in addition to the alternative of seamless integration with another platform, a car rental and travel planning system. This would enhance the overall user experience. Within our TripEase GenAI mechanism, users can effortlessly create personalized recommended itineraries after providing their essential trip details. This mechanism will generate a complete itinerary. The suggested itinerary would accompany a tailored packing list specific to the destination. We are leveraging cutting-edge tools such as LangChain, Semantic Kernel, and ChatGPT to achieve this objective. Our main aim is to revolutionize the travel planning experience by harnessing the power of AI technology. This research is envisioned to provide travelers with a seamless and personalized journey that caters to their individual preferences and needs. This paper outlines the design, evolution, and planning of the TripEase GenAI mechanism. We have laid a strong foundation for a user-friendly travel planning platform through meticulous documentation, thoughtful design, and iterative development. The ultimate goal is to enhance the overall travel planning experience by efficiently enriching imported services such as itinerary management and car rental modules. © (2024), (Insight Society). All rights reserved.
ER  -
TY  - CONF
AU  - Metzger, L.
AU  - Miller, L.
AU  - Baumann, M.
AU  - Kraus, J.
T1  - Empowering Calibrated (Dis-)Trust in Conversational Agents: A User Study on the Persuasive Power of Limitation Disclaimers vs. Authoritative Style
PB  - Association for Computing Machinery
AD  - Ulm University, Ulm, Germany
Y1  - 2024
IS  - 481
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194897802&doi=10.1145%2f3613904.3642122&partnerID=40&md5=9e9740fe212eb211ff3825151da7ca83
M3  - https://doi.org/10.1145/3613904.3642122
KW  - chatbots
KW  - ChatGPT
KW  - communicative style
KW  - conversational agents
KW  - elaboration likelihood model
KW  - large language models
KW  - trust in automation
KW  - Social psychology
KW  - User interfaces
KW  - Chatbots
KW  - Communicative style
KW  - Conversational agents
KW  - Elaboration likelihood models
KW  - Language model
KW  - Large language model
KW  - Power
KW  - Trust in automation
KW  - User study
KW  - Computational linguistics
U1  - 979-840070330-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 7; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199441
N2  - While conversational agents based on Large Language Models (LLMs) can drive progress in many domains, they are prone to generating faulty information. To ensure an efficient, safe, and satisfactory user experience maximizing benefits of these systems, users must be empowered to judge the reliability of system outputs. In this, both disclaimers and agents' communicative style are pivotal design instances. In an online study with 594 participants, we investigated how these affect users' trust and a mock-up agent's persuasiveness, based on an established framework from social psychology. While prior information on potential inaccuracies or faulty information did not affect trust, an authoritative communicative style elicited more trust. Also, a trusted agent was more persuasive resulting in more positive attitudes regarding the subject of the conversation. Results imply that disclaimers on agents' limitations fail to effectively alter users' trust but can be supported by appropriate communicative style during interaction. © 2024 Copyright held by the owner/author(s)
ER  -
TY  - JOUR
AU  - Degen, H.
AU  - Ntoa, S.
T1  - 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th International Conference on Human-Computer Interaction, HCI International 2024
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Y1  - 2024
VL  - 14736 LNAI
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195462297&partnerID=40&md5=99422dfcb8af2fd67ac8407aeb0f3633
U1  - 03029743 (ISSN); 978-303160614-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th International Conference on Human-Computer Interaction, HCI International 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 312569
N2  - The proceedings contain 32 papers. The special focus in this conference is on Artificial Intelligence in HCI. The topics include: Using a LLM-Based Conversational Agent in the Social Robot Mini; a Proposal to Extend the Modeling Language for Interaction as Conversation for the Design of Conversational Agents; optimizing Conversational Commerce Involving Multilingual Consumers Through Large Language Models’ Natural Language Understanding Abilities; A Map of Exploring Human Interaction Patterns with LLM: Insights into Collaboration and Creativity; The Use of Large Language Model in Code Review Automation: An Examination of Enforcing SOLID Principles; LLM Based Multi-agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain; enabling Human-Centered Machine Translation Using Concept-Based Large Language Model Prompting and Translation Memory; enhancing Large Language Models Through External Domain Knowledge; ChatGPT and Language Translation; large Language Models for Tracking Reliability of Information Sources; the Heuristic Design Innovation Approach for Data-Integrated Large Language Model; FER-Pep: A Deep Learning Based Facial Emotion Recognition Framework for Humanoid Robot Pepper; you Got the Feeling: Attributing Affective States to Dialogical Social Robots; enhancing Usability of Voice Interfaces for Socially Assistive Robots Through Deep Learning: A German Case Study; enhancing User Experience: Designing Intuitive Interfaces for Sumo Robot Operations; Adaptive Robotics: Integrating Robotic Simulation, AI, Image Analysis, and Cloud-Based Digital Twin Simulation for Dynamic Task Completion; Building Information Model (BIM) and Robotic Systems Integration for Construction: A Comprehensive Workflow Analysis and Future Perspectives; emphasizing with a Robot with a Personality; embodying Intelligence: Humanoid Robot Advancements and Future Prospects; people and Technology: An Investigation of the Adoption of Artificial Intelligence in the Kinesiology Context; Logical Interference: Using AI to Correct Flaws in Human Judgment; text Analysis Software Using Topic Modeling Techniques for the Extraction of Knowledge from Cases Related to Vulnerability and Access to Justice.
ER  -
TY  - CONF
AU  - Salehi, P.
AU  - Hassan, S. Z.
AU  - Baugerud, G. A.
AU  - Powell, M.
AU  - López Cano, M. C.
AU  - Johnson, M. S.
AU  - Røed, R. K.
AU  - Johansen, D.
AU  - Sabet, S. S.
AU  - Riegler, M. A.
AU  - Halvorsen, P.
T1  - Immersive Virtual Reality in Child Interview Skills Training: A Comparison of 2D and 3D Environments
PB  - Association for Computing Machinery, Inc
AD  - UiT The Arctic University of Norway, Norway
Y1  - 2024
SP  - 1–7
EP  - 1–7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191427124&doi=10.1145%2f3652212.3652219&partnerID=40&md5=f20a96e1cdea7a266fa6cadc309a6d5f
M3  - https://doi.org/10.1145/3652212.3652219
KW  - Immersion
KW  - Large language model (LLM)
KW  - Quality of Experience (QoE)
KW  - Virtual Reality (VR)
KW  - Quality control
KW  - Quality of service
KW  - User experience
KW  - User interfaces
KW  - 3-D environments
KW  - Immersive virtual reality
KW  - Language model
KW  - Large language model
KW  - Quality of experience
KW  - Training effectiveness
KW  - Users' experiences
KW  - Virtual reality
KW  - Visual fidelity
U1  - 979-840070618-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 6; Conference name: 16th International Workshop on Immersive Mixed and Virtual Environment Systems, MMVE 2024; Conference date: 15 April 2024 through 18 April 2024; Conference code: 198785
N2  - The current study aims to evaluate and compare the subjective quality of an AI-based training system developed for conducting child interviews, focusing on the distinction between immersive 3D (using virtual reality) and 2D desktop environments. To this end, a structured user study was conducted, involving 36 participants who were exposed to these two distinct environments. The study evaluated various aspects of user experience, namely presence, usability, visual fidelity, emotion, responsiveness, appropriateness, and training effectiveness. The findings reveal significant differences in user experience between the 2D and 3D environments. Notably, the 3D environment enhanced presence, visual fidelity, training effectiveness, and empathy. In contrast, the 2D environment was favored for usability. The study highlights the potential of immersive VR while also pointing out the need to improve the system response and emotional expressiveness of the avatars. © 2024 ACM.
ER  -
TY  - JOUR
AU  - Coman, A.
AU  - Vasilache, S.
T1  - 17th International Conference on Social Computing and Social Media, SCSM 2025, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15786 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007158998&partnerID=40&md5=5114fb7568d7bd9ecc2d2533660f44dc
U1  - 03029743 (ISSN); 978-303193538-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 17th International Conference on Social Computing and Social Media, SCSM 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332669
N2  - The proceedings contain 51 papers. The special focus in this conference is on Social Computing and Social Media. The topics include: A Serious Game Approach for Teaching Requirements Engineering: User Experience Evaluation; identification of Older Adults’ Characteristics that Affect the Usability of Mobile Applications: A Tertiary Study; generating Product Descriptions Using Customer Reviews on E-Commerce Sites; improving Intention Recognition Efficiency: A Study on Skeletal Data Dimensionality Reduction and Neural Architectures; first Steps Toward the Agile Integration of Information Architecture into a User-Centered Development Process; LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded Robot Teleoperation; redimensioning Visible Learning and Teaching in the Dynamics of a New Reality; Generative AI in Education: Exploring EAP Faculty Perspectives at a Multicultural UAE University; the Challenges Faced by Albanian Teachers in the Use of Media Technology During Teaching; Undergraduate Students’ Journey with AI in the United Arab Emirates; Perspectives of Faculty on the Easiness and Usefulness of AI Tutoring Systems in Higher Education; exploring the Use of Paraphrasing Tools in Academic Writing and Its Potential Relation with Instances of Plagiarism; a Property Checklist for Evaluating the Student Experience with Consideration of Cultural Aspects; human-Robot Interaction in Higher Education: A Literature Review; artificial Intelligence in Higher Education: Student Perceptions of the Adoption and Integration in Ghana, West Africa; a Management Model for Evaluating Scientific Productivity in Chilean Universities: A Case Analysis; virtual Reality Meets Social Media: Transforming Skill Acquisition in Physiotherapy, Veterinary Surgery, and Driver Training; Evolution of Emotional Response of PLEA–An Embodied Virtual Being with Emotional Capabilities.
ER  -
TY  - CONF
AU  - Andrews, P.
AU  - Nordberg, O. E.
AU  - Zubicueta Portales, S.
AU  - Borch, N.
AU  - Guribye, F.
AU  - Fujita, K.
AU  - Fjeld, M.
T1  - AiCommentator: A Multimodal Conversational Agent for Embedded Visualization in Football Viewing
PB  - Association for Computing Machinery
AD  - Chalmers, Sweden
Y1  - 2024
SP  - 14–34
EP  - 14–34
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190975308&doi=10.1145%2f3640543.3645197&partnerID=40&md5=abe893eab477a5941b653c4942a530fc
M3  - https://doi.org/10.1145/3640543.3645197
KW  - Computer Vision
KW  - Conversational User Interface
KW  - Deep Learning
KW  - Embedded Visualization
KW  - Human-Computer Interaction
KW  - Multi-Object Tracking
KW  - Multimodal Conversational Agent
KW  - Usability Testing
KW  - Computer games
KW  - Deep learning
KW  - Human computer interaction
KW  - Sports
KW  - Tracking (position)
KW  - User interfaces
KW  - Visualization
KW  - Conversational user interface
KW  - Embedded visualization
KW  - Game development
KW  - Game players
KW  - Multi-object tracking
KW  - Multimodal conversational agents
KW  - Performance
KW  - Usability testing
KW  - Young adults
KW  - Computer vision
U1  - 979-840070508-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 9; Conference name: 29th Annual Conference on Intelligent User Interfaces, IUI 2024; Conference date: 18 March 2024 through 21 March 2024; Conference code: 198805
N2  - Traditionally, sports commentators provide viewers with diverse information, encompassing in-game developments and player performances. Yet young adult football viewers increasingly use mobile devices for deeper insights during football matches. Such insights into players on the pitch and performance statistics support viewers' understanding of game stakes, creating a more engaging viewing experience. Inspired by commentators' traditional roles and to incorporate information into a single platform, we developed AiCommentator, a Multimodal Conversational Agent (MCA) for embedded visualization and conversational interactions in football broadcast video. AiCommentator integrates embedded visualization, either with an automated non-interactive or with a responsive interactive commentary mode. Our system builds upon multimodal techniques, integrating computer vision and large language models, to demonstrate ways for designing tailored, interactive sports-viewing content. AiCommentator's event system infers game states based on a multi-object tracking algorithm and computer vision backend, facilitating automated responsive commentary. We address three key topics: evaluating young adults' satisfaction and immersion across the two viewing modes, enhancing viewer understanding of in-game events and players on the pitch, and devising methods to present this information in a usable manner. In a mixed-method evaluation (n=16) of AiCommentator, we found that the participants appreciated aspects of both system modes but preferred the interactive mode, expressing a higher degree of engagement and satisfaction. Our paper reports on our development of AiCommentator and presents the results from our user study, demonstrating the promise of interactive MCA for a more engaging sports viewing experience. Systems like AiCommentator could be pivotal in transforming the interactivity and accessibility of sports content, revolutionizing how sports viewers engage with video content. © 2024 Owner/Author.
ER  -
TY  - JOUR
AU  - Chen, X.
AU  - Shu, Y.
AU  - Wang, R.
AU  - Zhou, J.
AU  - Chen, W.
T1  - Large language model powered UI evaluation system
JO  - Journal of Graphics
Y1  - 2024
VL  - 45
IS  - 6
SP  - 1178–1187
EP  - 1178–1187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213856596&doi=10.11996%2fJG.j.2095-302X.2024061178&partnerID=40&md5=88e99621e21e6b346c181f240d0f1534
M3  - https://doi.org/10.11996/JG.j.2095-302X.2024061178
KW  - accessibility
KW  - consistency
KW  - graphical user interface
KW  - large language model
KW  - UI evaluation
U1  - 2095302X (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0
N2  - The quality of user interface (UI) design directly impacts product usability and user experience. Designers often face challenges related to consistency and accessibility during the UI design process, increasing cognitive load for users reducing efficiency. Despite awareness of these issues, they currently lack comprehensive knowledge and tools or automatic identification and resolution. To address this challenge, a comprehensive set of UI design evaluation criteria was proposed, covering five key aspects: color, text, layout, control, and icon, specifically targeting consistency and accessibility issues in UI design. Based on these evaluation criteria, a prompt template for evaluating UI consistency and accessibility was developed to enhance the accuracy of large language models (LLMs) like GPT-4 in UI evaluation tasks. Furthermore, a UI evaluation system based on the GPT-4 model was developed. This [26] deeply understood UI design content, automatically detected UI design issues according to the evaluation criteria, and provided targeted improvement suggestions to help designers optimize their UI designs. Experimental results demonstrated that using the prompt template significantly improved the accuracy of GPT-4 in UI evaluations. User studies indicated that employing this UI evaluation system in design practice can significantly enhance the quality of UI designs, thereby boosting product usability and user experience. This system provided designers with an automated UI evaluation tool, offering a new approach to enhancing UI design quality. © 2024 Editorial of Board of Journal of Graphics. All rights reserved.
ER  -
TY  - CONF
AU  - Bisante, A.
AU  - Datla, V. S. V.
AU  - Panizzi, E.
AU  - Trasciatti, G.
AU  - Zeppieri, S.
T1  - Enhancing Interface Design with AI: An Exploratory Study on a ChatGPT-4-Based Tool for Cognitive Walkthrough Inspired Evaluations
PB  - Association for Computing Machinery
AD  - Department of Computer Science, Sapienza University, Rome, Italy
Y1  - 2024
IS  - 41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195382725&doi=10.1145%2f3656650.3656676&partnerID=40&md5=3b4edad92d9079a7cc6c28a441bd4402
M3  - https://doi.org/10.1145/3656650.3656676
KW  - AI
KW  - ChatGPT
KW  - Cognitive Walkthrough
KW  - GPT
KW  - HCI
KW  - Application programs
KW  - Usability engineering
KW  - Cognitive walkthrough
KW  - Conversational agents
KW  - Exploratory studies
KW  - Interface designs
KW  - Subtask
KW  - WEB application
KW  - Web applications
KW  - Web interface
KW  - Students
U1  - 979-840071764-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 6; Conference name: 2024 International Conference on Advanced Visual Interfaces, AVI 2024; Conference date: 3 June 2024 through 7 June 2024; Conference code: 199914
N2  - This paper introduces CWGPT, a ChatGPT-4-based tool designed for Cognitive Walkthrough (CW) inspired evaluations of web interfaces. The primary goal is to assist users, particularly students and inexperienced designers, in evaluating web interfaces. Our tool, operating as a conversational agent, provides detailed evaluations of a user-specified task by intelligently guessing the subtasks and actions required to accomplish them, answering the standard CW questions, and providing helpful feedback and practical suggestions to improve the usability of the analyzed interface. For our study, we selected a group of web applications designed by students from a Web and Software Architecture course. We compare the outcome of the CWs we executed on ten web apps against the corresponding CWGPT analyses. We then describe the study we conducted involving five author-students to assess the tool's efficacy in helping them recognize and solve usability issues. In addition to introducing a novel adaptation of ChatGPT, the outcomes of the described experience underscore the promising potential of AI in usability evaluations. © 2024 Owner/Author.
ER  -
TY  - CONF
AU  - Odesola, O.
AU  - Alsmadi, H.
AU  - Al Kafari, A. S.
AU  - Kandasamy, G.
T1  - AI-Powered Instant Textual Feedback on Physiotherapist Student Practical Perfomance
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - School of Health & Life Sciences, Teeside University, Middlesbrough, United Kingdom
Y1  - 2024
SP  - 443–449
EP  - 443–449
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204362920&doi=10.1109%2fIMSA61967.2024.10652614&partnerID=40&md5=edc3c79d99ac3faf83f147a019c6eb4a
M3  - https://doi.org/10.1109/IMSA61967.2024.10652614
KW  - Automatic Feedback
KW  - Large Language Model
KW  - Physiotherapist
KW  - Artificial intelligence
KW  - Economic and social effects
KW  - AI systems
KW  - Attention mechanisms
KW  - Automatic feedback
KW  - Clinical skills
KW  - Keyword-based
KW  - Language model
KW  - Large language model
KW  - Perfomance
KW  - Performance
KW  - Students
U1  - 979-835036263-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2nd International Conference of Intelligent Methods, Systems and Applications, IMSA 2024; Conference date: 13 July 2024 through 14 July 2024; Conference code: 202356
N2  - This study addresses the need for comprehensive feedback in academic physiotherapy programs. Existing methods often fall short of providing coherent feedback using keywords, leaving a gap in evaluating crucial clinical skills. Introducing iAtexF, a Keyword-Based AI system, it offers instant feedback during practical exams, categorized into 'Good', 'Improvement', and 'Read more', aiding students in understanding their performance. Utilizing a Seq2seq framework with diverse LSTM and attention mechanisms, iAtexF excels in relevance, achieving a high similarity score of 73% and a ROUGE score of 34%. It surpasses both ChatGPT and experts in providing relevant suggestions (80.7%), maintaining an appropriate tone (86.7%), and ensuring a logical structure order (100%). User experience evaluation of the iAtexF web application yielded a favourable 92% rating, indicating its usability and effectiveness. This research signifies a significant advancement in educational technology and natural language processing, enhancing physiotherapy training through personalized AI-generated feedback, thereby improving the overall learning experience. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Biyani, P.
AU  - Bajpai, Y.
AU  - Radhakrishna, A.
AU  - Soares, G.
AU  - Gulwani, S.
AU  - Adams, B.
AU  - Zimmermann, T.
AU  - Ozkaya, I.
AU  - Lin, D.
AU  - Zhang, J. M.
T1  - RUBICON: Rubric-Based Evaluation of Domain-Specific Human AI Conversations
PB  - Association for Computing Machinery, Inc
AD  - Microsoft, Redmond, United States
Y1  - 2024
SP  - 161–169
EP  - 161–169
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199862151&doi=10.1145%2f3664646.3664778&partnerID=40&md5=f9b178be0e3799985251112d8dc738c2
M3  - https://doi.org/10.1145/3664646.3664778
KW  - AI-assisted Programming
KW  - Conversation Evaluation
KW  - Conversational AI
KW  - Evaluation Metrics
KW  - Human-AI interaction
KW  - User Satisfaction
KW  - Computational linguistics
KW  - Software engineering
KW  - User interfaces
KW  - AI-assisted programming
KW  - Conversation evaluation
KW  - Domain specific
KW  - Evaluation metrics
KW  - Language model
KW  - Performance
KW  - Users' experiences
KW  - Users' satisfactions
KW  - Quality control
U1  - 979-840070685-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: Y. Bajpai; Microsoft, Bengaluru, India; email: ybajpai@microsoft.com; Conference name: 1st ACM International Conference on AI-Powered Software, AIware 2024, co-located with the ACM International Conference on the Foundations of Software Engineering, FSE 2024; Conference date: 15 July 2024 through 16 July 2024; Conference code: 201062
N2  - Evaluating conversational assistants, such as GitHub Copilot Chat, poses a significant challenge for tool builders in the domain of Software Engineering. These assistants rely on language models and chat-based user experiences, rendering their evaluation with respect to the quality of the Human-AI conversations complicated. Existing general-purpose metrics for measuring conversational quality found in literature are inadequate for appraising domain-specific dialogues due to their lack of contextual sensitivity. In this paper, we present RUBICON, a technique for evaluating domain-specific Human-AI conversations. RUBICON leverages large language models to generate candidate rubrics for assessing conversation quality and employs a selection process to choose the subset of rubrics based on their performance in scoring conversations. In our experiments, RUBICON effectively learns to differentiate conversation quality, achieving higher accuracy and yield rates than existing baselines. © 2024 Owner/Author.
ER  -
TY  - JOUR
AU  - Martin, H.
AU  - James, J.
AU  - Chadee, A.
T1  - Exploring Large Language Model AI tools in Construction Project Risk Assessment: Chat GPT Limitations in Risk Identification, Mitigation Strategies, and User Experience
JO  - Journal of Construction Engineering and Management
Y1  - 2025
VL  - 151
IS  - 04025119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009096808&doi=10.1061%2fJCEMD4.COENG-16658&partnerID=40&md5=ecfedf54ad5c25392bb61ffdd71a18fb
M3  - https://doi.org/10.1061/JCEMD4.COENG-16658
KW  - Artificial intelligence (AI)
KW  - ChatGPT
KW  - Construction
KW  - Large language model
KW  - Natural language processing
KW  - Project management
KW  - Prompt
KW  - Risk management
KW  - Artificial intelligence
KW  - Behavioral research
KW  - Construction industry
KW  - Efficiency
KW  - Natural language processing systems
KW  - Risk analysis
KW  - Risk assessment
KW  - Risk perception
KW  - User experience
KW  - User interfaces
KW  - Decisions makings
KW  - Language model
KW  - Language processing
KW  - Natural languages
KW  - Risks management
U1  - 07339364 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: H. Martin; School of Natural and Built Environment, Queens Univ. Belfast, Belfast, Elmwood Bldg., BT7 1NN, United Kingdom; email: hector.martin@qub.ac.uk; CODEN: JCEMD
N2  - The last 3 years have witnessed an increasing awareness and consensus on using artificial intelligence (AI) to enhance decision-making in the construction sector. This study explores the integration of ChatGPT (Generative Pre-trained Transformer) into traditional risk management frameworks within the construction industry, contributing to the ongoing discourse on AI's role in enhancing risk identification, analysis, and mitigation. Using a mixed-method approach comparing ChatGPT-assisted to human evaluations, interviews, and a case study, the research develops a better understanding of construction risk analysis processes and discusses decision-making errors of omission, over- and underestimation of probabilities and impacts, and treatment of the residual risk after proposed mitigation strategies. Results suggest that users' experience of ChatGPT is primarily favorable, characterized by quick responses and an intuitive interface that enhances decision-making efficiency. Findings indicate that GPT may be especially beneficial for less experienced practitioners since it provides comprehensive risk awareness. However, experienced professionals contend that the software lacks contextual depth. The study contributes a ChatGPT-4 prompt to evaluate infrastructure risk for a given project scope. An evidenced case study on a road upgrade project in Ireland demonstrates a lessened dependence on the quality of user prompting skills and emphasizes the quality of project scope data input. A collaborative approach, including Chat-GPT early involvement and human refinement, promises to enhance conventional risk management speed and efficiency and reduce bias and inflexibility while maintaining the adaptability and ethical rigour required in the industry's evolving risk landscape. © 2025 This work is made available under the terms of the Creative Commons Attribution 4.0 International license,.
ER  -
TY  - CONF
AU  - Constantinides, N.
AU  - Constantinides, A.
AU  - Koukopoulos, D.
AU  - Fidas, C.
AU  - Belk, M.
T1  - CulturAI: Exploring Mixed Reality Art Exhibitions with Large Language Models for Personalized Immersive Experiences
PB  - Association for Computing Machinery, Inc
AD  - University of Patras, Patras, Greece
Y1  - 2024
SP  - 102–105
EP  - 102–105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198910809&doi=10.1145%2f3631700.3664874&partnerID=40&md5=952d82629a3fcdc6e2a960dd532b2c09
M3  - https://doi.org/10.1145/3631700.3664874
KW  - Large Language Models
KW  - Mixed Reality
KW  - User Experience Evaluation
KW  - User Study
KW  - Computational linguistics
KW  - User interfaces
KW  - Immersive
KW  - Language model
KW  - Large language model
KW  - Mixed reality
KW  - Mixed reality art
KW  - Mixed reality technologies
KW  - Model-based OPC
KW  - User experience evaluations
KW  - User study
KW  - Users' experiences
U1  - 979-840070466-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 12; Conference name: 32nd ACM Conference on User Modeling, Adaptation and Personalization, UMAP 2024; Conference date: 1 July 2024 through 4 July 2024; Conference code: 200697
N2  - Mixed Reality (MR) technologies have transformed the way in which we interact and engage with digital content, offering immersive experiences that blend the physical and virtual worlds. Over the past years, there has been increasing interest in employing Artificial Intelligence (AI) technologies to improve user experience and trustworthiness in cultural contexts. However, the integration of Large Language Models (LLMs) into MR applications within the Cultural Heritage (CH) domain is relatively underexplored. In this work, we present an investigation into the integration of LLMs within MR environments, focusing on the context of virtual art exhibitions. We implemented a HoloLens MR application, which enables users to explore artworks while interacting with an LLM through voice. To evaluate the user experience and perceived trustworthiness of individuals engaging with an LLM-based virtual art guide, we adopted a between-subject study design, in which participants were randomly assigned to either the LLM-based version or a control group using conventional interaction methods. The LLM-based version allows users to pose inquiries about the artwork displayed, ranging from details about the creator to information about the artwork's origin and historical significance. This paper presents the technical aspects of integrating LLMs within MR applications and evaluates the user experience and perceived trustworthiness of this approach in enhancing the exploration of virtual art exhibitions. Results of an initial evaluation provide evidence about the positive aspect of integrating LLMs in MR applications. Findings of this work contribute to the advancement of MR technologies for the development of future interactive personalized art experiences. © 2024 Owner/Author.
ER  -
TY  - JOUR
AU  - Ponce, P.
AU  - Rojas, M.
AU  - Mendez, J. I.
AU  - Anthony, B.
AU  - Bradley, R.
AU  - Fayek, A. R.
T1  - Smart City Products and Their Materials Assessment Using the Pentagon Framework
JO  - Multimodal Technologies and Interaction
Y1  - 2025
VL  - 9
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215811471&doi=10.3390%2fmti9010001&partnerID=40&md5=ac17b5c995a05e8210bdb5b349a088d2
M3  - https://doi.org/10.3390/mti9010001
KW  - data-driven solutions
KW  - Penta-S framework
KW  - smart cities
KW  - smart citizens
KW  - smart city assessment
KW  - smart communities
KW  - user-centric design
U1  - 24144088 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: P. Ponce; Institute of Advanced Materials for Sustainable Manufacturing, Tecnologico de Monterrey, Mexico, 14380, Mexico; email: pedro.ponce@tec.mx
N2  - Smart cities are complex urban environments that rely on advanced technology and data analytics to enhance city services’ quality of life, sustainability, and efficiency. As these cities continue to evolve, there is a growing need for a structured framework to evaluate and integrate products that align with smart city objectives. This paper introduces the Pentagon Framework, a comprehensive evaluation method designed to ensure that products and their materials meet the specific needs of smart cities. The framework focuses on five key features–smart, sustainable, sensing, social, and safe–collectively called the Penta-S concept. These features provide a structured approach to categorizing and assessing products, ensuring alignment with the city’s goals for efficiency, sustainability, and user experience. The Smart City Pentagon Framework Analyzer is also presented, a dedicated web application that facilitates interaction with the framework. It allows product data input, provides feedback on alignment with the Penta-S features, and suggests personality traits based on the OCEAN model. Complementing the web application, the Smart City Penta-S Compliance Assistant API, developed through ChatGPT, offers a more profound, personalized evaluation of products, including the life cycle phase recommendations using the IPPMD model. This paper contributes to the development of smart city solutions by providing a flexible framework that can be applied to any product type, optimizing its life cycle, and ensuring compliance with the Pentagon Framework. This approach improves product integration and fosters user satisfaction by tailoring products and their materials to meet specific user preferences and needs within the smart city environment. The proposed framework emphasizes citizen-centric design and highlights its advantages over conventional evaluation methods, ultimately enhancing urban planning and smart city development. © 2024 by the authors.
ER  -
TY  - CONF
AU  - Ito, T.
AU  - Nakagawa, S.
T1  - Tender Document Analyzer with the Combination of Supervised Learning and LLM-based Improver
PB  - Association for Computing Machinery, Inc
AD  - Mitsui & CO., LTD, Japan
Y1  - 2024
SP  - 995–998
EP  - 995–998
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194497077&doi=10.1145%2f3589335.3651233&partnerID=40&md5=e7ebcc7137127650795bdbcd0e40bb41
M3  - https://doi.org/10.1145/3589335.3651233
KW  - support system
KW  - tender document
KW  - text mining
KW  - Data mining
KW  - Petroleum reservoir evaluation
KW  - Supervised learning
KW  - Data set
KW  - Language model
KW  - Model-based OPC
KW  - Sequence Labeling
KW  - Small training
KW  - Specialized knowledge
KW  - Support systems
KW  - Tender documents
KW  - Text-mining
KW  - Training dataset
KW  - Function evaluation
U1  - 979-840070172-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Conference name: 33rd Companion of the ACM World Wide Web Conference, WWW 2023; Conference date: 13 May 2024 through 17 May 2024; Conference code: 199461
N2  - Bidders often take a long time to read and understand tender documents because they require specialized knowledge, and tender documents are generally long. Bidders first overview the specific items, such as payment and warranty, in a tender document and then check the overall document. Therefore, the function that can extract specific items (i.e., item extractor) and the function that can highlight words or phrases related to specific items (i.e., word-phrase highlighter) are in great demand. To develop the above two types of functions, we need to solve two problems. The first problem is the problem related to the annotated data set. The second problem concerns the BERT NER-based prediction approach in a small training dataset setting. To solve the first problem, we created two types of sequence labeling datasets related to Item Extractor and Word-Phrase Highlighter. To solve the second problem, we propose the Information Extraction (IE) method, which combines (1) a supervised learning approach using Bidirectional Encoder Representations from Transformers (BERT) and (2) a large language model (LLM)-based improver. We then developed the web application system called Tender Document Analyzer (TDDA), which includes "Item Extractor" and "Word-Phrase Highlighter". Experimental evaluation shows that our approach is practical. Firstly, the evaluation for extraction ability shows that the performance of our proposed method is much higher than the baseline approach that uses GPT 3.5, as well as demonstrates that the proposed LLM-based improver can improve the IE ability. In addition, the usability evaluation shows that bidders can solve the task in less time using our system. © 2024 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Khurana, A.
AU  - Su, X.
AU  - Wang, A. Y.
AU  - Chilana, P. K.
T1  - Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software
PB  - Association for Computing Machinery
AD  - Computer Science, Eth Zürich, Zürich, Switzerland
Y1  - 2025
IS  - 880
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005740844&doi=10.1145%2f3706598.3713431&partnerID=40&md5=bca5e2f7a1c0c5282da1ba007cea7fd4
M3  - https://doi.org/10.1145/3706598.3713431
KW  - feature-rich software
KW  - human-AI collaboration
KW  - large language models
KW  - semi-automation
KW  - software copilots
KW  - user control
KW  - Computer software selection and evaluation
KW  - Enterprise software
KW  - High level languages
KW  - Mobile applications
KW  - Search engines
KW  - Feature-rich software
KW  - Human-AI collaboration
KW  - Language model
KW  - Large language model
KW  - Model-based OPC
KW  - Semi-automation
KW  - Software copilot
KW  - Software tasks
KW  - User control
KW  - User perceptions
KW  - Utility programs
U1  - 979-840071394-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025; Conference date: 26 April 2025 through 1 May 2025; Conference code: 208607
N2  - Large Language Model (LLM)-based in-application assistants, or copilots, can automate software tasks, but users often prefer learning by doing, raising questions about the optimal level of automation for an effective user experience. We investigated two automation paradigms by designing and implementing a fully automated copilot (AutoCopilot) and a semi-automated copilot (GuidedCopilot) that automates trivial steps while offering step-by-step visual guidance. In a user study (N=20) across data analysis and visual design tasks, GuidedCopilot outperformed AutoCopilot in user control, software utility, and learnability, especially for exploratory and creative tasks, while AutoCopilot saved time for simpler visual tasks. A follow-up design exploration (N=10) enhanced GuidedCopilot with task-and state-aware features, including in-context preview clips and adaptive instructions. Our findings highlight the critical role of user control and tailored guidance in designing the next generation of copilots that enhance productivity, support diverse skill levels, and foster deeper software engagement. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ER  -
TY  - CONF
AU  - Nandy, P.
AU  - Adalgeirsson, S. O.
AU  - Sinha, A. K.
AU  - Kraljic, T.
AU  - Cleron, M.
AU  - Shi, L.
AU  - Singh, A.
AU  - Chaudhary, A.
AU  - Ganti, A.
AU  - Melancon, C. A.
AU  - Zhang, S.
AU  - Robishaw, D.
AU  - Ciurdar, H.
AU  - Secor, J.
AU  - Robertsen, K. A.
AU  - Climer, K.
AU  - Le, M.
AU  - Venkatesan, M.
AU  - Chi, P.
AU  - Li, P.
AU  - McDermott, P. F.
AU  - Shim, R.
AU  - Onsan, S.
AU  - Vaishnav, S.
AU  - Guamán, S.
T1  - Bespoke: Using LLM agents to generate just-in-time interfaces by reasoning about user intent
PB  - Association for Computing Machinery
AD  - Google DeepMind, Mountain View, CA, United States
Y1  - 2024
SP  - 78–81
EP  - 78–81
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211118801&doi=10.1145%2f3686215.3688372&partnerID=40&md5=abc992917e4912cec33bce47913b8ada
M3  - https://doi.org/10.1145/3686215.3688372
KW  - Agents
KW  - Generated UI
KW  - HCI
KW  - LLM
KW  - Graphical user interfaces
KW  - Chatbots
KW  - Graphical interface
KW  - Just-in-time
KW  - Language model
KW  - Large language model
KW  - Model agents
KW  - Modeling architecture
KW  - Reasoning process
KW  - Users' experiences
U1  - 979-840070463-5 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: P. Nandy; Google DeepMind, Mountain View, United States; email: palash@google.com; Conference name: 26th International Conference on Multimodal Interaction, ICMI Companion 2024; Conference date: 4 November 2024 through 8 November 2024; Conference code: 204372
N2  - Large language models (LLMs) have emerged as a powerful tool for creating personalized knowledge experiences for users, often serving as their own interface through text-based chatbots. The interpretation of user intent and generation of output occur implicitly within the model's architecture. We propose an alternative approach in a system we call Bespoke where the LLM acts as an agent to explicitly reason about user intent, plan, and generate graphical interfaces to fulfill that intent. This approach enables the creation of visually rich interactions that complement chat-based interactions. By employing a step-by-step reasoning process to reduce ambiguity and keep the model on track, we compose interfaces from a toolkit of widgets, providing a designed and tailored user experience. Our early experiment shows that the output interface differs depending on the interpreted intent. In the current version, these interactions are multimodal in the automatic generation of UI; in future versions, this paradigm can be extended to multiple modalities of input and output. This agentive approach moves the interface towards a personalized, bespoke experience with multimodal interaction that adapts to the user's intentions. See video demonstration here [2]. © 2024 Owner/Author.
ER  -
TY  - CONF
AU  - Gunathilaka, S.
AU  - de Silva, N.
AU  - Rocha, A. P.
AU  - Steels, L.
AU  - van den Herik H.J.
T1  - Automatic Analysis of App Reviews Using LLMs
PB  - Science and Technology Publications, Lda
AD  - Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka
Y1  - 2025
VL  - 2
SP  - 828–839
EP  - 828–839
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001692633&doi=10.5220%2f0013375600003890&partnerID=40&md5=cd54669e0220f69e2d298771177eaffb
M3  - https://doi.org/10.5220/0013375600003890
KW  - App Review Analysis
KW  - Fine-Tuning
KW  - GPT-3.5
KW  - Large Language Models
KW  - Mobile Applications
KW  - Natural Language Processing
KW  - Open-Source Models
KW  - Review Classification
KW  - Software Evolution
KW  - Zero-Shot Learning
U1  - 21843589 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 17th International Conference on Agents and Artificial Intelligence, ICAART 2025; Conference date: 23 February 2025 through 25 February 2025; Conference code: 328949
N2  - Large Language Models (LLMs) have shown promise in various natural language processing tasks, but their effectiveness for app review classification to support software evolution remains unexplored. This study evaluates commercial and open-source LLMs for classifying mobile app reviews into bug reports, feature requests, user experiences, and ratings. We compare the zero-shot performance of GPT-3.5 and Gemini Pro 1.0, finding that GPT-3.5 achieves superior results with an F1 score of 0.849. We then use GPT-3.5 to autonomously annotate a dataset for fine-tuning smaller open-source models. Experiments with Llama 2 and Mistral show that instruction fine-tuning significantly improves performance, with results approaching commercial models. We investigate the trade-off between training data size and the number of epochs, demonstrating that comparable results can be achieved with smaller datasets and increased training iterations. Additionally, we explore the impact of different prompting strategies on model performance. Our work demonstrates the potential of LLMs to enhance app review analysis for software engineering while highlighting areas for further improvement in open-source alternatives. © 2025 by SCITEPRESS - Science and Technology Publications, Lda.
ER  -
TY  - CONF
AU  - Kang, L.
AU  - Ai, J.
AU  - Lu, M.
T1  - Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Beihang University, School of Reliability and Systems Engineering, Beijing, China
Y1  - 2024
SP  - 132–140
EP  - 132–140
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216918617&doi=10.1109%2fDSA63982.2024.00027&partnerID=40&md5=29ebf94f26cd19c947bf696664f8aba2
M3  - https://doi.org/10.1109/DSA63982.2024.00027
KW  - LLM
KW  - Software Test
KW  - Test Case
KW  - Application programs
KW  - Automatic test pattern generation
KW  - Computer simulation languages
KW  - Computer software selection and evaluation
KW  - Man machine systems
KW  - Modeling languages
KW  - Natural language processing systems
KW  - Computer interaction
KW  - Element extraction
KW  - Language model
KW  - Large language model
KW  - Natural languages
KW  - Software test
KW  - Structural tests
KW  - Structured tests
KW  - Test case
KW  - Test elements
KW  - Software testing
U1  - 979-833153239-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: M. Lu; Beihang University, School of Reliability and Systems Engineering, Beijing, China; email: lmy@buaa.edu.cn; Conference name: 11th International Conference on Dependable Systems and Their Applications, DSA 2024; Conference date: 2 November 2024 through 3 November 2024; Conference code: 205763
N2  - As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Wang, S.
AU  - Wang, S.
AU  - Fan, Y.
AU  - Li, X.
AU  - Liu, Y.
T1  - Leveraging Large Vision-Language Model for Better Automatic Web GUI Testing
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - The Hong Kong University of Science and Technology, Hong Kong
Y1  - 2024
SP  - 125–137
EP  - 125–137
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215514657&doi=10.1109%2fICSME58944.2024.00022&partnerID=40&md5=eba9dbb0c33edd73b24a2500d79b8fb9
M3  - https://doi.org/10.1109/ICSME58944.2024.00022
KW  - Automatic Web GUI Testing
KW  - Large Language Model
KW  - Large Vision-Language Model
KW  - Text Input Generation
KW  - Ability testing
KW  - Benchmarking
KW  - Computer debugging
KW  - Computer software selection and evaluation
KW  - Failure analysis
KW  - Graphical user interfaces
KW  - Input output programs
KW  - Model checking
KW  - Problem oriented languages
KW  - Program debugging
KW  - Software testing
KW  - Visual languages
KW  - Automatic web GUI testing
KW  - End to end
KW  - GUI testing
KW  - Language model
KW  - Large language model
KW  - Large vision-language model
KW  - Text input
KW  - Text input generation
KW  - Web testing
KW  - Application programs
U1  - 979-835039568-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: Y. Liu; Southern University of Science and Technology, Research Institute of Trustworthy Autonomous Systems, Shenzhen, China; email: liuyp1@sustech.edu.cn; Conference name: 40th IEEE International Conference on Software Maintenance and Evolution, ICSME 2024; Conference date: 6 October 2024 through 11 October 2024; Conference code: 205391
N2  - With the rapid development of web technology, more and more software applications have become web-based in the past decades. To ensure software quality and user experience, various techniques have been proposed to automatically test web applications by interacting with their GUIs. To achieve high functional coverage, web GUI testing tools often need to generate high-quality text inputs and interact with the associated GUI elements (e.g., click submit buttons). However, developing a holistic approach that solves both subtasks is challenging because the web GUI context can be complicated and highly dynamic, which is hard to process programmatically. The recent development of large vision-language models (LVLM) provides new opportunities to handle these longstanding problems. We in this paper propose VETL, the first LVLM-driven end-to-end web testing technique. With LVLM's scene understanding capabilities, VETL can generate valid and meaningful text inputs focusing on the local context, while avoiding the need to extract precise textual attributes. The selection of associated GUI elements is formulated as a visual question answering problem, allowing LVLM to capture the logical connection between the input box and the relevant element based on visual instructions. Further, the GUI exploration is guided by a multi-armed bandit module employing a curiosity-oriented strategy. Experiments show that VETL is effective in exploring web state/action spaces and detecting bugs. Compared with WebExplor, the state-of-the-art web testing technique, VETL can discover 25% more unique web actions on benchmark websites. Moreover, it can expose functional bugs in top-ranking commercial websites, which have been confirmed by the website maintainers. Our work makes the first attempt of leveraging LVLM in end-to-end GUI testing, demonstrating promising results of this research direction. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Gao, J.
AU  - Gebreegziabher, S. A.
AU  - Choo, K. T. W.
AU  - Li, T. J.-J.
AU  - Perrault, S. T.
AU  - Malone, T. W.
T1  - A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration
PB  - Association for Computing Machinery
AD  - Massachusetts Institute of Technology, Cambridge, MA, United States
Y1  - 2024
IS  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194143130&doi=10.1145%2f3613905.3650786&partnerID=40&md5=e6c6fa12035db195c92ced21ebb9456f
M3  - https://doi.org/10.1145/3613905.3650786
KW  - Human-LLM Interaction
KW  - Large Language Models
KW  - Taxonomy
KW  - Human engineering
KW  - Iterative methods
KW  - User interfaces
KW  - Complex task
KW  - Flow planning
KW  - Flow testing
KW  - Human-LLM interaction
KW  - Initial exploration
KW  - Interaction modes
KW  - Interface mode
KW  - Language model
KW  - Large language model
KW  - Systematic analysis
KW  - Taxonomies
U1  - 979-840070331-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 12; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI EA 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199442
N2  - With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow-planning, facilitating, iterating, and testing-to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the "5W1H" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction. © 2024 Association for Computing Machinery. All rights reserved.
ER  -
TY  - JOUR
AU  - Casheekar, A.
AU  - Lahiri, A.
AU  - Rath, K.
AU  - Prabhakar, K. S.
AU  - Srinivasan, K.
T1  - A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions
JO  - Computer Science Review
Y1  - 2024
VL  - 52
IS  - 100632
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189750212&doi=10.1016%2fj.cosrev.2024.100632&partnerID=40&md5=7b682286ea2343c194b1e0dff4fba85b
M3  - https://doi.org/10.1016/j.cosrev.2024.100632
KW  - Artificial intelligence
KW  - Chatbots
KW  - ChatGPT
KW  - Computational intelligence
KW  - Conversational agents
KW  - Ethical technology
KW  - User interfaces
KW  - Data collection
KW  - Design Analysis
KW  - Future research directions
KW  - In-depth analysis
KW  - Literature reviews
KW  - Review papers
KW  - Training data
KW  - Decision making
U1  - 15740137 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 69; Correspondence Address: K. Srinivasan; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India; email: kathiravan.srinivasan@vit.ac.in
N2  - This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI's ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT's applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT. © 2024 Elsevier Inc.
ER  -
TY  - CONF
AU  - Fontana, M.
AU  - Martini, B.
AU  - Sciarrone, F.
T1  - Exploring Large Language Models in Intent Acquisition and Translation
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Universitas Mercatorum, Faculty of Technological and Innovation Sciences, Italy
Y1  - 2024
SP  - 231–234
EP  - 231–234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199554368&doi=10.1109%2fNetSoft60951.2024.10588924&partnerID=40&md5=44f172f8c0b8b2c47353994679fb394e
M3  - https://doi.org/10.1109/NetSoft60951.2024.10588924
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Translation (languages)
KW  - Academic research
KW  - Automatic management
KW  - Language model
KW  - Language processing techniques
KW  - Management operation
KW  - Natural languages
KW  - Networks management
KW  - Oriented features
KW  - User oriented
KW  - Users' experiences
KW  - Graphical user interfaces
U1  - 979-835036958-8 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: M. Fontana; Universitas Mercatorum, Faculty of Technological and Innovation Sciences, Italy; email: mattia.fontana@studenti.unimercatorum.it; Conference name: 10th IEEE International Conference on Network Softwarization, NetSoft 2024; Conference date: 24 June 2024 through 28 June 2024; Conference code: 200962
N2  - Intent-based networking has attracted interest in the academic research for enhancing network management operations with user-oriented features. One of the main challenge in this field is the acquisition of the user intents and subsequently the relative translation into policies for the automatic management of the network. Concerning this task, the primary technique employed is relying on Graphical User Interfaces (GUI)s. In addition, the use of Natural Language Processing techniques has been extensively adopted for improved user experience. Recently, some preliminary studies have shown that using Large Language Models (LLMs) for this purpose leads to achieve interesting results. However, based on a comprehensive analysis of the state of the art, it has emerged that the works utilizing the LLMs do not fully exploit all the capabilities these tools could potentially offer. For this reason, the doctoral work aims to address the following challenges: enhancing user experience through the utilization of intelligent chatbots, improving the correct understanding of user intents and ensuring the translation of user intentions into a coherent set of network configurations, which are generated automatically. © 2024 IEEE.
ER  -
TY  - JOUR
AU  - Wang, Y.
AU  - Zhu, T.
AU  - Zhou, T.
AU  - Wu, B.
AU  - Tan, W.
AU  - Ma, K.
AU  - Yao, Z.
AU  - Wang, J.
AU  - Li, S.
AU  - Qin, F.
AU  - Xu, Y.
AU  - Tan, L.
AU  - Liu, J.
AU  - Wang, J.
T1  - Hyper-DREAM, a Multimodal Digital Transformation Hypertension Management Platform Integrating Large Language Model and Digital Phenotyping: Multicenter Development and Initial Validation Study
JO  - Journal of Medical Systems
Y1  - 2025
VL  - 49
IS  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001710787&doi=10.1007%2fs10916-025-02176-1&partnerID=40&md5=b70c8da7fa70b9111e6e2002897419ce
M3  - https://doi.org/10.1007/s10916-025-02176-1
KW  - ChatGPT
KW  - Digital health
KW  - Digital phenotyping
KW  - Digital transformation
KW  - Hypertension management
KW  - Large language models
KW  - Mobile health
KW  - Multimodal intervention
KW  - Precision medicine
KW  - Female
KW  - Humans
KW  - Hypertension
KW  - Male
KW  - Middle Aged
KW  - Mobile Applications
KW  - Phenotype
KW  - Telemedicine
KW  - antihypertensive agent
KW  - adult
KW  - Article
KW  - artificial intelligence
KW  - behavior change
KW  - behavior modification
KW  - blood pressure measurement
KW  - clinical assessment
KW  - clinical practice
KW  - cohort analysis
KW  - controlled study
KW  - digital health
KW  - digital transformation
KW  - female
KW  - health care management
KW  - human
KW  - hypertension
KW  - hypertensive patient
KW  - large language model
KW  - major clinical study
KW  - male
KW  - middle aged
KW  - multidisciplinary team
KW  - patient care
KW  - patient coding
KW  - patient engagement
KW  - patient satisfaction
KW  - personalized medicine
KW  - phenotype
KW  - questionnaire
KW  - social media
KW  - telehealth
KW  - usability testing
KW  - validation study
KW  - workload
KW  - clinical trial
KW  - mobile application
KW  - multicenter study
KW  - organization and management
KW  - telemedicine
U1  - 01485598 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J. Liu; Department of Cardiology, The First Affiliated Hospital of Bengbu Medical Universtiy, Bengbu City, 287 Changhuai Road, Longzihu District, Anhui Province, 430060, China; email: byyfyliujinjun@163.com; J. Wang; Department of Cardiology, The First Affiliated Hospital of Bengbu Medical Universtiy, Bengbu City, 287 Changhuai Road, Longzihu District, Anhui Province, 430060, China; email: junwang0607@163.com; L. Tan; Institute of Clinical Medicine and Department of Cardiology, Renmin Hospital, Hubei University of Medicine, Shiyan, Hubei, 442000, China; email: tanliguo2001@126.com; CODEN: JMSYD
N2  - Within the mHealth framework, systematic research that collects and analyzes patient data to establish comprehensive digital health archives for hypertensive patients, and leverages large language models (LLMs) to assist clinicians in health management and Blood Pressure (BP) control remains limited. In this study, our aims to describe the design, development and usability evaluation process of a management platform (Hyper-DREAM) for hypertension. Our multidisciplinary team employed an iterative design approach over the course of a year to develop the Hyper-DREAM platform. This platform's primary functionalities encompass multimodal data collection (personal hypertensive digital phenotype archive), multimodal interventions (BP measurement, medication assistance, behavior modification, and hypertension education) and multimodal interactions (clinician-patient engagement and BP Coach component). In August 2024, the mHealth App Usability Questionnaire (MAUQ) was conducted involving 51 hypertensive patients recruited from three distinct centers. In parallel, six clinicians engaged in management activities and contributed feedback via the Doctor’s Software Satisfaction Questionnaire (DSSQ). Concurrently, a real-world comparative experiment was conducted to evaluate the usability of the BP Coach, ChatGPT-4o Mini, ChatGPT-4o and clinicians. The comparative experiment demonstrated that the BP Coach achieved significantly higher scores in utility (mean scores 4.05, SD 0.87) and completeness (mean scores 4.12, SD 0.78) when compared to ChatGPT-4o Mini, ChatGPT-4o, and clinicians. In terms of clarity, the BP Coach was slightly lower than clinicians (mean scores 4.03, SD 0.88). In addition, the BP Coach exhibited lower performance in conciseness (mean scores 3.00, SD 0.96). Clinicians reported a marked improvement in work efficiency (2.67 vs. 4.17, P <.001) and experienced faster and more effective patient interactions (3.0 vs. 4.17, P =.004). Furthermore, the Hyper-DREAM platform significantly decreased work intensity (2.5 vs. 3.5, P =.01) and minimized disruptions to daily routines (2.33 vs. 3.55, P =.004). The Hyper-DREAM platform demonstrated significantly greater overall satisfaction compared to the WeChat-based standard management (3.33 vs. 4.17, P =.01). Additionally, clinicians exhibited a markedly higher willingness to integrate the Hyper-DREAM platform into clinical practice (2.67 vs. 4.17, P <.001). Furthermore, patient management time decreased from 11.5 min (SD 1.87) with Wechat-based standard management to 7.5 min (SD 1.84, P =.01) with Hyper-DREAM. Hypertensive patients reported high satisfaction with the Hyper-DREAM platform, including ease of use (mean scores 1.60, SD 0.69), system information arrangement (mean scores 1.69, SD 0.71), and usefulness (mean scores 1.57, SD 0.58). In conclusion, our study presents Hyper-DREAM, a novel artificial intelligence-driven platform for hypertension management, designed to alleviate clinician workload and exhibiting significant promise for clinical application. The Hyper-DREAM platform is distinguished by its user-friendliness, high satisfaction rates, utility, and effective organization of information. Furthermore, the BP Coach component underscores the potential of LLMs in advancing mHealth approaches to hypertension management. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
ER  -
TY  - JOUR
AU  - Degen, H.
AU  - Ntoa, S.
T1  - 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th HCI International Conference, HCII 2024
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Y1  - 2024
VL  - 14734 LNAI
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196175152&partnerID=40&md5=9fd70c107a18fbbb6b0e415af061749f
U1  - 03029743 (ISSN); 978-303160605-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th HCI International Conference, HCII 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 313149
N2  - The proceedings contain 90 papers. The special focus in this conference is on Artificial Intelligence in HCI. The topics include: What Makes People Say Thanks to AI; PyFlowML: A Visual Language Framework to Foster Participation in ML-Based Decision Making; A Three-Year Analysis of Human Preferences in Delegating Tasks to AI; qualitative User-Centered Requirements Analysis for a Recommender System for a Project Portfolio Platform in Higher Education Institutions; foreword; surveying Computational Theory of Mind and a Potential Multi-agent Approach; What Is the Focus of XAI in UI Design? Prioritizing UI Design Principles for Enhancing XAI User Experience; Evaluation of Generative AI-Assisted Software Design and Engineering: A User-Centered Approach; time Series Representation Learning: A Survey on Deep Learning Techniques for Time Series Forecasting; uncertainty of Information Applied to Network Monitoring Metrics; semi-supervised Sorting via Deep Feature Extraction and Density Based Clustering with User Feedback; towards a Framework for Interdisciplinary Studies in Explainable Artificial Intelligence; exploring the Impact of Explainability on Trust and Acceptance of Conversational Agents - A Wizard of Oz Study; WisCompanion: Integrating the Socratic Method with ChatGPT-Based AI for Enhanced Explainability in Emotional Support for Older Adults; how to Explain It to System Testers?; A Multidisciplinary Heuristic Evaluation of AI-Enhanced Web Tools: Insights and Implications for Legal Contract Management Systems; Evaluating the Effectiveness of the Peer Data Labelling System (PDLS); preface; reducing Human Annotation Effort Using Self-supervised Learning for Image Segmentation; Enhancing Historical Understanding in School Students: Designing a VR Application with AI-Animated Characters; Examining User Perceptions to Vocal Interaction with AI Bots in Virtual Reality and Mobile Environments: A Focus on Foreign Language Learning and Communication Dynamics; Human-Aligned GAI Driven by Conceptual Knowledge: System, Framework, and Co-creation; iterative Visual Interaction with Latent Diffusion Models; evidential Representation Proposal for Predicate Classification Output Logits in Scene Graph Generation.
ER  -
TY  - CONF
AU  - Shin, D.
AU  - Wang, L. L.
AU  - Hsieh, G.
T1  - From Paper to Card: Transforming Design Implications with Generative AI
PB  - Association for Computing Machinery
AD  - University of Washington, Allen Institute for AI, Seattle, WA, United States
Y1  - 2024
IS  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194876285&doi=10.1145%2f3613904.3642266&partnerID=40&md5=54b5a1a54cffb52fbfbe159b92c38d43
M3  - https://doi.org/10.1145/3613904.3642266
KW  - design card
KW  - generative AI
KW  - large language model
KW  - text-to-image model
KW  - translational science
KW  - Design
KW  - User interfaces
KW  - Academic paper
KW  - Design card
KW  - Design implications
KW  - Design-process
KW  - Generative AI
KW  - Image modeling
KW  - Language model
KW  - Large language model
KW  - Text-to-image model
KW  - Translational science
KW  - Paper
U1  - 979-840070330-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 11; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199441
N2  - Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N = 21) and authors of selected papers (N = 12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an efective way of communicating their design implications. We also propose future enhancements for AI-generated design cards. © 2024 Copyright held by the owner/author(s)
ER  -
TY  - JOUR
AU  - Hsueh, N.-L.
AU  - Lin, H.-J.
AU  - Lai, L.-C.
T1  - Applying Large Language Model to User Experience Testing
JO  - Electronics (Switzerland)
Y1  - 2024
VL  - 13
IS  - 4633
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212091038&doi=10.3390%2felectronics13234633&partnerID=40&md5=b7bcfbd456080ddfdbb2167843e50db3
M3  - https://doi.org/10.3390/electronics13234633
KW  - AI in software development
KW  - human-computer interaction (HCI)
U1  - 20799292 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: N.-L. Hsueh; Department of Information Engineering and Computer Science, Feng Chia University, Taichung, 407, Taiwan; email: nlhsueh@fcu.edu.tw
N2  - The maturation of internet usage environments has elevated User Experience (UX) to a critical factor in system success. However, traditional manual UX testing methods are hampered by subjectivity and lack of standardization, resulting in time-consuming and costly processes. This study explores the potential of Large Language Models (LLMs) to address these challenges by developing an automated UX testing tool. Our innovative approach integrates the Rapi web recording tool to capture user interaction data with the analytical capabilities of LLMs, utilizing Nielsen’s usability heuristics as evaluation criteria. This methodology aims to significantly reduce the initial costs associated with UX testing while maintaining assessment quality. To validate the tool’s efficacy, we conducted a case study featuring a tennis-themed course reservation system. The system incorporated multiple scenarios per page, allowing users to perform tasks based on predefined goals. We employed our automated UX testing tool to evaluate screenshots and interaction logs from user sessions. Concurrently, we invited participants to test the system and complete UX questionnaires based on their experiences. Comparative analysis revealed that varying prompts in the automated UX testing tool yielded different outcomes, particularly in detecting interface elements. Notably, our tool demonstrated superior capability in identifying issues aligned with Nielsen’s usability principles compared to participant evaluations. This research contributes to the field of UX evaluation by leveraging advanced language models and established usability heuristics. Our findings suggest that LLM-based automated UX testing tools can offer more consistent and comprehensive assessments. © 2024 by the authors.
ER  -
TY  - CONF
AU  - Miah, T.
AU  - Zhu, H.
T1  - User Centric Evaluation of Code Generation Tools (Invited Paper)
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, OX33 1HX, United Kingdom
Y1  - 2024
SP  - 109–119
EP  - 109–119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206466645&doi=10.1109%2fAITest62860.2024.00022&partnerID=40&md5=5815f4788bb17c7693c25d2809ed39e5
M3  - https://doi.org/10.1109/AITest62860.2024.00022
KW  - ChatGPT
KW  - Code generation
KW  - Large language models
KW  - Machine learning
KW  - Performance evaluation
KW  - R programming language
KW  - Usability
KW  - Computer software selection and evaluation
KW  - Code generation tools
KW  - Codegeneration
KW  - Language model
KW  - Large language model
KW  - Machine-learning
KW  - Performances evaluation
KW  - R programming
KW  - Contrastive Learning
U1  - 979-835036505-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Conference name: 6th IEEE International Conference on Artificial Intelligence Testing, AITest 2024; Conference date: 15 July 2024 through 18 July 2024; Conference code: 202906
N2  - With the rapid advance of machine learning (ML) technology, large language models (LLMs) are increasingly explored as an intelligent tool to generate program code from natural language specifications. However, existing evaluations of LLMs have focused on their capabilities in comparison with humans. It is desirable to evaluate their usability when deciding on whether to use a LLM in software production. This paper proposes a user centric method for this purpose. It includes metadata in the test cases of a benchmark to describe their usages, conducts testing in a multi-attempt process that mimics the uses of LLMs, measures LLM generated solutions on a set of quality attributes that reflect usability, and evaluates the performance based on user experiences in the uses of LLMs as a tool. The paper also reports a case study with the method in the evaluation of ChatGPT's usability as a code generation tool for the R programming language. Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks. The user experiences are good with overall average number of attempts being 1.61 and the average time of completion being 47.02 seconds. Our experiments also found that the weakest aspect of usability is conciseness, which has a score of 3.80 out of 5. © 2024 IEEE.
ER  -
TY  - JOUR
AU  - da Silva H., Plácido
AU  - Cipresso, P.
T1  - 8th International Conference on Computer-Human Interaction Research and Applications, CHIRA 2024
JO  - Communications in Computer and Information Science
Y1  - 2025
VL  - 2371 CCIS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006569161&partnerID=40&md5=20fa202ac181886699ad7ed4df91c799
U1  - 18650929 (ISSN); 978-303183844-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 8th International Conference on Computer-Human Interaction Research and Applications, CHIRA 2024; Conference date: 21 November 2024 through 22 November 2024; Conference code: 328309
N2  - The proceedings contain 61 papers. The special focus in this conference is on Computer-Human Interaction Research and Applications. The topics include: How Different Blink Patterns of Pet Robots Evoke Feelings of Affection in People; arm in Motion: How Motion Modality and Erratic Behavior of a Robotic Arm Shape User Perception; An Examination of Pre-school Children’s Usage Behavior of Augmented Reality: Traditional vs. AR-Assisted LEGO® Building; Interviewing ChatGPT-Generated Personas to Inform Design Decisions; an Experiment to Investigate Changes in Physiological Signals During Subtle Wind and Scent Presentation for Designing Subtle Notifications; Towards Multi-stakeholder Evaluation of ML Models: A Crowdsourcing Study on Metric Preferences in Job-Matching System; current Design Practices in Applied Augmented Reality Research: A Methodological Review; emotion-Aware Interfaces: Empirical Methods for Adaptive User Interface; bridging Medical Genetics, Genetic Counselling, and Patients: Proposing an Immersive, Interactive, and Holographic Health Information Platform with Evaluation Methods for Personalized Patient Education; why Do(n’t) You Trust Us? Highlighting the Importance of Trust and Transparency for Designing B2B Platforms in Electronics Manufacturing; enriched with Behaviour Theory Topic Guide Template for Digital Behaviour Change Interventions; evaluating Remote Communication Applications Using Student Usability Reviews; Enhancing EEG-Based User Verification with a Normalized Neural Network Ensemble Approach; systematic Literature Review of Gamification Design in Higher Education Programming Courses: Methodological Rigor Exposed; the Effect of Progressive Disclosure in the Transparency of Large Language Models; design and Implementation of a Practice Record Visualization System Using Piano Performance Tracking Technology; User Issues and Concerns in Generative AI: A Mixed-Methods Analysis of App Reviews; Caregiver Acceptability of an LLM-Powered Assistant Interface to Improve Sleep Quality of the Elderly; user Experience and Information Security Heuristics for Digital Identity Wallets; how Can Heuristics Be Communicated?.
ER  -
TY  - CONF
AU  - Rafly, M. T.
AU  - Arisandi, D.
AU  - Tony, T.
AU  - Pranata, E. B.
AU  - Siwi, S. H.
AU  - Priyomarsono, N. W.
T1  - Multi User Centered Design (MUCD) in Mobile UI/UX Development for Kampung Batik Laweyan
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Tarumanagara University, Faculty of Engineering, Jakarta, Indonesia
Y1  - 2024
SP  - 354–360
EP  - 354–360
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002473927&doi=10.1109%2fICITSI65188.2024.10929148&partnerID=40&md5=307f8ab9ba60425c0eb1c7e7fa912991
M3  - https://doi.org/10.1109/ICITSI65188.2024.10929148
KW  - Kampung Batik Laweyan
KW  - mobile application
KW  - Multi-User Centered Design (MUCD)
KW  - tourism enhancement
KW  - UI/UX design
KW  - Architectural design
KW  - Economic and social effects
KW  - Gamification
KW  - History
KW  - Integrated circuit design
KW  - Leisure
KW  - Marketing
KW  - Product development
KW  - Textile industry
KW  - Usability engineering
KW  - User centered design
KW  - Cultural traditions
KW  - Indonesia
KW  - Kampung batik laweyan
KW  - Mobile applications
KW  - Multi-user centered design
KW  - Multiusers
KW  - Neighbourhood
KW  - Tourism enhancement
KW  - User-centred
KW  - Tourism
U1  - 979-833151147-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: M.T. Rafly; Tarumanagara University, Faculty of Information Technology, Jakarta, Indonesia; email: muhammad.825210051@stu.untar.ac.id; Conference name: 2024 International Conference on Information Technology Systems and Innovation, ICITSI 2024; Conference code: 207844
N2  - Kampung Batik Laweyan, a historic neighborhood in Surakarta, Indonesia, is renowned for its rich cultural heritage and tradition of batik textile production. With a history spanning several centuries, this area has become a vibrant hub for batik artisans and enthusiasts, showcasing intricate designs and craftsmanship that reflect local identity. The development of a mobile UI/UX application aimed at enhancing tourism in Kampung Batik Laweyan employs a Multi-User Centered Design (MUCD) methodology. The project unfolds in two iterations, beginning with a Minimum Viable Product (MVP) followed by the collection of feedback from primary user groups, including local business operators, residents, and tourists. Data collection involves interviews with local authorities, observations of the Micro, Small, and Medium Enterprises (MSME) environment in Laweyan, and an evaluation of current tourism marketing strategies. This comprehensive approach ensures that the application effectively caters to the diverse needs of its users. By integrating location-based gamification and a Large Language Model (LLM), the application not only enhances user engagement but also promotes the cultural and economic potential of the region. The System Usability Scale (SUS) assessment reveals an average score of 72.5, indicating a "good"level of usability, with qualitative feedback affirming its intuitive design and potential for further mobile app development. © 2024 IEEE.
ER  -
TY  - JOUR
AU  - Angyal, V.
AU  - Bertalan, Á.
AU  - Domján, P.
AU  - Dinya, E.
T1  - Exploring the possibilities and limitations of customized large language model to support and improve cervical cancer screening
JO  - BMC Medical Informatics and Decision Making
Y1  - 2025
VL  - 25
IS  - 242
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009609973&doi=10.1186%2fs12911-025-03088-3&partnerID=40&md5=b272eaa0d0235390aa9b74ba81eb21ac
M3  - https://doi.org/10.1186/s12911-025-03088-3
KW  - Artificial intelligence
KW  - Cervical cancer
KW  - Custom GPT
KW  - Natural language processing
KW  - Prevention
KW  - Prompt engineering
U1  - 14726947 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: V. Angyal; Semmelweis University Doctoral College, Health Sciences Division, Institute of Digital Health Sciences, Budapest, Hungary; email: angyal.viola@phd.semmelweis.hu
N2  - Background: The rapid advancement of artificial intelligence, driven by Generative Pre-trained Transformers (GPT), has transformed natural language processing. Prompt engineering plays a key role in guiding model outputs effectively. Our primary objective was to explore the possibilities and limitations of a custom GPT, developed via prompt engineering, as a patient education tool, which delivers publicly available information through a user-friendly design that facilitates more effective access to cervical cancer screening knowledge. Method: The system was developed using the OpenAI GPT-4 model and Python programming language, with the interface built on Streamlit for cloud-based accessibility and testing. It initially presented questions to testers for preliminary assessment. For cervical cancer-related information, we referenced medical guidelines. Iterative testing optimized the prompts for quality and relevance; techniques like context provision, question chaining, and prompt-based constraints were used. Human-in-the-loop and two independent medical doctor evaluations were employed. Additionally, system performance metrics were measured. Result: The web application was tested 115 times over a three-week period in 2024, with 87 female (76%) and 28 male (24%) participants. A total of 112 users completed the user experience questionnaire. Statistical analysis showed a significant association between age and perceived personalization (p = 0.047) and between gender and system customization (p = 0.037). Younger participants reported higher engagement, though not significantly. Females valued guidance on screening schedules and early detection, while males highlighted the usefulness of information regarding HPV vaccination and its role in preventing HPV-related cancers. Independent evaluations by medical doctors demonstrated consistent assessments of the system’s responses in terms of accuracy, clarity, and usefulness. Discussion: While the system demonstrates potential to enhance public health awareness and promote preventive behaviors, encouraging individuals to seek information on cervical cancer screening and HPV vaccination, its conversational capabilities remain constrained by the inherent limitations of current language model technology. Conclusions: Although custom GPTs can not substitute a healthcare consultations, these tools can streamline workflows, expedite information access, and support personalized care. Further research should focus on conducting well-designed randomized controlled trials to establish definitive conclusions regarding its impact and reliability. Clinical trial number: Not applicable. © The Author(s) 2025.
ER  -
TY  - JOUR
AU  - van der Aa H.
AU  - Bork, D.
AU  - Schmidt, R.
AU  - Sturm, A.
T1  - 25th International Working Conference on Business Process Modeling, Development, and Support, BPMDS 2024 and 29th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development, EMMSAD 2024
JO  - Lecture Notes in Business Information Processing
Y1  - 2024
VL  - 511 LNBIP
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197169304&partnerID=40&md5=50db9971a937fd2ce3c294e4039f4f17
U1  - 18651348 (ISSN); 978-303161006-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 25th International Working Conference on Business Process Modeling, Development, and Support, BPMDS 2024 and 29th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development, EMMSAD 2024; Conference date: 3 June 2024 through 4 June 2024; Conference code: 313719
N2  - The proceedings contain 28 papers. The special focus in this conference is on Business Process Modeling, Development, and Support and Exploring Modeling Methods for Systems Analysis and Development. The topics include: Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies; mapping the Landscape: Exploring Large Language Model Applications in Business Process Management; designing a User Interface to Explore Collections of Directly-Follows Graphs for Process Mining Analysis; precision-Guided Minimization of Arbitrary Declarative Process Models; leveraging Data Augmentation for Process Information Extraction; a Generic Approach Towards Adapting User Preferences in Business Process Execution; introducing Agile Controllability in Temporal Business Processes; reviewing Conformance Checking Uses for Run-Time Regulatory Compliance; visual Representation of Resource Analysis Insights for Process Mining; process Variant Analysis Across Continuous Features: A Novel Framework; a Novel Contextualization Method for Process Discovery Using Activity Specialization Hierarchies; enhancing Our Understanding of Business Process Model Comprehension Using Biometric Data; a Method for Digital Business Ecosystem Design: Evaluation of Two Cases in the Maritime Dataspaces; technology for Automatic Usability Evaluation Using Model Driven Engineering; Building BESSER: An Open-Source Low-Code Platform; Towards Taming Large Language Models with Prompt Templates for Legal GRL Modeling; process Modeling with Large Language Models; could a Large Language Model Contribute Significantly to Requirements Analysis?; fast & Sound: Accelerating Synthesis-Rules-Based Process Discovery; Navigating the Data Model Divide in Smart Manufacturing: An Empirical Investigation for Enhanced AI Integration; a Multi-dimensional Model for the Design and Development of Analytical Information Systems; situational Environmental, Social and Governance Accounting: From Ethical Value Elicitation to Sustainability Reporting; realizing the Accountability of Algorithms in the Public Sector: A Reference Method for Managing Algorithm Registers; requirements for a Digital Twin for Energy, Social, and Governance Data of Commercial Buildings.
ER  -
TY  - CONF
AU  - Lee, J.
AU  - Choi, Y.
AU  - Song, M.
AU  - Park, S.
T1  - ChatFive: Enhancing User Experience in Likert Scale Personality Test through Interactive Conversation with LLM Agents
PB  - Association for Computing Machinery, Inc
AD  - School of Computing, KAIST Data Science Group, IBS, Daejeon, South Korea
Y1  - 2024
IS  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199542158&doi=10.1145%2f3640794.3665572&partnerID=40&md5=a281bcbc1b0f8991e4e6477e54a87cdf
M3  - https://doi.org/10.1145/3640794.3665572
KW  - Conversational Agents
KW  - Conversational User Interface(CUI)
KW  - Large Language Models(LLM)
KW  - Personality Test
KW  - Behavioral research
KW  - Computational linguistics
KW  - Conveying
KW  - Conversational agents
KW  - Conversational user interface
KW  - Language model
KW  - Large language model
KW  - Likert scale
KW  - Model agents
KW  - Personality assessments
KW  - Personality tests
KW  - User engagement
KW  - Users' experiences
KW  - User interfaces
U1  - 979-840070511-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: J. Lee; School of Computing, KAIST, Daejeon, South Korea; email: dlwjdwo00701@kaist.ac.kr; Y. Choi; School of Computing, KAIST Data Science Group, IBS, Daejeon, South Korea; email: yubin.choi@kaist.ac.kr; Conference name: 6th Conference on ACM Conversational User Interfaces, CUI 2024; Conference date: 8 July 2024 through 10 July 2024; Conference code: 200901
N2  - Personality assessments provide insights into understanding individual differences. In HCI, personality assessments are used to model user behavior or tailor user interfaces. However, conventional Likert-scale personality tests face issues in user engagement and capturing comprehensive personality nuances. Building upon prior work using conversational user interfaces for personality prediction, we delve deeper into personalized personality tests. Through a formative study (n=4), we identified three design goals for user engagement. Informed by these goals, we propose a novel architecture integrating multiple large language model agents to support free-form conversation-based personality assessment. Our system, ChatFive, predicts users' Big Five traits through real-Time personalized dialogue. Evaluations from our user study (n=20) revealed that ChatFive significantly improved conveying true responses and felt more engaged, though requiring longer response times and different validation. We discuss the limitations on the validity of ChatFive and its implications. © 2024 Owner/Author.
ER  -
TY  - CONF
AU  - Iong, I. L.
AU  - Liu, X.
AU  - Chen, Y.
AU  - Lai, H.
AU  - Yao, S.
AU  - Shen, P.
AU  - Yu, H.
AU  - Dong, Y.
AU  - Tang, J.
AU  - Cao, Y.
AU  - Feng, Y.
AU  - Xiong, D.
T1  - OpenWebAgent: An Open Toolkit to Enable Web Agents on Large Language Models
PB  - Association for Computational Linguistics (ACL)
AD  - University of the Chinese Academy of Sciences, China
Y1  - 2024
VL  - 3
SP  - 72–81
EP  - 72–81
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203808884&doi=10.18653%2fv1%2f2024.acl-demos.8&partnerID=40&md5=a2ccbc58538d94aef7a813ad416a8d7e
M3  - https://doi.org/10.18653/v1/2024.acl-demos.8
KW  - Chatbots
KW  - Computer simulation languages
KW  - HTML
KW  - Intelligent agents
KW  - User interfaces
KW  - Web browsers
KW  - Web Design
KW  - Agent Framework
KW  - Complex task
KW  - Computer interaction
KW  - HTML parsers
KW  - Intuitive user interface
KW  - Language model
KW  - Modular designs
KW  - Multimodal models
KW  - Web Agents
KW  - Web automation
KW  - Computational linguistics
U1  - 0736587X (ISSN); 979-889176096-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: ; ; Conference name: 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024; Conference date: 11 August 2024 through 16 August 2024; Conference code: 202222
N2  - We introduce OpenWebAgent, an open toolkit designed to optimize web automation by integrating both large language models (LLMs) and large multimodal models (LMMs). This toolkit focuses on enhancing human-computer interactions on the web, simplifying complex tasks through an advanced HTML parser, a rapid action generation module, and an intuitive user interface. At the core of OpenWebAgent is an innovative web agent framework that uses a modular design to allow developers to seamlessly integrate a variety of models and tools to process web information and automate tasks on the web. This enables the development of powerful, task-oriented web agents, significantly enhancing user experience and operational efficiency on the web. The OpenWebAgent framework, Chrome plugin, and demo video are available at https://github.com/THUDM/OpenWebAgent/. © 2024 Association for Computational Linguistics.
ER  -
TY  - CONF
AU  - Balazs Neszlenyi, K.
AU  - Milos, A.
AU  - Kiss, A.
T1  - AssistantGPT: Enhancing User Interaction with LLM Integration
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Eötvös Loránd University, Department of Information Systems, Budapest, Hungary
Y1  - 2024
SP  - 619–624
EP  - 619–624
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210890681&doi=10.1109%2fSISY62279.2024.10737548&partnerID=40&md5=29cce2eb86deed982a48b10b1fbe2f9c
M3  - https://doi.org/10.1109/SISY62279.2024.10737548
KW  - Accessibility
KW  - Automation
KW  - Command and Control Systems
KW  - Human-Computer Interaction
KW  - Information Retrieval
KW  - Large Language Models
KW  - Natural Language Processing
KW  - User Experience
KW  - User Interfaces
KW  - Voice Recognition
KW  - Application programming interfaces (API)
KW  - Human computer interaction
KW  - Job analysis
KW  - Man machine systems
KW  - Modeling languages
KW  - Natural language processing systems
KW  - Problem oriented languages
KW  - Systems analysis
KW  - Command-and-control systems
KW  - Computer interaction
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Natural language processing
KW  - Natural languages
KW  - System functionality
KW  - Users' experiences
KW  - Command and control systems
U1  - 979-835038560-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 22nd IEEE International Symposium on Intelligent Systems and Informatics, SISY 2024; Conference date: 19 September 2024 through 21 September 2024; Conference code: 203937
N2  - As user interface technologies evolve, integrating voice and text commands becomes crucial for enhancing interaction with operating systems. AssistantGPT leverages OpenAI's GPT API to enable natural language-driven system functionalities, improving productivity through precise task interpretation and execution. The application supports a diverse range of operations, including web searches, API interactions via OpenAPI schemas, voice conversations, and command execution through the shell, demonstrating adaptability and scalability across various use cases and environments. Implemented using Electron.js, React, Python, and FastAPI, AssistantGPT demonstrates high efficiency, reliability, and user-friendliness. By bridging complex system functionalities and intuitive user interfaces, AssistantGPT significantly advances human-computer interaction, offering practical solutions for both professional and personal applications. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Caslini, G.
AU  - Gianotti, M.
AU  - Garzotto, F.
AU  - Santoro, C.
AU  - Schmidt, A.
AU  - Matera, M.
AU  - Bellucci, A.
T1  - From User Needs to Smart Ecosystems Through LLMs: The Smartifier Case Study
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Università degli Studi di Milano-Bicocca, MI, Milan, 20126, Italy
Y1  - 2025
VL  - 15713 LNCS
SP  - 80–98
EP  - 80–98
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009001857&doi=10.1007%2f978-3-031-95452-8_5&partnerID=40&md5=07c561ce2f811e5ff4f888a71747bbc4
M3  - https://doi.org/10.1007/978-3-031-95452-8_5
KW  - Conversational Agents
KW  - IoT
KW  - LLMs
KW  - Smart Objects
KW  - Smart Spaces
KW  - Automation
KW  - Human computer interaction
KW  - Iterative methods
KW  - User centered design
KW  - User experience
KW  - User interfaces
KW  - Case-studies
KW  - Conversational agents
KW  - Development approach
KW  - End-User Development
KW  - Language model
KW  - Large language model
KW  - Non-technical users
KW  - Smart objects
KW  - Smart space
KW  - User need
KW  - Internet of things
U1  - 03029743 (ISSN); 978-303195451-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: G. Caslini; Politecnico di Milano, Milan, MI, 20133, Italy; email: giacomo.caslini@polimi.it; Conference name: 10th International Symposium on End-User Development, IS-EUD 2025; Conference date: 16 June 2025 through 18 June 2025; Conference code: 333599
N2  - Smartifier is a conversational platform that leverages Large Language Models (LLMs) to help non-technical users design, implement, and manage smart objects. It addresses persistent challenges in the Internet of Things (IoT) domain, where the “making community” and End-User Development approaches have yet to empower novices fully. The system features two distinct chat interfaces: one dedicated to creating customized devices, and another for generating automation rules. Building on an iterative design model, Smartifier translates high-level, daily-life needs into hardware selections and automatically compiled software, ensuring correct APIs for seamless integration. A technical evaluation compared different ChatGPT-based models and temperature values, revealing an optimal configuration that curbs hallucinations and maximizes reliability. In a user study with 17 participants lacking ICT expertise, Smartifier achieved high correctness (averaging over 91% across all tasks) and high scores regarding usability and user satisfaction. A thematic analysis of conversations revealed insights around user need expression, iterative negotiation, and system feedback. These findings will inform future refinements, especially regarding user experience design and the handling of advanced functionalities in complex real-world settings. While the study’s sample size is limited, findings confirm that Smartifier effectively bridges complex technical tasks and everyday needs, reducing cognitive load and streamlining the creation of IoT solutions. These results highlight the potential of LLM-driven platforms to simplify and democratize technology development, suggesting new directions for future research on EUD and software generation for non-expert users. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - CONF
AU  - Waseem, M.
AU  - Das, T.
AU  - Paloniemi, T.
AU  - Koivisto, M.
AU  - Räsänen, E.
AU  - Setälä, M.
AU  - Mikkonen, T.
AU  - Hyrynsalmi, S.
AU  - Münch, J.
AU  - Smolander, K.
AU  - Melegati, J.
T1  - Artificial Intelligence Procurement Assistant: Enhancing Bid Evaluation
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Solita, Tampere, Finland
Y1  - 2024
VL  - 500 LNBIP
SP  - 108–114
EP  - 108–114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188726315&doi=10.1007%2f978-3-031-53227-6_8&partnerID=40&md5=b9a2790dc8b77b280621d1b8baea9a49
M3  - https://doi.org/10.1007/978-3-031-53227-6_8
KW  - Artificial intelligence
KW  - Competition
KW  - Advanced systems
KW  - Bid evaluation
KW  - Data analytics
KW  - Finnish
KW  - Funding opportunities
KW  - Language model
KW  - Model agents
KW  - Software company
KW  - State-of-the-art technology
KW  - User interaction
KW  - Data Analytics
U1  - 18651348 (ISSN); 978-303153226-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: M. Waseem; Faculty of Information Technology, Jyväskylä University, Jyväskylä, Finland; email: muhammad.m.waseem@jyu.fi; Conference name: 14th International Conference on Software Business, ICSOB 2023; Conference date: 27 November 2023 through 29 November 2023; Conference code: 309399
N2  - In modern business, maintaining competitiveness and efficiency necessitates the integration of state-of-the-art technology. This paper introduces the Artificial Intelligence Procurement Assistant (AIPA), an advanced system co-developed with Solita, a Finnish software company. AIPA leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids and funding opportunities. The system incorporates LLM agents to enhance user interactions, from intelligent search execution to results evaluation. Rigorous usability testing and real-world evaluation, conducted in collaboration with our industry partner, validated AIPA’s intuitive interface, personalized search functionalities, and effective results filtering. The platform significantly streamlines the identification of optimal calls by synergizing LLMs with resources from the European Commission TED and other portals. Feedback from the company guided essential refinements, particularly in the performance of ChatGPT agents for tasks like translation and keyword extraction. Further contributing to its scalability and adaptability, AIPA has been made open-source, inviting community contributions for its ongoing refinement and enhancement. Future developments will focus on extensive case studies, iterative improvements through user feedback, and expanding data sources to further elevate its utility in streamlining and optimizing procurement processes. © The Author(s) 2024.
ER  -
TY  - JOUR
AU  - Liu, Y.
AU  - Martens, J.-B.
T1  - Conversation-based hybrid UI for the repertory grid technique: A lab experiment into automation of qualitative surveys
JO  - International Journal of Human Computer Studies
Y1  - 2024
VL  - 184
IS  - 103227
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183466132&doi=10.1016%2fj.ijhcs.2024.103227&partnerID=40&md5=9c4644f20ae1abe675a64043de47f49c
M3  - https://doi.org/10.1016/j.ijhcs.2024.103227
KW  - Chatbot
KW  - Hybrid UI
KW  - Qualitative survey automation
KW  - Repertory grid technique
KW  - Graphical user interfaces
KW  - Chatbots
KW  - Grid techniques
KW  - Hybrid User Interfaces
KW  - Lab. experiment
KW  - Qualitative surveys
KW  - Repertory grids
KW  - Users' experiences
KW  - Automation
U1  - 10715819 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 4; Correspondence Address: Y. Liu; Eindhoven University of Technology, Eindhoven, De Zaale, Noord Brabant, 5600 MB, Netherlands; email: y.liu10@tue.nl; CODEN: IHSTE
N2  - A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language. © 2024 The Authors
ER  -
TY  - CONF
AU  - Göldi, A.
AU  - Wambsganss, T.
AU  - Neshaei, S. P.
AU  - Rietsche, R.
T1  - Intelligent Support Engages Writers Through Relevant Cognitive Processes
PB  - Association for Computing Machinery
AD  - University of St. Gallen, Institut Digital Technology Management, Bern University of Applied Sciences, Bern, Switzerland
Y1  - 2024
IS  - 1047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194891492&doi=10.1145%2f3613904.3642549&partnerID=40&md5=2b3065fb23c70a9be6ef6b4ded1c3fa1
M3  - https://doi.org/10.1145/3613904.3642549
KW  - Artifact or System
KW  - Creativity Support
KW  - Education/Learning
KW  - Schools/Educational Setting
KW  - Cognitive systems
KW  - Education computing
KW  - User centered design
KW  - User interfaces
KW  - Artifact or system
KW  - Cognitive process
KW  - Creativity support
KW  - Education/learning
KW  - Educational settings
KW  - Intelligent support
KW  - Language model
KW  - Peer review
KW  - School/educational setting
KW  - Support tool
KW  - Students
U1  - 979-840070330-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 8; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199441
N2  - Student peer review writing is prevalent and important in education for fostering critical thinking and learning motivation. However, it often entails challenges such as high effort and writer's block. Leaving students unsupported may thus diminish the efficacy of the process. Large Language Models (LLMs) offer a potential remedy, but their utility hinges on user-centered design. Guided by design-determining constructs from the Cognitive Process Theory of Writing, we developed an intelligent writing support tool to alleviate these challenges, aiding 1) ideation and 2) evaluation. A randomized experiment (n=120) confirmed users were less inclined to utilize the tool's intelligent features when offered pre-supplied ideas or evaluations, validating our approach. Moreover, students engaged not less but more with their writing if support was available, indicating an enhanced experience. Our research illuminates design choices for enhancing LLM-based tools' usability and user experience, specifically optimizing intelligent writing support tools to facilitate student peer review. © 2024 Copyright held by the owner/author(s)
ER  -
TY  - CONF
AU  - Mathis, L.-A.
AU  - Günes, C.
AU  - Entz, K.
AU  - Lerch, D.
AU  - Diederichs, F.
AU  - Widlroither, H.
T1  - Generating Proactive Suggestions based on the Context: User Evaluation of Large Language Model Outputs for In-Vehicle Voice Assistants
PB  - Association for Computing Machinery, Inc
AD  - Fraunhofer Institute for Industrial, Engineering IAO, Germany
Y1  - 2024
IS  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199534280&doi=10.1145%2f3640794.3665568&partnerID=40&md5=8da1c6ff2a0f1912aa009be329225049
M3  - https://doi.org/10.1145/3640794.3665568
KW  - Quality control
KW  - User interfaces
KW  - Context situations
KW  - Context-based
KW  - Language model
KW  - Model outputs
KW  - Model-based OPC
KW  - Online studies
KW  - Response behaviour
KW  - User evaluations
KW  - User surveys
KW  - Users' experiences
KW  - Computational linguistics
U1  - 979-840070511-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Conference name: 6th Conference on ACM Conversational User Interfaces, CUI 2024; Conference date: 8 July 2024 through 10 July 2024; Conference code: 200901
N2  - Large Language Models (LLMs) have recently been explored for a variety of tasks, most prominently for dialogue-based interactions with users. The future in-car voice assistant (VA) is envisioned as a proactive companion making suggestions to the user during the ride. We investigate the use of selected LLMs to generate proactive suggestions for a VA given different context situations by using a basic prompt design. An online study with users was conducted to evaluate the generated suggestions. We demonstrate the feasibility of generating context-based proactive suggestions with different off-the-shelf LLMs. Results of the user survey show that suggestions generated by the LLMs GPT4.0 and Bison received an overall positive evaluation regarding the user experience for response quality and response behavior over different context situations. This work can serve as a starting point to implement proactive interaction for VA with LLMs based on the recognized context situation in the car. © 2024 Owner/Author.
ER  -
TY  - JOUR
AU  - Sottilare, R. A.
AU  - Schwarz, J.
T1  - 7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15813 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007761709&partnerID=40&md5=f22bfbcd529a7aecc5f763b22dd580fe
U1  - 03029743 (ISSN); 978-303192969-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332889
N2  - The proceedings contain 40 papers. The special focus in this conference is on Adaptive Instructional Systems. The topics include: Evaluating Adaptive Training for Nautical Rules of the Road; from Standardization to Personalization: Leveraging Learner Profiles to Tailor Education; an Adaptive Simulated Startup Financial Modeling Mentor Using a Large Language Model to Address Shortages in Skilled Advisors: Architecture and Design Considerations; integrating Adaptive Interventions into Learning Engineering Workflows; leveraging Deterministic Algorithms to Personalize Education and Enhance Student Success: The Story of an Engineered Learning Experience; curriculum Sequencing as a Generalised Travelling Salesperson Problem: A Novel Perspective on Learning Path Generation; evaluation of Difficulty-Based Adaptive Training Strategies on Simulator Flight Training Performance; designing an Adaptive Mobile Application for Learning Programming Among Computing Students from Marginalized Backgrounds; architecture for a Large Scale Learning Ecosystem; automated Response Generation Using Language Models: An Approach to Enhancing User Interaction; Metacognition in HCI: Designing Systems for Planning and Flexibility; stress and Performance: Understanding the Effect of Appraisal and Coping on Performance in Complex Tasks; exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty; macrocognition in Simulation Based Training: A Practical Application of Learning Engineering in a Complex Training Environment; public Safety Personnel Readiness Prediction: A Hybrid Model of Neurophysiological and Psychometric Data; understanding the Complexity of Music Improvisation: Leveraging Cognitive Models to Inform Adaptive Instruction Design; advancing Cognitive State Monitoring: Diagnosing Cognitive Control States Under Varying Automation Reliability Level; the Weaponization of Critical Thinking: Understanding the Dynamics of Human Memory.
ER  -
TY  - CONF
AU  - Troussas, C.
AU  - Krouska, A.
AU  - Papakostas, C.
AU  - Mylonas, P.
AU  - Sgouropoulou, C.
T1  - Assessing the Impact of Integrating ChatGPT as an Advice Generator in Educational Software
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - University of West Attica, Department of Informatics and Computer Engineering, Egaleo, Greece
Y1  - 2024
SP  - 127–133
EP  - 127–133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210801425&doi=10.1109%2fSEEDA-CECNSM63478.2024.00031&partnerID=40&md5=af6cc81e67bf34ea2bc7488eaaa0083d
M3  - https://doi.org/10.1109/SEEDA-CECNSM63478.2024.00031
KW  - Advice generator
KW  - AI-Powered Tutoring Systems
KW  - Artificial Intelligence in Education
KW  - ChatGPT in Education
KW  - Custom Questionnaire Assessment
KW  - Educational Software
KW  - Interrupted Time Series Analysis
KW  - Java Programming Learning
KW  - Programming Education
KW  - Technology-Enhanced Learning
KW  - User Experience Evaluation
KW  - Adversarial machine learning
KW  - Computer software selection and evaluation
KW  - Java programming language
KW  - Students
KW  - AI-powered tutoring system
KW  - Artificial intelligence in education
KW  - ChatGPT in education
KW  - Custom questionnaire assessment
KW  - Educational software
KW  - Interrupted time
KW  - Interrupted time series analyze
KW  - Java programming
KW  - Java programming learning
KW  - Programming education
KW  - Programming learning
KW  - Technology enhanced learning
KW  - Time-series analysis
KW  - Tutoring system
KW  - User experience evaluations
KW  - Contrastive Learning
U1  - 979-835034248-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: C. Troussas; University of West Attica, Department of Informatics and Computer Engineering, Egaleo, Greece; email: ctrouss@uniwa.gr; Conference name: 9th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference, SEEDA-CECNSM 2024; Conference date: 20 September 2024 through 22 September 2024; Conference code: 203802
N2  - This paper reports on the study of the integration of ChatGPT as an advice generator in custom educational software developed for Java programming. The software, in cooperation with ChatGPT API, pursues providing real-time, context-specific advice to students for better learning. This work adopted a two-fold evaluation approach to evaluating this integration. First, this study examines the effectiveness of this integration with the help of the Interrupted Time Series Analysis methodology to measure possible improvement in the performance metrics of the students in terms of error rates and task completion times. Second, this work presents a custom-designed questionnaire used to get student perceptions regarding the clarity, usefulness, and impact of ChatGPT's advice, and the level of student satisfaction with the user interface. The key takeaways from this research study are the substantial improvements in performance metrics that were noted quantitatively, with students achieving lower error rates and faster completion times after the intervention of ChatGPT. Qualitatively, learners express their satisfaction with the clarity of advice, which gives them an understanding that works on their learning and confidence in Java programming. These findings point toward the promise of integrating such advanced AI solutions in educational software toward a significant improvement in learning outcomes and the necessity of human-aided continuous user feedback for system refinement. © 2024 IEEE.
ER  -
TY  - JOUR
AU  - Yu, H.
AU  - Guo, Y.
AU  - Yang, H.
AU  - Zhang, W.
AU  - Dong, Y.
T1  - Can ChatGPT Revolutionize Language Learning? Unveiling the Power of AI in Multilingual Education Through User Insights and Pedagogical Impact
JO  - European Journal of Education
Y1  - 2025
VL  - 60
IS  - e12749
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207281163&doi=10.1111%2fejed.12749&partnerID=40&md5=0fc667167b4a2a367030da1c498dfb58
M3  - https://doi.org/10.1111/ejed.12749
KW  - ChatGPT
KW  - educational interaction
KW  - human machine collaboration
KW  - multilingual teaching assistance
KW  - user experience
U1  - 01418211 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 12; Correspondence Address: H. Yu; Faculty of Education, Beijing Normal University, Beijing, Beijing, China; email: hao.yu@mail.bnu.edu.cn; Y. Dong; Faculty of Education, Beijing Normal University, Beijing, Beijing, China; email: yan.dong@bnu.edu.cn
N2  - In the era of accelerating globalization, the necessity for multilingual education is increasingly prominent. This study investigates the effectiveness of the AI-based chatbot ChatGPT in multilingual teaching applications. This study employs a quasi-experimental research methodology to examine the experiences of 100 international students at a university in Western China. The investigation delves into multiple dimensions of ChatGPT's effectiveness, including user interface design, operational experience, educational interaction, student engagement, personalized learning recommendations, and the enhancement of cultural understanding. The findings indicate that ChatGPT demonstrates significant potential in multilingual teaching, particularly in providing personalized learning support and facilitating cultural comprehension. However, improvements are needed in aspects of user interface friendliness, interaction naturalness, and depth. This research not only provides empirical support for the use of ChatGPT in multilingual education but also introduces a fresh perspective on integrating AI technology into educational practices, thereby advancing its role in multilingual teaching. © 2024 John Wiley & Sons Ltd.
ER  -
TY  - CONF
AU  - McCabe, A. T.
AU  - Björkman, M.
AU  - Engström, J.
AU  - Kuang, P.
AU  - Söderberg, E.
AU  - Church, L.
AU  - Soderberg, E.
AU  - Church, L.
T1  - Ironies of Programming Automation: Exploring the Experience of Code Synthesis via Large Language Models
PB  - Association for Computing Machinery, Inc
AD  - Lund University, Sweden
Y1  - 2024
SP  - 12–21
EP  - 12–21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199036149&doi=10.1145%2f3660829.3660835&partnerID=40&md5=314584296d29ea9f2e6cadca6bd65450
M3  - https://doi.org/10.1145/3660829.3660835
KW  - code comprehension
KW  - human-computer interaction
KW  - large language models
KW  - prompt engineering
KW  - prompt programming
KW  - Computational linguistics
KW  - Computer programming
KW  - User interfaces
KW  - Code comprehension
KW  - Code synthesis
KW  - Language model
KW  - Large language model
KW  - Novice programmer
KW  - Pilot studies
KW  - Prompt engineering
KW  - Prompt programming
KW  - Users' experiences
KW  - Human computer interaction
U1  - 979-840070634-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Conference name: 8th International Conference on on the Art, Science, and Engineering of Programming, Programming Companion 2024; Conference date: 11 March 2024 through 15 March 2024; Conference code: 201013
N2  - The widespread availability of large language models (LLMs) has presented the opportunity for novice programmers to make use of them for the purpose of understanding and synthesising code. In this paper, we discuss a small pilot study intended to explore the user experience of doing so in a limited way, and the attitudes of a group of novice programmers towards this style of programming. We also draw parallels to the seminal work of Lisanne Bainbridge, and discuss the way in which her "ironies of automation" are also present when attempting to automate the activity of programming. © 2024 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Mirmotahari, O.
AU  - Mørch, A.
AU  - Berg, Y.
AU  - Barricelli, B. R.
AU  - Fischer, G.
AU  - Fogli, D.
AU  - Morch, A.
AU  - Piccinno, A.
AU  - Valtolina, S.
T1  - Evolution of Technological Innovations, User Experiences, and Literacies
PB  - CEUR-WS
AD  - Department of Education, University of Oslo, Oslo, Norway
Y1  - 2024
VL  - 3685
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194164342&partnerID=40&md5=0e91f4eee33b56d8a2f9c588b6e147ce
KW  - End-user
KW  - evolution of technology
KW  - technological gap
KW  - user experience
KW  - user literacy
KW  - Engineering education
KW  - User interfaces
KW  - End-users
KW  - Evolution of technology
KW  - Hardware and software
KW  - Passive users
KW  - Skill sets
KW  - Technological development
KW  - Technological gap
KW  - Technological innovation
KW  - User literacy
KW  - Users' experiences
KW  - Personal computers
U1  - 16130073 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: O. Mirmotahari; Department of Informatics, University of Oslo, Oslo, Norway; email: omidmi@uio.no; Conference name: 8th International Workshop on Cultures of Participation in the Digital Age: Differentiating and Deepening the Concept of "End User" in the Digital Age, CoPDA 2024; Conference code: 199625
N2  - The advancement of computer-based innovations over the years has drastically altered the role of the end-user, from passive users of technology to active participants influencing technological development. Each major innovation, associated with preceding hardware and software milestones, has led to new understandings and skill sets required of end-users. This workshop position paper explores the technological progress from mainframe computers to personal computers, the internet, and the nascent age of artificial intelligence (AI) based on Large Language Models (LLMs). We examine how each technological leap has transformed the end-user experience, the evolving literacies demanded, and speculate how the evolution of user interaction with technology in the future might move in different directions and widen the technological gap. © 2024 Copyright for this paper by its authors.
ER  -
TY  - JOUR
AU  - Rangaswamy, N.
AU  - Sim, G. R.
AU  - Borah, P. P.
T1  - 15th Indian Conference on Human-Computer Interaction Design and Research, IndiaHCI 2024
JO  - Communications in Computer and Information Science
Y1  - 2025
VL  - 2338 CCIS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219192908&partnerID=40&md5=36010a409a1f2ba49ac7865856550ed3
U1  - 18650929 (ISSN); 978-303180831-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 15th Indian Conference on Human-Computer Interaction Design and Research, IndiaHCI 2024; Conference date: 7 November 2024 through 9 November 2024; Conference code: 327009
N2  - The proceedings contain 41 papers. The special focus in this conference is on Human-Computer Interaction Design and Research. The topics include: Fogg Behavioural Model Based Cybersecurity Awareness Framework: An Empirical Analysis; could You Hear That? Identifying Marathi Phrases Suitable for Aural Transcription Tasks; IDCText: An Application for Conducting Text Input Research Studies in Indian Languages; comparative Evaluation of Speech Interfaces of Conversational Agents in Hindi; exploring the Impact of Foot-Based Haptic Feedback on User Experience in Virtual Reality Navigation; investigating Contextual Factors in Technology-Based Solutions Designed to Support Health and Fitness Routines for Older Adults: A Systematic Review; Is ChatGPT Ready for Indian-Language Speakers? Findings From a Preliminary Mixed Methods Study; using Graph Analysis for Evaluating Usability of Software-Based Keyboard for Password Creation; Spatial Audio Training for Visually Impaired Users Navigation in VR: An Analytical Approach; culturally Relevant Novel Interaction Methods for Immersive Video Streaming Experience in Virtual Reality; allerGuard: An Innovative mHealth Solution for Food Allergy Management in India; visual Feedback Interface for Audio Communication Over Lossy and High Delay Networks.
ER  -
TY  - CONF
AU  - Weitl-Harms, S.
AU  - Hastings, J. D.
AU  - Lum, J.
AU  - Wani, M. A.
AU  - Angelov, P.
AU  - Luo, F.
AU  - Ogihara, M.
AU  - Wu, X.
AU  - R.-E., Precup
AU  - Ramezani, R.
AU  - Gu, X.
T1  - Using LLMs to Establish Implicit User Sentiment of Software Desirability
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - The Beacom College of Computer & Cyber Sciences, Dakota State University, Madison, SD, United States
Y1  - 2024
SP  - 1645–1650
EP  - 1645–1650
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000909813&doi=10.1109%2fICMLA61862.2024.00254&partnerID=40&md5=9e44562ab9ce418468054946cac2634a
M3  - https://doi.org/10.1109/ICMLA61862.2024.00254
KW  - GPT
KW  - LLM
KW  - Machine Learning
KW  - Product Desirability Toolkit
KW  - Sentiment Analysis
KW  - Software Desirability
KW  - Adversarial machine learning
KW  - Computer software reusability
KW  - Computer software selection and evaluation
KW  - Contrastive Learning
KW  - Convergence of numerical methods
KW  - Federated learning
KW  - Program processors
KW  - Software testing
KW  - Transfer learning
KW  - Analysis tools
KW  - Critical challenges
KW  - Machine-learning
KW  - Product desirability toolkit
KW  - Product evaluation
KW  - Sentiment analysis
KW  - Software desirability
KW  - Users' experiences
KW  - Zero-shot learning
U1  - 979-835037488-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 2; Conference name: 23rd IEEE International Conference on Machine Learning and Applications, ICMLA 2024; Conference date: 18 December 2024 through 20 December 2024; Conference code: 207395
N2  - This study explores the use of LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability, addressing a critical challenge in product evaluation where traditional review scores, though convenient, fail to capture the richness of qualitative user feedback. Innovations include establishing a method that 1) works with qualitative user experience data without the need for explicit review scores, 2) focuses on implicit user satisfaction, and 3) provides scaled numerical sentiment analysis, offering a more nuanced understanding of user sentiment, instead of simply classifying sentiment as positive, neutral, or negative. Data is collected using the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of two software systems. PDT data was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader, a leading sentiment analysis tool. Each system was asked to evaluate the data in two ways, by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Numerical analysis is used to provide insights into the magnitude of sentiment to drive high quality decisions regarding product desirability. Each LLM is asked to provide its confidence (low, medium, high) in its sentiment score, along with an explanation of its score. All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding user sentiment. This study adds deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Lu, Y.
AU  - Knearem, T.
AU  - Dutta, S.
AU  - Blass, J.
AU  - Kliman-Silver, C.
AU  - Bentley, F.
T1  - AI Is Not Enough: A Hybrid Technical Approach to AI Adoption in UI Linting with Heuristics
PB  - Association for Computing Machinery
AD  - Google, Asheville, NC, United States
Y1  - 2024
IS  - 501
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194170869&doi=10.1145%2f3613905.3637135&partnerID=40&md5=6b0a92f1630b3a4d5aaab45818a1651a
M3  - https://doi.org/10.1145/3613905.3637135
KW  - artificial intelligence
KW  - design systems
KW  - large language models
KW  - UI linting
KW  - user interface (UI) design
KW  - Computational linguistics
KW  - Software design
KW  - User centered design
KW  - Case-studies
KW  - Collaborative prototyping
KW  - Design systems
KW  - Deterministics
KW  - Digital interfaces
KW  - Industry standards
KW  - Language model
KW  - Large language model
KW  - User interface designs
KW  - User interface linting
KW  - User interfaces
U1  - 979-840070331-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 3; Correspondence Address: Y. Lu; University of Notre Dame, Notre Dame, United States; email: ylu23@nd.edu; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI EA 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199442
N2  - Design systems have become an industry standard for creating consistent, usable, and effective digital interfaces. However, detecting and correcting violations of design system guidelines, known as UI linting, is a major challenge. Manual UI linting is time-consuming and tedious, making it a prime candidate for automation. This paper presents a case study of adopting AI for UI linting. Through collaborative prototyping with UX designers, we analyzed the limitations of existing AI models and identified designers' core needs and priorities in UI linting. With such knowledge, we designed a hybrid technical pipeline that combines the deterministic nature of heuristics with the flexibility of large language models. Our case study demonstrates that AI alone is not sufficient for practical adoption and highlights the importance of a deep understanding of AI capabilities and user-centered design approaches. © 2024 Owner/Author.
ER  -
TY  - JOUR
AU  - Shrestha, S.
AU  - Mahmoud, A.
T1  - Mobile application review summarization using chain of density prompting
JO  - Automated Software Engineering
Y1  - 2025
VL  - 32
IS  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008805104&doi=10.1007%2fs10515-025-00533-5&partnerID=40&md5=d6389e8195d50343fbfe79ef7379b07a
M3  - https://doi.org/10.1007/s10515-025-00533-5
KW  - LLMs
KW  - Mobile app reviews
KW  - Summarization
KW  - Large datasets
KW  - Mobile applications
KW  - Natural language processing systems
KW  - App stores
KW  - Information overloads
KW  - Language model
KW  - Large language model
KW  - Mobile app
KW  - Mobile app review
KW  - Selection decisions
KW  - Source text
KW  - Abstracting
U1  - 09288910 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: S. Shrestha; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, United States; email: sshre35@lsu.edu; A. Mahmoud; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, United States; email: mahmoud@csc.lsu.edu; CODEN: ASOEE
N2  - Mobile app users commonly rely on app store ratings and reviews to find apps that suit their needs. However, the sheer volume of reviews available on app stores can lead to information overload, thus impeding users’ ability to make informed app selection decisions. To overcome this limitation, in this paper, we leverage Large Language Models (LLMs) to summarize mobile app reviews. In particular, we use the Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate abstractive, semantically dense, and readable summaries of mobile app reviews. The CoD prompt is engineered to iteratively extract salient entities from the source text and fuse them into a fixed-length summary. We evaluate the performance of our approach using a large dataset of mobile app reviews. We further conduct an empirical evaluation with 48 study participants to assess the readability of the generated CoD summaries. Our results show that an altered CoD prompt can correctly identify the main themes in user reviews and consolidate them into a natural language summary that is intended for end-user consumption. The prompt also manages to maintain the readability of the generated summaries while increasing their density. Our work in this paper aims to substantially improve mobile app users’ experience by providing an effective mechanism for summarizing important user feedback in the review stream. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
ER  -
TY  - JOUR
AU  - Sánchez-Berriel, I.
AU  - Pérez-Nava, F.
AU  - Pérez-Rosario, L.
T1  - Natural Interaction in Virtual Heritage: Enhancing User Experience with Large Language Models
JO  - Electronics (Switzerland)
Y1  - 2025
VL  - 14
IS  - 2478
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009087008&doi=10.3390%2felectronics14122478&partnerID=40&md5=1302a9ccafcf6d8369ca1c0dfa5751ce
M3  - https://doi.org/10.3390/electronics14122478
KW  - cultural heritage
KW  - Large Language Models
KW  - natural interaction
KW  - prompt engineering
KW  - user experience
KW  - virtual humans
KW  - virtual reality
KW  - Behavioral research
KW  - Computational linguistics
KW  - Human computer interaction
KW  - Natural language processing systems
KW  - Speech communication
KW  - Speech processing
KW  - Usability engineering
KW  - User centered design
KW  - User experience
KW  - User interfaces
KW  - Cultural heritages
KW  - Dialogue generations
KW  - Language model
KW  - Large language model
KW  - Limited devices
KW  - Natural interactions
KW  - Prompt engineering
KW  - Users' experiences
KW  - Virtual heritage
KW  - Virtual humans
KW  - Virtual reality
U1  - 20799292 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: I. Sánchez-Berriel; Department of Ingeniería Informática y de Sistemas, Escuela Técnica Superior de Ingeniería Informatica, Universidad de La Laguna, San Cristóbal de La Laguna, 38200, Spain; email: isanchez@ull.edu.es
N2  - In recent years, Virtual Reality (VR) has emerged as a powerful tool for disseminating Cultural Heritage (CH), often incorporating Virtual Humans (VHs) to guide users through historical recreations. The advent of Large Language Models (LLMs) now enables natural, unscripted communication with these VHs, even on limited devices. This paper details a natural interaction system for VHs within a VR application of San Cristóbal de La Laguna, a UNESCO World Heritage Site. Our system integrates Speech-to-Text, LLM-based dialogue generation, and Text-to-Speech synthesis. Adhering to user-centered design (UCD) principles, we conducted two studies: a preliminary study revealing user interest in historically adapted language, and a qualitative test that identified key user experience improvements, such as incorporating feedback mechanisms and gender selection for VHs. The project successfully developed a prioritized user experience, focusing on usability evaluation, immersion, and dialogue quality. We propose a generalist methodology and recommendations for integrating unscripted VH dialogue in VR. However, limitations include dialogue generation latency and reduced quality in non-English languages. While a formative usability test evaluated the process, the small sample size restricts broad generalizations about user behavior. © 2025 by the authors.
ER  -
TY  - JOUR
AU  - Patel, H.
AU  - Boucher, D.
AU  - Fallahzadeh, E.
AU  - Hassan, A. E.
AU  - Adams, B.
T1  - A State-of-the-Practice Release-Readiness Checklist for Generative AI-Based Software Products: A Gray Literature Survey
JO  - IEEE Software
Y1  - 2025
VL  - 42
IS  - 1
SP  - 74–83
EP  - 74–83
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002680342&doi=10.1109%2fMS.2024.3440190&partnerID=40&md5=090d16c79e99ba0f2970debe2d3813f3
M3  - https://doi.org/10.1109/MS.2024.3440190
KW  - Grey literature
KW  - Key release
KW  - Language model
KW  - Performance-monitoring
KW  - Pre-training
KW  - Software
KW  - Software products
KW  - State of the practice
KW  - Systematic Review
KW  - Users' experiences
U1  - 07407459 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; CODEN: IESOE
N2  - We investigate the complexities of integrating large language models (LLMs) into software products, focusing on challenges encountered for determining their readiness for release. Our review of gray literature identifies common challenges in deploying LLMs, from pretraining and fine-tuning to user experience considerations. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Sergeyuk, A.
AU  - Titov, S.
AU  - Izadi, M.
T1  - In-IDE Human-AI Experience in the Era of Large Language Models; A Literature Review
PB  - Association for Computing Machinery, Inc
AD  - Delft University of Technology, Delft, Netherlands
Y1  - 2024
SP  - 95–100
EP  - 95–100
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197312387&doi=10.1145%2f3643796.3648463&partnerID=40&md5=151adacb81dec63a045b134bb6a61601
M3  - https://doi.org/10.1145/3643796.3648463
KW  - artificial intelligence
KW  - human-computer interaction
KW  - integrated development environment
KW  - programming
KW  - user experience
KW  - user studies
KW  - Computer software selection and evaluation
KW  - Integrodifferential equations
KW  - Problem oriented languages
KW  - Research and development management
KW  - Artificial intelligence tools
KW  - Computer interaction
KW  - Decisions makings
KW  - Integrated development environment
KW  - Language model
KW  - Literature reviews
KW  - Programming
KW  - Software development process
KW  - User study
KW  - Users' experiences
KW  - Decision making
U1  - 979-840070580-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 4; Correspondence Address: A. Sergeyuk; JetBrains Research, Belgrade, Serbia; email: agnia.sergeyuk@jetbrains.com; Conference name: 1st Integrated Development Environments Workshop, IDE 2024, co-located with the 46th International Conference on Software Engineering, ICSE 2024; Conference code: 201673
N2  - Integrated Development Environments (IDEs) have become central to modern software development, especially with the integration of Artificial Intelligence (AI) to enhance programming efficiency and decision-making. The study of in-IDE Human-AI Experience is critical in understanding how these AI tools are transforming the software development process, impacting programmer productivity, and influencing code quality.We conducted a literature review to study the current state of in-IDE Human-AI Experience research, bridging a gap in understanding the nuanced interactions between programmers and AI assistants within IDEs. By analyzing 36 selected papers, our study illustrates three primary research branches: Design, Impact, and Quality of Interaction.The trends, challenges, and opportunities identified in this paper emphasize the evolving landscape of software development and inform future directions for research, and development in this dynamic field. Specifically, we invite the community to investigate three aspects of these interactions: designing task-specific user interface, building trust, and improving readability. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ER  -
TY  - JOUR
AU  - Coman, A.
AU  - Vasilache, S.
T1  - 17th International Conference on Social Computing and Social Media, SCSM 2025, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15787 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007138383&partnerID=40&md5=d549c67cd04881029d069804f24abcce
U1  - 03029743 (ISSN); 978-303193535-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 17th International Conference on Social Computing and Social Media, SCSM 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332669
N2  - The proceedings contain 51 papers. The special focus in this conference is on Social Computing and Social Media. The topics include: A Serious Game Approach for Teaching Requirements Engineering: User Experience Evaluation; identification of Older Adults’ Characteristics that Affect the Usability of Mobile Applications: A Tertiary Study; generating Product Descriptions Using Customer Reviews on E-Commerce Sites; improving Intention Recognition Efficiency: A Study on Skeletal Data Dimensionality Reduction and Neural Architectures; first Steps Toward the Agile Integration of Information Architecture into a User-Centered Development Process; LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded Robot Teleoperation; redimensioning Visible Learning and Teaching in the Dynamics of a New Reality; Generative AI in Education: Exploring EAP Faculty Perspectives at a Multicultural UAE University; the Challenges Faced by Albanian Teachers in the Use of Media Technology During Teaching; Undergraduate Students’ Journey with AI in the United Arab Emirates; Perspectives of Faculty on the Easiness and Usefulness of AI Tutoring Systems in Higher Education; exploring the Use of Paraphrasing Tools in Academic Writing and Its Potential Relation with Instances of Plagiarism; a Property Checklist for Evaluating the Student Experience with Consideration of Cultural Aspects; human-Robot Interaction in Higher Education: A Literature Review; artificial Intelligence in Higher Education: Student Perceptions of the Adoption and Integration in Ghana, West Africa; a Management Model for Evaluating Scientific Productivity in Chilean Universities: A Case Analysis; virtual Reality Meets Social Media: Transforming Skill Acquisition in Physiotherapy, Veterinary Surgery, and Driver Training; Evolution of Emotional Response of PLEA–An Embodied Virtual Being with Emotional Capabilities.
ER  -
TY  - JOUR
AU  - Algahtani, A.
T1  - A COMPARATIVE STUDY OF AI-BASED EDUCATIONAL TOOLS: EVALUATING USER INTERFACE EXPERIENCE AND EDUCATIONAL IMPACT
JO  - Journal of Theoretical and Applied Information Technology
Y1  - 2024
VL  - 102
IS  - 5
SP  - 1746–1758
EP  - 1746–1758
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188191183&partnerID=40&md5=c9c129240fe881684f80fbaa0669730b
KW  - AI In Education
KW  - Availability
KW  - Educational Tools
KW  - Reliability
KW  - Skills
U1  - 19928645 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 4; Correspondence Address: A. Algahtani; College of Education, Majmaah University, Almajmaah, Saudi Arabia; email: a.algahtani@mu.edu.sa
N2  - This study provides a comprehensive analysis of AI-based educational tools, focusing on their impact on user experience and education. It explores the capabilities of AI tools in transforming the teaching and learning process through specialized AI tool-based learning, intelligent educational AI systems, Automation in the grading process, and predictive analytics. This research helps investigate the role of large language models (LLMs) in educational assessment, including test planning, item generation, test administration, and scoring. It involves teachers with STEM-related teaching experience who were introduced to an AI-enhanced scaffolding system for scientific writing. The study also includes a systematic review of AI applications in higher education, highlighting the ethical implications, challenges, and risks associated with AI in education. The findings provide a deep dive for educators, management, and stakeholders working on maximizing the outputs of AI in education while eliminating the associated risks. The study emphasizes the importance of understanding teachers' attitudes and experiences with AI in education to effectively integrate AI into teaching and learning practices. It also highlights the need to further explore ethical and educational approaches to applying AI in education. The research underscores the benefits and challenges of AI integration in education, emphasizing the need for transparent and ethical AI algorithms, personalized and adaptive assessment approaches, and the importance of human judgment in AI-powered education. © Little Lion Scientific.
ER  -
TY  - CONF
AU  - Rasool, Z.
AU  - Barnett, S.
AU  - Willie, D.
AU  - Kurniawan, S.
AU  - Balugo, S.
AU  - Thudumu, S.
AU  - Abdelrazek, M.
T1  - LLMS for test input generation for semantic applications
PB  - Association for Computing Machinery, Inc
AD  - Applied Artificial Intelligence Institute, Deakin University, Geelong, VIC, Australia
Y1  - 2024
SP  - 160–165
EP  - 160–165
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196548201&doi=10.1145%2f3644815.3644948&partnerID=40&md5=6dd5aaf6d58f72396a4fcb1ee9007b99
M3  - https://doi.org/10.1145/3644815.3644948
KW  - large language model
KW  - query evaluation
KW  - question answering
KW  - semantic cache
KW  - test input generation
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Open source software
KW  - Query processing
KW  - Statistical tests
KW  - Language model
KW  - Large language model
KW  - Query evaluation
KW  - Question Answering
KW  - Semantic cache
KW  - Software-systems
KW  - State of the art
KW  - Test input generation
KW  - Test inputs
KW  - Unstructured texts
KW  - Semantics
U1  - 979-840070591-5 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 3; Correspondence Address: Z. Rasool; Applied Artificial Intelligence Institute, Deakin University, Geelong, Australia; email: zafaryab.rasool@deakin.edu.au; Conference name: 3rd International Conference on AI Engineering, CAIN 2024, co-located with the 46th International Conference on Software Engineering, ICSE 2024; Conference date: 14 April 2024 through 15 April 2024; Conference code: 200197
N2  - Large language models (LLMs) enable state-of-the-art semantic capabilities to be added to software systems such as semantic search of unstructured documents and text generation. However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience. To address this problem, semantic caches are used to check for answers to similar queries (that may have been phrased differently) without hitting the LLM service. Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system. Adopting semantic cache techniques usually requires testing the effectiveness of a semantic cache (accurate cache hits and misses) which requires a labelled test set of similar queries and responses which is often unavailable. In this paper, we present VaryGen, an approach for using LLMs for test input generation that produces similar questions from unstructured text documents. Our novel approach uses the reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise subtle variations to queries, and 3) evaluate the synthesised test dataset. We evaluated our approach in the domain of a student question and answer system by qualitatively analysing 100 generated queries and result pairs, and conducting an empirical case study with an open source semantic cache. Our results show that query pairs satisfy human expectations of similarity and our generated data demonstrates failure cases of a semantic cache. Additionally, we also evaluate our approach on Qasper dataset. This work is an important first step into test input generation for semantic applications and presents considerations for practitioners when calibrating a semantic cache. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ER  -
TY  - CONF
AU  - Krogstie, J.
AU  - Krogstie, B. R.
AU  - Van Der Velden, M.
AU  - Gasparini, A. A.
AU  - Chasanidou, D.
T1  - Sustainability analysis of AI-based tools in higher education
PB  - Association for Computing Machinery
AD  - Norwegian University of Science and Technology Rocquencourt, Norway
Y1  - 2024
IS  - 46
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206584554&doi=10.1145%2f3677045.3685461&partnerID=40&md5=414191aff8e78e1a7c88ab3c7ae5b12d
M3  - https://doi.org/10.1145/3677045.3685461
KW  - Artificial Intelligence
KW  - Higher Education
KW  - Sustainability Analysis
KW  - Sustainability Awareness Framework
KW  - Design and development process
KW  - Digital tools
KW  - High educations
KW  - Holistic view
KW  - Learn+
KW  - Software-systems
KW  - Sustainability analysis
KW  - Sustainability awareness framework
KW  - University course
KW  - User-centric
KW  - Sustainable development goals
U1  - 979-840070965-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J. Krogstie; Norwegian University of Science and Technology, Dublin, Ireland; email: john.krogstie@ntnu.no; Conference name: 13th Nordic Conference on Human-Computer Interaction, NordiCHI 2024; Conference date: 13 October 2024 through 16 October 2024; Conference code: 202954
N2  - Educating the next generation of students on pathways toward sustainability is important and highly relevant for various disciplines, including HCI and CSCW. HCI research has contributed much to the field of sustainability, though mainly taking user-centric and environmental perspectives, missing a holistic view of sustainability. By incorporating sustainability goals into design and development processes, students can learn that they can have a significant impact and improve the longevity and durability of digital tools. This workshop will present and explore the SusAF framework as a tool for the sustainability analysis of digital tools in university courses. SusAF is an established framework that enables exploring the different dimensions of sustainability. In this workshop, we will apply this framework to AI-based tools, such as ChatGPT and DALL-E. The goal is to present SusAF as a framework for sustainability analysis of software systems used in higher education and to explore, with the participants, the different sustainability challenges of using AI-based tools. In this full-day workshop, we want to engage an interdisciplinary group of researchers, lecturers, and practitioners in sustainability analysis and we will explore the relevance of SusAF in various educational settings. © 2024 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Li, Y.
AU  - Liu, H.
AU  - Wald, M.
T1  - DeepVision: Heads-up Computing and AI in Education
PB  - Association for Computing Machinery, Inc
AD  - University of Southampton, Southampton, United Kingdom
Y1  - 2024
SP  - 627–630
EP  - 627–630
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206156694&doi=10.1145%2f3675094.3678991&partnerID=40&md5=69fa756b952717ea32716b9689db6896
M3  - https://doi.org/10.1145/3675094.3678991
KW  - AI
KW  - AR
KW  - Heads-up Computing
KW  - Inclusive User Experience Design
KW  - Large Language Model
KW  - Multimodal Information Access and Retrieval
KW  - Knowledge representation
KW  - Mixed reality
KW  - Head-up computing
KW  - In-class learning
KW  - Inclusive user experience design
KW  - Information access
KW  - Language model
KW  - Large language model
KW  - Learning experiences
KW  - Multi-modal information
KW  - Multimodal information access and retrieval
KW  - User experience design
KW  - Augmented reality
U1  - 979-840071058-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp Companion 2024; Conference date: 5 October 2024 through 9 October 2024; Conference code: 202842
N2  - Heads-up computing together with AI can enhance in-class learning experiences. In this position paper, we propose the development of a multimodal AI system called DeepVision that integrates Automatic Speech Recognition (ASR), Large Language Models (LLM), Large Vision Models (LVM), Information Retrieval (IR) and Inclusive User Experience Design (IUX) to convert real-time lectures into multiple knowledge representations. These will be visualized on heads-up communication devices such as Augmented Reality (AR) and Mixed Reality (MR) devices. The initiative is a collaboration between Habitat Learn Limited (HLL) and the University of Southampton, leveraging HLL's existing software and extensive data repository to address the challenges of traditional and digital learning environments, especially for students with disabilities or language differences. © 2024 Copyright is held by the owner/author(s).
ER  -
TY  - JOUR
AU  - Marchiori, M.
AU  - García Peñalvo, F.
T1  - 19th International Conference on Web Information Systems and Technologies, WEBIST 2023
JO  - Lecture Notes in Business Information Processing
Y1  - 2025
VL  - 543 LNBIP
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006822226&partnerID=40&md5=0635d1a3b27ef48691750ab5695ed441
U1  - 18651348 (ISSN); 978-303189620-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 19th International Conference on Web Information Systems and Technologies, WEBIST 2023; Conference date: 15 November 2023 through 17 November 2023; Conference code: 332309
N2  - The proceedings contain 13 papers. The special focus in this conference is on Web Information Systems and Technologies. The topics include: Overview of Web Application Performance Optimization Techniques; interest and Challenges of Students Regarding Web and Mobile Technologies; SWI (Soft Web Intelligence) Powered by User-Defined Fuzzy Operators and Aggregators; matching Pre-processed Database Records Using Natural Language Queries on Advertisements; mapping Business Web Applications for Web Automation; On the Construction of Text-to-SQL Tools Based on Large Language Models for Real-World Relational Databases; refining Community Boundaries in Bee Swarm Optimization for Enhanced Community Detection in Social Networks; digital Citizen Rights: Accessibility of E-Government Websites in Italy; Assessing the Validity of the Trust Factor Within User Experience Questionnaire Plus (UEQ+): A Comprehensive Validation Study; digital Transition to a Paperless Checklist Integrated into the Industrial Information System; personalized User Experience and Engagement in Interactive Mobile Augmented Reality Software Through Fuzzy Weights.
ER  -
TY  - CONF
AU  - Fischer, D. V.
AU  - Haug, J.
AU  - Schoppel, P.
AU  - Abke, J.
AU  - Becker, M.
AU  - Hagel, G.
T1  - Evaluation of a Node-based Automatic Short Answer Tool “NodeGrade”
PB  - Association for Computing Machinery, Inc
AD  - Faculty of Engineering and Computer Science, University of Applied Sciences Aschaffenburg, Bavaria, Aschaffenburg, Germany
Y1  - 2025
SP  - 20–29
EP  - 20–29
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008422181&doi=10.1145%2f3723010.3723021&partnerID=40&md5=390800fa65e519dd72c307c7d1370ecf
M3  - https://doi.org/10.1145/3723010.3723021
KW  - AI in Education
KW  - ASAG
KW  - Automatic Short Answer Grading
KW  - Large Language Models
KW  - Natural Language Processing
KW  - Short Answer Scoring
KW  - Software Engineering Education
KW  - Artificial intelligence
KW  - Automatic testing
KW  - Education computing
KW  - Engineering education
KW  - Grading
KW  - Software testing
KW  - User experience
KW  - User interfaces
KW  - AI in education
KW  - Automatic short answer grading
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Natural language processing
KW  - Natural languages
KW  - Short answer scoring
KW  - Software engineering education
KW  - Natural language processing systems
U1  - 979-840071282-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: D.V. Fischer; Faculty of Computer Science, Kempten University of Applied Sciences, Kempten, Bavaria, Germany; email: david.fischer@hs-kempten.de; Conference name: 6th European Conference on Software Engineering Education, ECSEE 2025; Conference date: 2 June 2025 through 4 June 2025; Conference code: 208985
N2  - NodeGrade tries to provide a suitable solution for the problem of time-intensive short answer grading. This research focuses simultaneously on performance, functionality and user experience, which is underlined by a triangulated approach. The evaluation results show comparable performance of NodeGrade on public datasets, even outperforming GPT-4 on the SemEval 2013 Task 7. Matching of NodeGrade’s output with multiple human expert raters reveals some weaknesses regarding cases at the lower and upper boundary. In terms of user experience, the interviewed and observed students recognized both positive facets, like better learning support and helpful feedback, and negative sides, including technical limitations and lack of transparency. Overall, NodeGrade promises high potential for further practical use and testing in the field of software engineering education and automatic short answer grading. © 2025 Copyright held by the owner/author(s).
ER  -
TY  - JOUR
AU  - Yang, R.
AU  - Fu, M.
AU  - Tantithamthavorn, C.
AU  - Arora, C.
AU  - Vandenhurk, L.
AU  - Chua, J.
T1  - RAGVA: Engineering retrieval augmented generation-based virtual assistants in practice
JO  - Journal of Systems and Software
Y1  - 2025
VL  - 226
IS  - 112436
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000995469&doi=10.1016%2fj.jss.2025.112436&partnerID=40&md5=f208c6532413ab90fa8e55b72756aa44
M3  - https://doi.org/10.1016/j.jss.2025.112436
KW  - AI engineering
KW  - LLMOps
KW  - Responsible AI
KW  - Retrieval augmented generation
KW  - SE4AI
KW  - Software engineering
KW  - Virtual assistants
KW  - Computer software selection and evaluation
KW  - Human engineering
KW  - Search engines
KW  - Virtual reality
KW  - Engineering teams
KW  - Excel
KW  - Language model
KW  - LLMOp
KW  - Retrieval mechanisms
KW  - Application programs
U1  - 01641212 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: C. Tantithamthavorn; Monash University, Australia; email: chakkrit@monash.edu; CODEN: JSSOD
N2  - Retrieval-augmented generation (RAG)-based applications are gaining prominence due to their ability to leverage large language models (LLMs). These systems excel at combining retrieval mechanisms with generative capabilities, resulting in contextually relevant responses that enhance user experience. In particular, Transurban, a road operation company, replaced its rule-based virtual assistant (VA) with a RAG-based VA (RAGVA) to offer flexible customer interactions and support a wider range of scenarios. This paper presents an experience report from Transurban's engineering team on building and deploying a RAGVA, offering a step-by-step guide for creating a conversational application and engineering a RAGVA. The report serves as a reference for future researchers and practitioners. While the engineering processes for traditional software applications are well-established, the development and evaluation of RAG-based applications are still in their early stages, with numerous emerging challenges remaining uncharted. To address this gap, we conduct a focus group study with Transurban practitioners regarding developing and evaluating their RAGVA. We identified eight challenges encountered by the engineering team and proposed eight future directions that should be explored to advance the development of RAG-based applications. This study contributes to the foundational understanding of a RAG-based conversational application and the emerging AI software engineering challenges it presents. © 2025 The Authors
ER  -
TY  - CONF
AU  - Ahi, K.
AU  - Mansour, S.
AU  - Wu, S.
AU  - Fenger, G.
AU  - Ayya, A. S.
AU  - Opitz, J.
AU  - Zine El Abidine, N.
AU  - Hatem, H.
AU  - Essam, A.
AU  - El Dessouki, A.
AU  - Sriram, S.
AU  - Biswas, S.
AU  - Bhamidipati, S.
AU  - Pereira, M.
AU  - Fekry, M.-A.
AU  - Fawzi, R.
AU  - Samir, H.
AU  - Nalakath, M. D.
AU  - Mabrouk, O.
AU  - Sendelbach, M. J.
AU  - Schuch, N. G.
T1  - AI-Powered End-to-End Product Lifecycle: UX-Centric Human-in-the-Loop System Boosting Reviewer Productivity by 82% and Accelerating Decision-Making via Real-Time Anomaly Detection and Data Refinement with GPU-Accelerated Computer Vision, Edge Computing, and Scalable Cloud
PB  - SPIE
AD  - SIEMENS EDA, France
Y1  - 2025
VL  - 13426
IS  - 1342632
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007150138&doi=10.1117%2f12.3052252&partnerID=40&md5=febde7180cdc9bd8b9ff8d2acc336a8f
M3  - https://doi.org/10.1117/12.3052252
KW  - Cutting
KW  - Image correlation
KW  - Medical robotics
KW  - Mobile edge computing
KW  - Photointerpretation
KW  - Precision agriculture
KW  - Virtualization
KW  - Data refinements
KW  - Decisions makings
KW  - Defect detection
KW  - Edge computing
KW  - GPU-accelerated
KW  - High-throughput
KW  - Human-in-the-loop
KW  - Image processing algorithm
KW  - Real- time
KW  - Real-time anomaly detections
KW  - Medical imaging
U1  - 0277786X (ISSN); 978-151068638-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: Metrology, Inspection, and Process Control XXXIX 2025; Conference date: 24 February 2025 through 28 February 2025; Conference code: 209146; CODEN: PSISD
N2  - As the amount of data continues to grow, with images becoming larger in terms of bytes and details across scientific, photography, industrial, medical, gaming, augmented reality (AR), virtual reality (VR), autonomous systems, and digital content creation domains, and as generative AI (Gen AI) and large language models (LLMs) gain popularity, the computational demands on modern systems have reached unprecedented levels. In this work, we introduce optimized, GPU-parallelized image processing algorithms that significantly reduce computation requirements, improving both processing speed and scalability. Additionally, in our work, edge computing plays a critical role in reducing latency and bandwidth consumption, enabling real-time decision-making by processing data at the edge, closer to the source. Scalable cloud architectures further support this approach by providing elastic, high-throughput processing capabilities for data-intensive tasks. Moreover, in many applications such as medical imaging, autonomous vehicle perception, satellite image analysis, high-precision manufacturing, robotics, and real-time anomaly detection, image review by humans is not only error-prone but also time-consuming, often limiting scalability and response times. To address this, we present a human-in-the-loop (HITL) automation framework, coupled with a UX-centric, GUI-based system, that reduces human workload by 82%, making high-throughput image review feasible even at the scale required for cutting-edge applications like medical imaging, digital marketplaces, platform integrity, semiconductor manufacturing, autonomous systems, precision agriculture, and industrial defect detection. This framework leverages GPU-accelerated computer vision, edge computing, and scalable cloud infrastructure to optimize data processing across the entire cycle, from initial capture to real-time analysis and decision-making. To demonstrate this approach, we utilize challenging images characterized by low signal-to-noise ratios (SNR), defects, focus variations, and contrast inconsistencies. For this purpose, we selected a dataset of nanometer-resolution images, including ultra-high-resolution scanning electron microscope (SEM) images. These images, captured at the physical limits of SEM technology, contain nanometer-scale details essential for device performance but are characterized by extreme noise, focus variations, and high pixel density. This makes them ideal for evaluating the limits of real-time, high-precision image processing algorithms. Even though these high-variability images exhibit extremely low SNR, they are critical for lithography process control, defect detection, and optical proximity correction (OPC) modeling in advanced semiconductor manufacturing. Traditional review methods, such as manual inspection and critical dimension (CD)-based modeling, are not only slow and prone to human error but also limited to localized measurements, often missing critical pattern variations that impact device performance. As the industry moves toward contour-based modeling, which requires analyzing entire images to capture edge roughness, line width variation, and pattern fidelity, there is a growing demand for fully automated, GPU-accelerated, cloud-scalable AI-driven solutions capable of processing this highly complex data efficiently. In our work, the use case of nanometer-resolution SEM images provides a compelling demonstration of the framework's capabilities in handling extremely complex data. Additionally, this paper presents a novel GPU-accelerated machine learning (ML) and AI-powered digital image processing framework for automated image and data refinement and defect detection. The system generates high-resolution pattern fidelity and defect maps by leveraging structural similarity with AI-enhanced adaptive reference models. Key features include AI-driven extrema counting, sharpness metrics, noise level evaluation, and line edge roughness (LER) and line width roughness (LWR) analysis. These features are extracted to cluster images into quality-based groups, enabling real-time defect classification through cloud-scalable hierarchical clustering that surpasses CD-based methods in both accuracy and efficiency. A critical limitation of conventional filtering is its reliance on multiple images per process condition (exposure and focus) to identify outliers. This paper addresses this challenge by introducing an AI-powered clustering approach capable of detecting failures, even when only a single image is available per process window condition, eliminating the need for rule-based filtering. This method integrates AI-driven LER and LWR evaluation to ensure robust clustering across varying process conditions, improving both detection accuracy and operational efficiency. To further streamline image and data analysis, the framework includes a GPU-accelerated, edge computing, cloud-integrated graphical user interface (GUI) for real-time, high-performance computing (HPC)-optimized data and image analysis, feature extraction, classification, edge detection, segmentation, data cleaning, measurement, anomaly detection, and defect identification. This solution, integrated into Calibre SEMSuite™, supports multi-cloud deployment for enhanced scalability, usability, and performance, providing users with a powerful tool for fully automated, AI-driven image classification, making high-throughput image review feasible even at the scale required for cutting-edge applications. By integrating these advanced capabilities, the proposed framework not only addresses the computational challenges of modern AI-driven image processing but also significantly reduces human intervention, paving the way for scalable, real-time AI processing in diverse applications such as medical imaging, autonomous vehicle perception, satellite image analysis, high-precision manufacturing, robotics, and real-time anomaly detection. © 2025 SPIE.
ER  -
TY  - JOUR
AU  - Zhang, C.
AU  - He, S.
AU  - Qian, J.
AU  - Li, B.
AU  - Li, L.
AU  - Qin, S.
AU  - Kang, Y.
AU  - Ma, M.
AU  - Liu, G.
AU  - Lin, Q.
AU  - Rajmohan, S.
AU  - Zhang, D.
AU  - Zhang, Q.
T1  - Large Language Model-Brained GUI Agents: A Survey
JO  - Transactions on Machine Learning Research
Y1  - 2025
VL  - 2025-June
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008342615&partnerID=40&md5=9e7a2bec87ff46c89dc5e0e862575318
U1  - 28358856 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: C. Zhang; Microsoft, United States; email: chaoyun.zhang@microsoft.com; S. He; Microsoft, United States; email: shilin.he@microsoft.com
N2  - Graphical User Interfaces (GUIs) have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. Traditionally, automating GUI interactions relied on script-based or rule-based approaches, which, while effective for fixed workflows, lacked the flexibility and adaptability required for dynamic, real-world applications. The advent of Large Language Models (LLMs), particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, task generalization, and visual processing. This has paved the way for a new generation of “LLM-brained” GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry. To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-powered GUI agents, exploring their historical evolution, core components, and advanced techniques. We address critical research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of fine-tuned models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-powered GUI agents. We anticipate that this survey will serve both as a practical cookbook for constructing LLM-powered GUI agents, and as a definitive reference for advancing research in this rapidly evolving domain. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey. Additionally, a searchable webpage is available at https://aka.ms/gui-agent for easier access and exploration. © 2025, Transactions on Machine Learning Research. All rights reserved.
ER  -
TY  - JOUR
AU  - da Silva H., Plácido
AU  - Cipresso, P.
T1  - 8th International Conference on Computer-Human Interaction Research and Applications, CHIRA 2024
JO  - Communications in Computer and Information Science
Y1  - 2025
VL  - 2370 CCIS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000799924&partnerID=40&md5=9e56f467a79c9982ab38fb51f51baa32
U1  - 18650929 (ISSN); 978-303182632-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 8th International Conference on Computer-Human Interaction Research and Applications, CHIRA 2024; Conference date: 21 November 2024 through 22 November 2024; Conference code: 328309
N2  - The proceedings contain 61 papers. The special focus in this conference is on Computer-Human Interaction Research and Applications. The topics include: How Different Blink Patterns of Pet Robots Evoke Feelings of Affection in People; arm in Motion: How Motion Modality and Erratic Behavior of a Robotic Arm Shape User Perception; An Examination of Pre-school Children’s Usage Behavior of Augmented Reality: Traditional vs. AR-Assisted LEGO® Building; Interviewing ChatGPT-Generated Personas to Inform Design Decisions; an Experiment to Investigate Changes in Physiological Signals During Subtle Wind and Scent Presentation for Designing Subtle Notifications; Towards Multi-stakeholder Evaluation of ML Models: A Crowdsourcing Study on Metric Preferences in Job-Matching System; current Design Practices in Applied Augmented Reality Research: A Methodological Review; emotion-Aware Interfaces: Empirical Methods for Adaptive User Interface; bridging Medical Genetics, Genetic Counselling, and Patients: Proposing an Immersive, Interactive, and Holographic Health Information Platform with Evaluation Methods for Personalized Patient Education; why Do(n’t) You Trust Us? Highlighting the Importance of Trust and Transparency for Designing B2B Platforms in Electronics Manufacturing; enriched with Behaviour Theory Topic Guide Template for Digital Behaviour Change Interventions; evaluating Remote Communication Applications Using Student Usability Reviews; Enhancing EEG-Based User Verification with a Normalized Neural Network Ensemble Approach; systematic Literature Review of Gamification Design in Higher Education Programming Courses: Methodological Rigor Exposed; the Effect of Progressive Disclosure in the Transparency of Large Language Models; design and Implementation of a Practice Record Visualization System Using Piano Performance Tracking Technology; User Issues and Concerns in Generative AI: A Mixed-Methods Analysis of App Reviews; Caregiver Acceptability of an LLM-Powered Assistant Interface to Improve Sleep Quality of the Elderly; user Experience and Information Security Heuristics for Digital Identity Wallets; how Can Heuristics Be Communicated?.
ER  -
TY  - CONF
AU  - Rusdianto, D. S.
AU  - Fabroyir, H.
AU  - Yuhana, U. L.
T1  - Innovative Approaches to Impact Analysis of Requirement Changes using LLM in Software Projects
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia
Y1  - 2024
SP  - 604–610
EP  - 604–610
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215267650&doi=10.1109%2fISCT62336.2024.10791169&partnerID=40&md5=a9c7a86abb8012703251e2925780faec
M3  - https://doi.org/10.1109/ISCT62336.2024.10791169
KW  - Change Impact Analysis
KW  - GPT-4
KW  - Large Language Model
KW  - Predictive Analytics
KW  - Requirement Change Management
KW  - Computer software selection and evaluation
KW  - Program processors
KW  - Change impact analysis
KW  - Impact analysis
KW  - Innovative approaches
KW  - Language model
KW  - Large language model
KW  - Requirement change managements
KW  - Requirements change
KW  - Software project
KW  - Software requirements
KW  - Enterprise software
U1  - 0747668X (ISSN); 979-835036519-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Conference name: 1st IEEE International Symposium on Consumer Technology, ISCT 2024; Conference date: 13 August 2024 through 16 August 2024; Conference code: 205290; CODEN: DTPEE
N2  - The software industry is continuously evolving because of emerging technologies, evolving development methodologies, changing business needs, and shifting stakeholder preferences. Effectively managing daily changes is critical, especially given the complexity of requirement change management (RCM), which requires the development of adaptable systems that accurately reflect dynamic conditions. This paper explores the integration of Large Language Models (LLMs), specifically GPT-4, into software engineering to enhance the effectiveness of RCM. This paper proposes a novel methodology that leverages the sophisticated reasoning capabilities of LLM GPT-4 to analyze and predict the impacts of software requirement changes before implementation. Our approach combines LLMs with a structured analysis framework that interprets changes in software requirements and evaluates their potential impacts across multiple dimensions. The use of GPT-4 can revolutionize how software engineers understand and implement requirements changes, providing a comprehensive tool for predicting the impact of such changes and evaluating the consequences accurately. The results of this research can help requirements engineers perform automated change impact analysis, enabling them to make informed decisions regarding the acceptance or rejection of requirement change requests. This ultimately supports the creation of software products that better align with consumer needs and expectations, ensuring enhanced user experiences and satisfaction. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Brusilovsky, P.
AU  - de Gemmis, M.
AU  - Felfernig, A.
AU  - Polignano, M.
AU  - Semeraro, G.
AU  - Willemsen, M. C.
T1  - 11th Joint Workshop on Interfaces and Human Decision Making for Recommender Systems (IntRS’24)
PB  - Association for Computing Machinery, Inc
AD  - Eindhoven University of Technology, Eindhoven, Netherlands
Y1  - 2024
SP  - 1253–1257
EP  - 1253–1257
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210497185&doi=10.1145%2f3640457.3687098&partnerID=40&md5=47b69ec053cfb42c07f0689c132368cc
M3  - https://doi.org/10.1145/3640457.3687098
KW  - Decision Biases
KW  - Evaluation Methods
KW  - Human Decision Making
KW  - Human-Computer Interaction
KW  - Large Language Models
KW  - LLMs
KW  - Recommender Systems
KW  - Symbiotic-AI
KW  - User Interfaces
KW  - Cognitive systems
KW  - Decision support systems
KW  - Human engineering
KW  - User interfaces
KW  - Computer interaction
KW  - Decision bias
KW  - Evaluation methods
KW  - Human decision-making
KW  - Language model
KW  - Large language model
KW  - Symbiotics
KW  - Man machine systems
U1  - 979-840070505-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 18th ACM Conference on Recommender Systems, RecSys 2024; Conference date: 14 October 2024 through 18 October 2024; Conference code: 203832
N2  - The primary goal of Recommender Systems is to suggest the most suitable items to a user, aligning them with the user’s interests and needs. RSs are essential for modern e-commerce, helping users discover content and products by predicting suitable items based on their past behavior. However, their success isn’t just about advanced algorithms. The design of the user interface and a good integration with the human decision-making process are equally crucial. A well-designed interface enhances the user experience and makes recommendations more effective, while a poor interface can lead to frustration. Recognizing this limitation, recent trends in Recommender Systems (RSs) are increasingly focusing on integrating Symbiotic Human-Machine Decision-Making models. These models aim to offer users a dynamic and persuasive interface that helps them better understand and engage with recommendations. This shift is a crucial step toward developing recommender systems that truly connect with users and offer a more enjoyable, trustworthy, explainable, and user-friendly experience. Although early efforts concentrated on creating systems that could proactively predict user preferences and needs, modern RSs also emphasize the importance of providing users with control and transparency over their recommendations. Finding the right balance between proactivity and user control is essential to ensure that the system supports users without being too intrusive, thus improving their overall satisfaction. As Large Language Models (LLMs) become more integrated into recommender systems, the importance of user-centric interfaces and a deep understanding of decision-making becomes even more critical. Effective integration of LLMs requires interfaces that are both visually and cognitively engaging. These aspects are the main discussion topics of the Joint Workshop on Interfaces and Human Decision Making for Recommender Systems at RecSys’24. In this summary, we introduce the motivation and perspective of the workshop, review its history, and discuss the most critical issues that deserve attention for future research directions. © 2024 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Krauss, C.
AU  - Bassbouss, L.
AU  - Upravitelev, M.
AU  - An, T.-S.
AU  - Altun, D.
AU  - Reray, L.
AU  - Balitzki, E.
AU  - El Tamimi, T.
AU  - Karagülle, M.
AU  - Sottilare, R. A.
AU  - Schwarz, J.
T1  - Opportunities and Challenges in Developing Educational AI-Assistants for the Metaverse
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Technical University Berlin, Advanced Web Technologies, Straße des 17. Juni 135, Berlin, 10623, Germany
Y1  - 2024
VL  - 14727 LNCS
SP  - 219–238
EP  - 219–238
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196214138&doi=10.1007%2f978-3-031-60609-0_16&partnerID=40&md5=9a66876cb30e9e5d287a86e6cfa66e05
M3  - https://doi.org/10.1007/978-3-031-60609-0_16
KW  - AI-Assistants
KW  - Education
KW  - Interoperability
KW  - Large Language Models
KW  - Learning Technologies
KW  - LLM
KW  - Metaverse
KW  - Virtual Assistants
KW  - Virtual Reality
KW  - 3D modeling
KW  - Computational linguistics
KW  - Computer aided instruction
KW  - E-learning
KW  - Learning systems
KW  - Natural language processing systems
KW  - User interfaces
KW  - Virtual reality
KW  - AI-assistant
KW  - Concept-based
KW  - Language model
KW  - Large language model
KW  - Learning environments
KW  - Learning technology
KW  - Metaverses
KW  - Proof of concept
KW  - Virtual assistants
U1  - 03029743 (ISSN); 978-303160608-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 5; Correspondence Address: C. Krauss; Fraunhofer Institute for Open Communication Systems FOKUS, Berlin, Kaiserin-Augusta-Allee 31, 10589, Germany; email: christopher.krauss@fokus.fraunhofer.de; Conference name: 6th International Conference on Adaptive Instructional Systems, AIS 2024, held as part of the 26th HCI International Conference, HCII 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 313089
N2  - The paper explores the opportunities and challenges for metaverse learning environments with AI-Assistants based on Large Language Models. A proof of concept based on popular but proprietary technologies is presented that enables a natural language exchange between the user and an AI-based medical expert in a highly immersive environment based on the Unreal Engine. The answers generated by ChatGPT are not only played back lip-synchronously, but also visualized in the VR environment using a 3D model of a skeleton. Usability and user experience play a particularly important role in the development of the highly immersive AI-Assistant. The proof of concept serves to illustrate the opportunities and challenges that lie in the merging of large language models, metaverse applications and educational ecosystems, which are self-contained research areas. Development strategies, tools and interoperability standards will be presented to facilitate future developments in this triangle of tension. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
ER  -
TY  - JOUR
AU  - Kessel, A.-L.
AU  - Sahri, S.
AU  - Groppe, S.
AU  - Groppe, J.
AU  - Khorashadizadeh, H.
AU  - Pignal, M.
AU  - Perez Pimparé, E.
AU  - Vignes-Lebbe, R.
T1  - Impact of Chatbots on User Experience and Data Quality on Citizen Science Platforms
JO  - Computers
Y1  - 2025
VL  - 14
IS  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216250466&doi=10.3390%2fcomputers14010021&partnerID=40&md5=6b8304e292c518199fc379ff02d2e8d3
M3  - https://doi.org/10.3390/computers14010021
KW  - chatbot
KW  - citizen science
KW  - data quality
KW  - Large Language Model (LLM)
KW  - LLM application
KW  - user interface
U1  - 2073431X (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: A.-L. Kessel; Institut für Informationssysteme, Universität zu Lübeck, Lübeck, Ratzeburger Allee 160, 23562, Germany; email: akashaleonie.kessel@student.uni-luebeck.de; S. Groppe; Institut für Informationssysteme, Universität zu Lübeck, Lübeck, Ratzeburger Allee 160, 23562, Germany; email: sven.groppe@uni-luebeck.de
N2  - Citizen science (CS) projects, which engage the general public in scientific research, often face challenges in ensuring high-quality data collection and maintaining user engagement. Recent advancements in Large Language Models (LLMs) present a promising solution by providing automated, real-time assistance to users, reducing the need for extensive human intervention, and offering instant support. The CS project Les Herbonautes, dedicated to mass digitization of the French National Herbarium, serves as a case study for this paper, which details the development and evaluation of a network of open source LLM agents to assist users during data collection. The research involved the review of related work, stakeholder meetings with the Muséum National d’Histoire Naturelle, and user and context analyses to formalize system requirements. With these, a prototype with a user interface in the form of a chatbot was designed and implemented using LangGraph, and afterward evaluated through expert evaluation to assess its effect on usability and user experience (UX). The findings indicate that such a chatbot can enhance UX and improve data quality by guiding users and providing immediate feedback. However, limitations due to the non-deterministic nature of LLMs exist, suggesting that workflows must be carefully designed to mitigate potential errors and ensure reliable performance. © 2025 by the authors.
ER  -
TY  - CONF
AU  - Kailash Varma, N. M.
AU  - Aryan, A.
AU  - Dhanush, P.
AU  - Manikanta, R.
AU  - Chandhu, N.
AU  - Arora, G. D.
T1  - Developing an AI-Based Library Assistant: Enhancing Book Retrieval with Natural Language Processing and Machine Learning
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Department of Artificial Intelligence & Machine Learning, Vardhaman College of Engineering, Telangana, Hyderabad, 501218, India
Y1  - 2025
SP  - 136–140
EP  - 136–140
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002677633&doi=10.1109%2fCICTN64563.2025.10932399&partnerID=40&md5=7131a308b2665dcf9ff9954e577e65cf
M3  - https://doi.org/10.1109/CICTN64563.2025.10932399
KW  - Al-based librarian
KW  - Large Language Model (LLM)
KW  - library automation
KW  - library management system
KW  - natural language processing
KW  - personalized recommendations
KW  - student identification
KW  - Information management
KW  - Records management
KW  - Search engines
KW  - Students
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Library automation
KW  - Library management
KW  - Library management system
KW  - Management systems
KW  - Natural language processing
KW  - Natural languages
KW  - Personalized recommendation
KW  - Student identification
KW  - Efficiency
U1  - 979-833153038-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: N.M. Kailash Varma; Department of Artificial Intelligence & Machine Learning, Vardhaman College of Engineering, Hyderabad, Telangana, 501218, India; email: kailashvarma2k2@gmail.com; Conference name: 2nd International Conference on Computational Intelligence, Communication Technology and Networking, CICTN 2025; Conference date: 6 February 2025 through 7 February 2025; Conference code: 207865
N2  - The current college library system is hindered by inefficiencies in book searching, availability updates, identity verification, and checkout processes, resulting in delays and errors that negatively impact student experiences. This research paper presents an Al-based librarian system designed to address these challenges through automation and digital transformation. Leveraging a Large Language Model (LLM), the proposed system facilitates personalized interactions between students and library resources. Upon student verification, the AI librarian provides real-time information on book availability, location, and tailored recommendations, significantly reducing the time spent on manual searches. The streamlined checkout process allows for automated book issuance and instant confirmation notifications, minimizing human error and enhancing record- keeping. This innovative solution not only improves operational efficiency but also enriches the user experience by offering a user-friendly interface and timely assistance. Future enhancements, such as voice integration and mobile application support, are suggested to further modernize library services. This research underscores the potential of AI technologies to revolutionize library management and improve service delivery in academic settings. © 2025 IEEE.
ER  -
TY  - CONF
AU  - Hu, Y.
AU  - Jin, H.
AU  - Wang, X.
AU  - Gu, J.
AU  - Guo, S.
AU  - Chen, C.
AU  - Wang, X.
AU  - Zhou, Y.
T1  - AutoConsis: Automatic GUI-driven Data Inconsistency Detection of Mobile Apps
PB  - Association for Computing Machinery
AD  - School of Computer Science, Fudan University, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China
Y1  - 2024
SP  - 137–146
EP  - 137–146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195312035&doi=10.1145%2f3639477.3639748&partnerID=40&md5=601c04f3b87ed32c4b241a3a1ef067c4
M3  - https://doi.org/10.1145/3639477.3639748
KW  - automatic testing
KW  - functional bug
KW  - in-context learning
KW  - mobile apps
KW  - Automatic testing
KW  - Data mining
KW  - Deep learning
KW  - Natural language processing systems
KW  - Context learning
KW  - Data inconsistencies
KW  - Functional bug
KW  - In contexts
KW  - In-context learning
KW  - Inconsistency detection
KW  - Industrial practices
KW  - Industrial testing
KW  - Mobile app
KW  - Users' experiences
KW  - Graphical user interfaces
U1  - 979-840070500-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-SEIP 2024; Conference date: 14 April 2024 through 20 April 2024; Conference code: 199907
N2  - In industrial practice, many bugs in commercial mobile apps manifest as self-conflicts of data presented in the GUI (Graphical User Interface). Such data inconsistency bugs can bring confusion to the users and deteriorate user experiences. They are a major target of industrial testing practice. However, due to the complication and diversity of GUI implementation and data presentation (e.g., the ways to present the data in natural language), detecting data inconsistency bugs is a very challenging task. It still largely relies on manual efforts. To reduce such human efforts, we proposed AutoConsis, an automated data inconsistency testing tool we designed for Meituan. one of the largest E-commerce providers with over 600 million transacting users. AutoConsis can automatically analyze GUI pages via a multi-modal deep-learning model and extract target data from textual phrases leveraging LLMs (Large Language Models). With these extracted data, their inconsistencies can then be detected. We evaluate the design of AutoConsis via a set of ablation experiments. Moreover, we demonstrate the effectiveness of AutoConsis when applying it to real-world commercial mobile apps with eight representative cases. Copyright © 2024 held by the owner/author(s).
ER  -
TY  - JOUR
AU  - Florindi, F.
AU  - Fedele, P.
AU  - Dimitri, G. M.
T1  - A novel solution for the development of a sentimental analysis chatbot integrating ChatGPT
JO  - Personal and Ubiquitous Computing
Y1  - 2024
VL  - 28
IS  - 6
SP  - 947–960
EP  - 947–960
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197273605&doi=10.1007%2fs00779-024-01824-6&partnerID=40&md5=28636645f019297a8ea72ecc7003bab5
M3  - https://doi.org/10.1007/s00779-024-01824-6
KW  - Chatbots
KW  - Emotion recognition
KW  - Implementation
KW  - Emotion Recognition
KW  - Optimization
KW  - User interfaces
KW  - Business contexts
KW  - Emotional response
KW  - Novel solutions
KW  - Process optimisation
KW  - Real-time updates
KW  - Sentiment analysis
KW  - Users' experiences
KW  - Human computer interaction
U1  - 16174909 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: G.M. Dimitri; Dipartimento di Ingegneria dell’Informazione e Scienze Matematiche, University of Siena, Siena, Via Roma 56, 53100, Italy; email: giovanna.dimitri@unisi.it
N2  - In today’s business landscape, Chatbots play a pivotal role in innovation and process optimization. In this paper, we introduced a novel advanced Emotional Chatbot AI, introducing sentiment analysis for human chatbot conversations. Adding an emotional component within the human-computer interaction, can in fact dramatically improve the quality of the final conversation between Chatbots and humans. More specifically, in our paper, we provided a practical evaluation of the EmoROBERTA software, introducing it into a novel implementation of an Emotional Chatbot. The pipeline we present is novel, and we developed it within a business context in which the use of sentimental and emotional responses can act in a significant and fundamental way toward the final success and use of the Chatbot itself. The architecture enriches user experience with real-time updates on the topic of interest, maintaining a user-centric design, toward an affective-response enhancement of the interaction established between the Chatbot and the user. The source code is fully available on GitHub: https://github.com/filippoflorindi/F-One. © The Author(s) 2024.
ER  -
TY  - CONF
AU  - Jiang, Y.
AU  - Zhang, C.
AU  - He, S.
AU  - Yang, Z.
AU  - Ma, M.
AU  - Qin, S.
AU  - Kang, Y.
AU  - Dang, Y.
AU  - Rajmohan, S.
AU  - Lin, Q.
AU  - Zhang, D.
T1  - XPERT: Empowering Incident Management with Query Recommendations via Large Language Models
PB  - IEEE Computer Society
AD  - Peking University, China
Y1  - 2024
SP  - 1121–1133
EP  - 1121–1133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196785986&doi=10.1145%2f3597503.3639081&partnerID=40&md5=92fbb9462eefbde5ee641e637459173b
M3  - https://doi.org/10.1145/3597503.3639081
KW  - Incident Management
KW  - Large Language Model
KW  - Query Generation
KW  - Computational linguistics
KW  - Digital subscriber lines
KW  - Quality control
KW  - Recommender systems
KW  - User interfaces
KW  - Cloud systems
KW  - Domains specific languages
KW  - Empirical studies
KW  - IT infrastructures
KW  - Language model
KW  - Large language model
KW  - Large-scales
KW  - Query generation
KW  - Query recommendations
KW  - Problem oriented languages
U1  - 02705257 (ISSN); 979-840070217-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 2; Correspondence Address: ; ; ; Conference name: 44th ACM/IEEE International Conference on Software Engineering, ICSE 2024; Conference date: 14 April 2024 through 20 April 2024; Conference code: 200261; CODEN: PCSED
N2  - Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at MICROSOFT. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management. Building upon these valuable insights, we introduce XPERT, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, XPERT generates customized KQL queries tailored to new incidents. Furthermore, XPERT incorporates a novel performance metric called XCORE, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of XPERT, demonstrating its effectiveness in offline settings. Notably, we deploy XPERT in the real production environment of a large-scale incident management system in MICROSOFT, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and XPERT stands as a pioneering DSL query recommendation framework designed for incident management. © 2024 ACM.
ER  -
TY  - CONF
AU  - Anastasiou, E.
AU  - Gkouvas, K.
AU  - Papavasileiou, A.
AU  - Michalos, G.
AU  - Makris, S.
AU  - Thiede, S.
AU  - Damgrave, R.
AU  - Vanekar, T.
AU  - Lutters, E.
T1  - A User-Centric AI-Enhanced Approach to AR frameworks in Manufacturing
PB  - Elsevier B.V.
AD  - Laboratory for Manufacturing Systems and Automation, Department of Mechanical Engineering and Aeronautics, University of Patras, Patras, 26504, Greece
Y1  - 2025
VL  - 134
SP  - 1071–1076
EP  - 1071–1076
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009409219&doi=10.1016%2fj.procir.2025.02.235&partnerID=40&md5=0d7f91d81627a03ca1deafa786489ccc
M3  - https://doi.org/10.1016/j.procir.2025.02.235
KW  - Artificial Intelligence
KW  - Augmented Reality
KW  - Large Language Models
KW  - Manufacturing Systems
KW  - User-Centered
KW  - Artificial intelligence
KW  - Human computer interaction
KW  - Man machine systems
KW  - Smart manufacturing
KW  - User centered design
KW  - User interfaces
KW  - Augmented reality applications
KW  - Augmented reality technology
KW  - Best practices
KW  - Human machine interaction
KW  - Human machine interaction system
KW  - Language model
KW  - Large language model
KW  - Smart interface
KW  - User-centred
KW  - User-centric
KW  - Augmented reality
U1  - 22128271 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: E. Anastasiou; Laboratory for Manufacturing Systems and Automation, Department of Mechanical Engineering and Aeronautics, University of Patras, Patras, 26504, Greece; email: makris@lms.mech.upatras.gr; Conference name: 58th CIRP Conference on Manufacturing Systems, CMS 2025; Conference date: 13 April 2025 through 16 April 2025; Conference code: 209616
N2  - Augmented Reality (AR) technology has been proven a valuable tool for establishing seamless human-machine interaction systems in manufacturing. Considering the constraints and best practices of past implementations, this paper provides a set of guidelines for designing user-centric AR applications, focusing on intuitive human-machine interaction. The proposed methodology includes smart interfaces, intelligent interaction techniques, and AI-based large language models (LLMs) for natural language communication. Additionally, AI-driven vision systems are proposed for identifying user actions and providing feedback automatically to the machine system. The guidelines apply to all AR frameworks, aiming to reduce operational effort while enhancing user experience. © 2025 Elsevier B.V.. All rights reserved.
ER  -
TY  - CONF
AU  - Rajendran, V.
AU  - Besiahgari, D.
AU  - Patil, S. C.
AU  - Chandrashekaraiah, M.
AU  - Challagulla, V.
T1  - A Multi-Agent LLM Environment for Software Design and Refactoring: A Conceptual Framework
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Egen.ai, Naperville, United States
Y1  - 2025
SP  - 488–493
EP  - 488–493
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004581815&doi=10.1109%2fSoutheastCon56624.2025.10971563&partnerID=40&md5=78b116398c34c8c74d3d2841f6b2cad3
M3  - https://doi.org/10.1109/SoutheastCon56624.2025.10971563
KW  - Agent specialization
KW  - Auction mechanisms
KW  - Code quality
KW  - Consensus protocols
KW  - Large Language Models
KW  - Multi-agent systems
KW  - Software refactoring
KW  - Application programs
KW  - Autonomous agents
KW  - Computer aided software engineering
KW  - Computer operating systems
KW  - Computer software maintenance
KW  - Computer software selection and evaluation
KW  - Formal methods
KW  - Intelligent agents
KW  - Search engines
KW  - Software packages
KW  - Software quality
KW  - Software reliability
KW  - Auctions mechanisms
KW  - Language model
KW  - Large language model
KW  - Multi agent
KW  - Multiagent systems (MASs)
KW  - Specialisation
KW  - Software design
U1  - 10910050 (ISSN); 979-833150484-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2025 IEEE SoutheastCon, SoutheastCon 2025; Conference date: 22 March 2025 through 30 March 2025; Conference code: 208545; CODEN: CPISD
N2  - Modern software systems demand continuous evolution to maintain performance, scalability, and security. Traditional single-agent AI-driven code refactoring approaches are often limited in addressing the multi-faceted constraints (e.g., performance, security, maintainability) that emerge during complex software design tasks. In this paper, we propose a novel Multi-Agent Large Language Model (LLM) Environment for automated software design and refactoring. Our conceptual framework comprises specialized LLM 'experts,' each trained or fine-tuned on a different aspect of software engineering (performance optimization, security hardening, UI/UX, maintainability). These agents collaborate in a cooperative or competitive fashion-using coordination protocols akin to consensus or auction mechanisms-to synthesize design insights and refactoring recommendations. We present formal definitions of agent interactions (including mathematical notation for termination conditions), a sequence diagram demonstrating agent collaboration, a complexity analysis of the coordination mechanism, and an expanded reference list. Preliminary experimental design is outlined to demonstrate how multi-agent interactions may resolve conflicting design goals more effectively than a single-agent approach. Our aim is to provide a roadmap for integrating multi-agent LLMs into the software development lifecycle, thereby improving development efficiency, reducing technical debt, and enhancing software quality. © 2025 IEEE.
ER  -
TY  - CONF
AU  - Chimuco, F. T.
AU  - Sequeiros, J. B. F.
AU  - Freire, M. M.
AU  - Inácio, P. R. M.
AU  - Praça, I.
AU  - Bernardi, S.
AU  - P.R.M., Inácio
T1  - Framework and Roadmap for Secure Design and Development of Applications in the Cloud and Mobile Ecosystem
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Instituto Superior de Ciências de Educação da Huíla, Huíla, Lubango, Angola
Y1  - 2025
VL  - 2500 CCIS
SP  - 182–200
EP  - 182–200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009211425&doi=10.1007%2f978-3-031-94855-8_12&partnerID=40&md5=1929c079c7109b84a5a572959f5e0870
M3  - https://doi.org/10.1007/978-3-031-94855-8_12
KW  - Cloud Computing
KW  - Mobile Computing
KW  - secD4CloudMobile
KW  - Security
KW  - Security Attack Models
KW  - Security Best Practice Guidelines
KW  - Security Framework
KW  - Security Mechanisms
KW  - Security Requirement Elicitation
KW  - Security Tests Specification and Tools
KW  - Software Implementation
KW  - Application programs
KW  - Big data
KW  - Cloud security
KW  - Cryptography
KW  - Internet of things
KW  - Mobile applications
KW  - Mobile cloud computing
KW  - Mobile security
KW  - Network security
KW  - Requirements engineering
KW  - Security systems
KW  - Software testing
KW  - Attack modeling
KW  - Best practice guidelines
KW  - Cloud-computing
KW  - Mobile-computing
KW  - Requirements elicitation
KW  - Secd4cloudmobile
KW  - Security attack model
KW  - Security attacks
KW  - Security best practice guideline
KW  - Security Best Practices
KW  - Security frameworks
KW  - Security mechanism
KW  - Security requirement elicitation
KW  - Security requirements
KW  - Security test specification and tool
KW  - Security tests
KW  - Software implementation
KW  - Test specifications
KW  - Test tools
KW  - Mobile computing
U1  - 18650929 (ISSN); 978-303194854-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: F.T. Chimuco; Universidade da Beira Interior, Covilhã, Portugal; email: franciso.chimuco@ubi.pt; Conference name: 9th European Interdisciplinary Cybersecurity Conference, EICC 2025; Conference date: 18 June 2025 through 19 June 2025; Conference code: 333789
N2  - Mobile applications and devices became widely popular in the second decade of the 21st century, mostly driven by technologies like cloud computing, Internet of Things (IoT), blockchain, and big data, leading to and fostering very fast paced development cycles of cloud-based mobile apps, which offer enhanced convenience and accessibility for end users. However, significant security challenges remain, as cybersecurity is often overlooked in favor of a shorter time-to-market. This paper presents the Security by Design for the Cloud and Mobile Ecosystem framework, consisting of a set of tools and underlying logic aimed at helping mobile application developers build secure applications in the specific mobile and cloud ecosystem. It features a modular architecture and includes five main tools built in Python, and its outputs are enhanced through the use of ChatGPT. The paper also discusses real-world applications and presents usability evaluation results, showing the framework is effective and meets its goals. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - CONF
AU  - Cannavo, A.
AU  - Visconti, A.
AU  - Lamberti, F.
T1  - Exploring the Advantages and Challenges of Combining ChatGPT with Blender
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - Politecnico di Torino, Italy
Y1  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004788276&doi=10.1109%2fICIR64558.2024.10976975&partnerID=40&md5=c95190cefcb29a15446ae1d530bbd1d0
M3  - https://doi.org/10.1109/ICIR64558.2024.10976975
KW  - 3D modeling
KW  - Blender
KW  - ChatGPT
KW  - natural language processing
KW  - scripting
KW  - Animation
KW  - 3D graphics
KW  - 3D models
KW  - 3d-modeling
KW  - Autodesk mayas
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Scripting
U1  - 979-833153442-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: A. Cannavo; Politecnico di Torino, Italy; email: alberto.cannavo@polito.it; Conference name: 3rd IEEE International Conference on Intelligent Reality, ICIR 2024; Conference date: 4 December 2024 through 6 December 2024; Conference code: 208635
N2  - Traditional graphic suites such as Blender and Autodesk Maya are widely used for 3D graphics creation, but their steep learning curve poses challenges for novice users. This complexity can make content creation time-consuming and mentally demanding. To address these issues, there is a growing interest in innovative approaches that integrate Artificial Intelligence (AI) tools into the traditional workflow. These tools promise to enhance user experience and facilitate tasks such as 3D modeling, automation, texturing, lighting setup, animation, and storytelling, This paper investigates the feasibility and potential strengths and weaknesses of integrating OpenAI's ChatGPT 3.5 with Blender, i.e., a prominent 3D modeling software. By combining advanced natural language processing (NLP) capabilities with the robust features of Blender, this work aims to enhance user interaction and facilitate the 3D design workflow. The paper proposes use cases that could benefit from the combination of these two technologies and warns about some limitations of this approach. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Stütz, J.-D.
AU  - Karras, O.
AU  - Oelen, A.
AU  - Auer, S.
AU  - Comuzzi, M.
AU  - Grigori, D.
AU  - Sellami, M.
AU  - Zhou, Z.
T1  - A User-Driven Hybrid Neuro-Symbolic Approach for Knowledge Graph Creation from Relational Data
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Leibniz Information Centre for Science and Technology (TIB), Hanover, Germany
Y1  - 2025
VL  - 15506 LNCS
SP  - 169–185
EP  - 169–185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218937379&doi=10.1007%2f978-3-031-81375-7_10&partnerID=40&md5=419c0009ed51993e17e4503421c493ee
M3  - https://doi.org/10.1007/978-3-031-81375-7_10
KW  - HCI
KW  - Knowledge graph creation
KW  - Neuro-symbolic
KW  - Graphical user interfaces
KW  - Knowledge graph
KW  - User profile
KW  - Complex methods
KW  - Error prones
KW  - Knowledge graphs
KW  - Ontology's
KW  - Relational data
KW  - Semantic technologies
KW  - Technical skills
KW  - User driven
KW  - Mapping
U1  - 03029743 (ISSN); 978-303181374-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J.-D. Stütz; Robert Bosch GmbH, Gerlingen, Germany; email: jan-david.stuetz@de.bosch.com; Conference name: 30th International Conference on Cooperative Information Systems, CoopIS 2024; Conference date: 19 November 2024 through 21 November 2024; Conference code: 327039
N2  - In all kinds of organizations, relational data is prevalent and ubiquitous in a plethora of systems. However, the integration and exchange of such data is cumbersome, time-consuming, and error-prone. Semantic technologies, such as ontologies, KGs, and linked data, were developed to facilitate this but require comprehensive technical skills and complex methods for mapping relational data to semantic formalisms. Naturally, this process lacks speed, scalability, and automation. This work presents a novel user-driven neuro-symbolic approach to transform relational data into KGs. In our approach, users are supported by neural models (in particular Large Language Models) and symbolic formalisms (ontologies and mappings) to automate various mapping tasks and thus speed up and scale up the transformation from relational to linked data. We implemented our approach in a comprehensive intelligent assistant dubbed LXS. Our experimental evaluation, conducted primarily with participants from the Robert Bosch GmbH, demonstrates enhanced mapping quality compared to manual creation, a competitive application, and AI-only generations. Additionally, it significantly reduces user interaction time by nearly half, independent of the user’s experience level. Also, qualitatively, users appreciated the attractiveness and novelty of the user interface. Furthermore, the neuro-symbolic approach of LXS contributes to a more trustworthy human-AI interaction since it keeps users in the loop and provides transparency in the transformation process. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - CONF
AU  - Bong, J. H.
AU  - Cai, C.
AU  - Toh, S. Y.
AU  - Liu, S.
AU  - Fan, X.
AU  - Sottilare, R. A.
AU  - Schwarz, J.
T1  - Interactive Sensemaking with SurveySense: Enhancing Survey Insights Through Human-AI Collaboration on an LLM-Based Platform
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Lee Kong Chian School of Medicine, Nanyang Technological University, Singapore, Singapore
Y1  - 2025
VL  - 15813 LNCS
SP  - 153–170
EP  - 153–170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007853443&doi=10.1007%2f978-3-031-92970-0_11&partnerID=40&md5=8a5d41879b0f960ccc1cc1c0113568a4
M3  - https://doi.org/10.1007/978-3-031-92970-0_11
KW  - Automated Survey Analysis
KW  - Human-AI Collaboration
KW  - Usability Evaluation
KW  - Ambient intelligence
KW  - Stockpile surveys
KW  - Analysis process
KW  - Automated survey analyze
KW  - Decisions makings
KW  - Human-AI collaboration
KW  - Informed decision
KW  - Sense making
KW  - Software-tools
KW  - Survey analysis
KW  - Usability evaluation
KW  - Users' satisfactions
KW  - Usability engineering
U1  - 03029743 (ISSN); 978-303192969-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J.H. Bong; College of Computing and Data Science, Singapore, Singapore; email: jbong006@e.ntu.edu.sg; Conference name: 7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332889
N2  - Surveys are essential in various fields to gather feedback and information. Efficient data processing is thus crucial for quick, informed decision-making. As the volume of data continues to grow, automation in survey analysis has become increasingly important. This paper proposes a new software tool, SurveySense, which aims to streamline the survey analysis process. A study involving 26 participants was also conducted to evaluate the tool across six dimensions - Insight Quality, Efficiency, Consistency, User satisfaction and Trustworthiness, Human-AI Collaboration and Usability and Interaction. Results showed that SurveySense would be a helpful assistant for survey analysis but human inputs remain crucial in the process to further refine the AI-generated insights to align with personal goals. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - CONF
AU  - Lange, E. M.
AU  - Cajander, Å.
AU  - Normark, M.
AU  - Degen, H.
AU  - Ntoa, S.
T1  - Exploring Flow in IT Professionals’ Use of AI-Integrated Tools: Insights from Interviews
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Department of Information Technology, Uppsala University, Uppsala, Sweden
Y1  - 2025
VL  - 15822 LNAI
SP  - 44–58
EP  - 44–58
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008211937&doi=10.1007%2f978-3-031-93429-2_3&partnerID=40&md5=23f2d5dcecc6b17f3dd8c5d17b8e9a97
M3  - https://doi.org/10.1007/978-3-031-93429-2_3
KW  - Artificial intelligence
KW  - HCI design and evaluation methods
KW  - Human-computer interaction
KW  - Information systems applications
KW  - Natural language interfaces
KW  - User experience
KW  - Feedback
KW  - Human computer interaction
KW  - Information systems
KW  - Information use
KW  - User interfaces
KW  - Computer interaction
KW  - Design and evaluation methods
KW  - Design recommendations
KW  - Flow experience
KW  - HCI design
KW  - HCI design and evaluation method
KW  - Information systems application
KW  - IT professional
KW  - Users' experiences
U1  - 03029743 (ISSN); 978-303193428-5 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: Å. Cajander; Department of Information Technology, Uppsala University, Uppsala, Sweden; email: asa.cajander@it.uu.se; Conference name: 6th International Conference on Artificial Intelligence in HCI, AI-HCI 2025, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332909
N2  - This study explores how AI-enabled work tools influence the Flow experiences of IT professionals, focusing on their potential to enhance or hinder states of deep focus and immersion. Drawing on interviews with 12 IT professionals, the research examines the interplay between AI tools, task characteristics, and Flow preconditions such as goal clarity, feedback, and the challenge-skill balance. The study follows ethical research design recommendations, ensuring participant privacy and informed consent. Findings reveal that AI tools generally play a supportive role, aiding productivity and creativity without consistently inducing Flow states. Their impact depends heavily on task complexity and user goals, with tools like GitHub Copilot and ChatGPT enhancing Flow-related conditions such as feedback and exploratory possibilities. However, limitations such as inaccuracies and poor adaptability to complex tasks occasionally disrupt Flow experiences. The study offers design recommendations to better align AI tools with Flow principles, including enhancing goal clarity, dynamic adaptability, and support for iterative exploration. These insights contribute to the growing discourse on AI’s role in fostering psychological engagement and optimal experiences in professional settings. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - CONF
AU  - Kretzer, F.
AU  - Kolthoff, K.
AU  - Bartelt, C.
AU  - Ponzetto, S. P.
AU  - Maedche, A.
T1  - Closing the Loop between User Stories and GUI Prototypes: An LLM-Based Assistant for Cross-Functional Integration in Software Development
PB  - Association for Computing Machinery
AD  - Human-centered Systems Lab (H-lab), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
Y1  - 2025
IS  - 879
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005760101&doi=10.1145%2f3706598.3713932&partnerID=40&md5=361b9039f774e41212071b20a937e60f
M3  - https://doi.org/10.1145/3706598.3713932
KW  - Assistance
KW  - GUI Prototypes
KW  - Requirements
KW  - User Stories
KW  - Computer operating systems
KW  - Computer software selection and evaluation
KW  - Open source software
KW  - Requirements engineering
KW  - Software packages
KW  - Software prototyping
KW  - Cross functional integration
KW  - Graphical user interface prototype
KW  - Language model
KW  - Model-based OPC
KW  - Plug-ins
KW  - Requirement
KW  - User interface components
KW  - User interface prototypes
KW  - User stories
KW  - Software design
U1  - 979-840071394-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025; Conference date: 26 April 2025 through 1 May 2025; Conference code: 208607
N2  - Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements' completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes. © 2025 Copyright held by the owner/author(s).
ER  -
TY  - JOUR
AU  - Li, J.
AU  - Mynatt, E. D.
AU  - Mishra, V.
AU  - Bell, J.
T1  - 'Always Nice and Confident, Sometimes Wrong': Developer's Experiences Engaging Generative AI Chatbots Versus Human-Powered Q&A Platforms
JO  - Proceedings of the ACM on Human-Computer Interaction
Y1  - 2025
VL  - 9
IS  - CSCW029
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004257534&doi=10.1145%2f3710927&partnerID=40&md5=d7236870adb24280656aa39755e0ae23
M3  - https://doi.org/10.1145/3710927
KW  - chatgpt
KW  - data mining
KW  - developer support
KW  - generative ai
KW  - human-ai collaboration
KW  - large language model
KW  - programming assistance
KW  - Q&A platform
KW  - reddit
KW  - social media
KW  - stack overflow
KW  - thematic analysis
KW  - user experience
KW  - Computer software selection and evaluation
KW  - Data flow analysis
KW  - Human engineering
KW  - Software prototyping
KW  - Software reliability
KW  - Text mining
KW  - Chatgpt
KW  - Developer support
KW  - Generative ai
KW  - Human-ai collaboration
KW  - Language model
KW  - Large language model
KW  - Programming assistance
KW  - Reddit
KW  - Social media
KW  - Stack overflow
KW  - Thematic analysis
KW  - Users' experiences
KW  - Software design
U1  - 25730142 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0
N2  - Software engineers have historically relied on human-powered Q&A platforms like Stack Overflow (SO) as coding aids. With the rise of generative AI, developers have started to adopt AI chatbots, such as ChatGPT, in their software development process. Recognizing the potential parallels between human-powered Q&A platforms and AI-powered question-based chatbots, we investigate and compare how developers integrate this assistance into their real-world coding experiences by conducting a thematic analysis of 1700+ Reddit posts. Through a comparative study of SO and ChatGPT, we identified each platform's strengths, use cases, and barriers. Our findings suggest that ChatGPT offers fast, clear, comprehensive responses and fosters a more respectful environment than SO. However, concerns about ChatGPT's reliability stem from its overly confident tone and the absence of validation mechanisms like SO's voting system. Based on these findings, we synthesized the design implications for future GenAI code assistants and recommend a workflow leveraging each platform's unique features to improve developer experiences. © 2025 Owner/Author.
ER  -
TY  - CONF
AU  - Ashish Tarun, R.
AU  - Priyadarshini, B.
AU  - Sneha, M.
AU  - Akila, K.
AU  - Sivakumar, P. D.
AU  - Pasupathi, C.
AU  - Balakrishnan, P.
AU  - Ramachandran, R.
T1  - Leveraging LangChain Framework and Large Language Models for Conversational Chatbot Development
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Department of Computer Science and Engineering, SRM Institute of Science and Technology, Vadapalani Campus, Chennai, India
Y1  - 2025
VL  - 2362
SP  - 244–255
EP  - 244–255
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219189838&doi=10.1007%2f978-3-031-82386-2_19&partnerID=40&md5=9937d6f30863826142fb77ad82bc0b1b
M3  - https://doi.org/10.1007/978-3-031-82386-2_19
KW  - Chatbot system
KW  - Conversational AI
KW  - LangChain framework
KW  - Large Language Models(LLMs)
KW  - OpenAI’s GPT-3.5 Turbo
KW  - AI Technologies
KW  - Chatbots
KW  - Educational settings
KW  - Human machine interaction
KW  - Langchain framework
KW  - Language model
KW  - Large language model
KW  - Openai’s GPT-3.5 turbo
U1  - 18650929 (ISSN); 978-303182385-5 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: K. Akila; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, Vadapalani Campus, India; email: akilak@srmist.edu.in; Conference name: 1st International Research Conference on Computing Technologies for Sustainable Development, IRCCTSD 2024; Conference date: 9 May 2024 through 10 May 2024; Conference code: 327059
N2  - Conversational AI technologies have transformed human- machine interactions in educational settings, such as the SRM Institute of Science and Technology, where intelligent chatbots play pivotal roles in supporting students, faculty, and staff. This research presents a sophisticated chatbot system tailored for the SRM Institute, leveraging cutting-edge technologies like the LangChain framework and Large Language Models (LLMs), including OpenAI's GPT-3.5 Turbo, to enhance language understanding and generation capabilities. The methodology emphasizes meticulous data preparation, including data collection, preprocessing, and embedding creation, integrated with vector databases to establish a robust knowledge base. The chatbot's user interface, built with Flask, ensures an intuitive user experience with a visually appealing and responsive layout. Evaluation results demonstrate a 95% accuracy rate and high reliability in handling diverse user inquiries, highlighting the chatbot's capacity to manage growing volumes of queries while maintaining consistent accuracy. These outcomes underscore significant improvements in user engagement and satisfaction, marking a substantial advancement in educational chatbot technology. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
ER  -
TY  - JOUR
AU  - Siau, K. L.
AU  - F.F.-H., Nah
T1  - 12th International Conference on HCI in Business, Government and Organizations, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15804 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007166117&partnerID=40&md5=2746445e0bb2f9cc917cdcd2fed6cbad
U1  - 03029743 (ISSN); 978-303192822-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 12th International Conference on HCI in Business, Government and Organizations, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332659
N2  - The proceedings contain 41 papers. The special focus in this conference is on HCI in Business, Government and Organizations. The topics include: Apology or Gratitude: The Impact of Verbal Recovery Strategies of AI-Powered Virtual Tourism Assistants on Tourists’ Post-Recovery Satisfaction; enhancing the Omnichannel Retailing Customer Experience in the Pre-purchase Phase: Evaluation and Improvement of a Digital Grocery Brochure; investigating Users’ Responses to Blurred Boundary Advertisements in Short Video Websites; RoBuddy - An Innovative Research Project on AI in Office Environments; a Method of Assembly Guidance Information Delivery in Augmented Reality Considering Users’ Proficiency Levels; Once More with (the Right) Feeling: How Historical Fiction Writing Processes of Character Design, Plot Outline, and Context Checking Are Affected by Co-Writing with ChatGPT; Using CNN Models to Predict the Future Trends of Listed Stocks on the Taiwan Stock Exchange; Integrating AI-Driven Personas and Procedural Visualization for Complete Communities Design and Urban Planning for Large-Scale Urban Development; Generative AI and Changing Work: Systematic Review of Practitioner-Led Work Transformations Through the Lens of Job Crafting; Follow My Logic: Generative AI Workflows in Designing for Serious Table-Top Games; AI Integration in ERP Systems: Optimizing Knowledge Management and Business Process Re-engineering for Strategic Outcomes; rethink the Way of Conducting Research: It is Time to Change to the Artificial Intelligence Era for Reliable Outcomes; augmenting Student Startups’ Customer Validation Efforts Through Adaptive Coaching Using Large Language Models; skilled Labor Shortage and Artificial Intelligence: Challenges and Opportunities for the Regional Labor Market; LLM-Assisted Collaborative Change Specification of Industrial Control Software; ChatGPT and Financial Investing: The Advantages, the Disadvantages, and the Perils.
ER  -
TY  - CONF
AU  - Ilagan, J. B.
AU  - Ilagan, J. R.
AU  - Zulueta, P. Y.
AU  - Rodrigo, M. M.
AU  - Degen, H.
AU  - Ntoa, S.
T1  - Optimizing Conversational Commerce Involving Multilingual Consumers Through Large Language Models’ Natural Language Understanding Abilities
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Ateneo de Manila University, Quezon City, Philippines
Y1  - 2024
VL  - 14736 LNAI
SP  - 47–59
EP  - 47–59
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195474442&doi=10.1007%2f978-3-031-60615-1_4&partnerID=40&md5=07118c28a512264b485c935771f73a04
M3  - https://doi.org/10.1007/978-3-031-60615-1_4
KW  - Co-pilots
KW  - conversational commerce
KW  - Conversational user experience
KW  - Generative AI
KW  - Large language models
KW  - Named entity recognition
KW  - Natural language understanding
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Sales
KW  - Social networking (online)
KW  - User interfaces
KW  - Co-pilot
KW  - Conversational commerce
KW  - Language model
KW  - Large language model
KW  - Natural languages
KW  - Users' experiences
KW  - Commerce
U1  - 03029743 (ISSN); 978-303160614-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J.B. Ilagan; Ateneo de Manila University, Quezon City, Philippines; email: jbilagan@ateneo.edu; Conference name: 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th International Conference on Human-Computer Interaction, HCI International 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 312569
N2  - Due to the emergence of natural language processing (NLP) interfaces, there has been growing intent to use conversational channels for commerce. Beyond customer service, NLP-enabled AI agents are being integrated into various steps of the order-to-cash (OTC) process. Social media and messaging platforms such as Facebook Messenger have become pivotal for businesses, especially during and after the COVID-19 pandemic, but adoption has been limited. In addition, attitudes towards fully-automated conversational agents (CA) have been mixed, and there is room for human involvement in transactional conversations. A distinguishing contribution of this research is leveraging the inherent capabilities of Large Language Models (LLMs) in handling multilingual conversations and extracting transactional details through named entity recognition (NER). The study describes a hybrid human-AI setup augmenting agents with an auto-agent leveraging LLMs’ natural language understanding (NLU) capabilities, designed using the OTC process pattern applied to conversational UX frameworks. A prototype of the setup aims to streamline operations and reduce errors by enhancing the user experience during key OTC steps through improved conversational design. Recognizing the irreplaceable essence of human interaction, the hybrid human-in-the-loop approach was chosen, mitigating the impersonal nature of full automation. A prototype handling customers and humans augmented by LLMs for NER handling of transaction, customer, and product information was built. Sample synthetic bilingual conversations between customers and sales agents were generated using ChatGPT and fed into the system for evaluation. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
ER  -
TY  - JOUR
AU  - Kurosu, M.
AU  - Hashizume, A.
T1  - Thematic Area Human Computer Interaction, HCI 2024, Held as Part of the 26th HCI International Conference, HCII 2024
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Y1  - 2024
VL  - 14684 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195844991&partnerID=40&md5=51c0fedf8fd1499d7f3fd37e8700e113
U1  - 03029743 (ISSN); 978-303160404-1 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: Thematic Area Human Computer Interaction, HCI 2024, Held as Part of the 26th HCI International Conference, HCII 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 312899
N2  - The proceedings contain 117 papers. The special focus in this conference is on Thematic Area Human Computer Interaction. The topics include: Unified UI Design System for Industrial HMI Software Development; evaluation of a Voice-Based Emotion Recognition Software in the Psycho-Oncological Care of Cancer Patients; does the Voice-Based Lifelogging Method “Laughter Map” of Visualizing a User’s Laughter Experiences Enhance Positive Mood?; design and User Acceptance of Dynamic User Interface Adaptations Based on Situation-Awareness and Emotion-Recognition; automatically Identifying the Human Sense of Familiarity Using Eye Gaze Features; of Politics, Behavior and Commands: Processing Information Unspoken for Sentiment Analysis and Spoken Interaction Applications; evolution of Executive Education in Interactive Digital Design Field: A Case Study Analysis; beyond Future Skills: Developing Company Personas in Disruptive Transformation Processes; Generating Specifications from Requirements Documents for Smart Devices Using Large Language Models (LLMs); Modeling Theory of Mind in Multimodal HCI; designing Artificial Serendipity; Pictorial Usability Metric for User Experience LITE: Development and Psychological Measurement; a Study of the Impact of Different Teaching Methods on Students’ Learning in Design Thinking Courses in Taiwan; Integrated DBR and ADDIE Model to Improve Pedagogical Practices in Mechatronic Design; the Trends and Research Progress of Mental Models in Interaction Design: A Bibliometric Study; enhancing Episodic Memory Recall Through Nostalgic Image Generation and Interactive Modification; a Bibliometric Analysis of Eye Tracking in User Experience Research; Through the Waves: An Auto-ethnographic Perspective on HCI Design and Research; foreword; preface; A Transformer Based Emotion Recognition Model for Social Robots Using Topographical Maps Generated from EEG Signals; spontaneous Theory of Mind for Artificial Intelligence.
ER  -
TY  - CONF
AU  - Juarez-Ramirez, R.
AU  - y Fernandez C.A., Fernandez
AU  - Jimenez Calleros, S. P.
AU  - Ramirez-Noriega, A.
AU  - Guerra-Garcia, C. A.
AU  - Sandoval, G. L.
AU  - Menendez-Ortiz, M. A.
AU  - Hernandez-Ocharan, J. O.
T1  - Proceedings - 2024 12th International Conference in Software Engineering Research and Innovation, CONISOFT 2024
PB  - Institute of Electrical and Electronics Engineers Inc.
Y1  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216741409&partnerID=40&md5=2596d8d369279626fe1befd8c8c33347
U1  - 979-833153211-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 12th International Conference in Software Engineering Research and Innovation, CONISOFT 2024; Conference date: 28 October 2024 through 1 November 2024; Conference code: 205321
N2  - The proceedings contain 42 papers. The topics discussed include: systematic literature review of low-code and its future trends; a novel framework for ai-integrated electronic manifest generation; agile user-centered development of mobile applications: a systematic literature review; digital assistant to improve user experience on websites: innovative support and efficient communication strategies; cluster analysis in the identification of patterns in software development with agile methodologies: a systematic literature review; price and quote index prediction with deep learning; agile user-centered development of mobile applications: a systematic literature review; and transforming software development: a study on the integration of multi-agent systems and large language models for automatic code generation.
ER  -
TY  - JOUR
AU  - Xiang, B.
AU  - Shao, Y.
T1  - SumLLaMA: Efficient Contrastive Representations and Fine-Tuned Adapters for Bug Report Summarization
JO  - IEEE Access
Y1  - 2024
VL  - 12
SP  - 78562–78571
EP  - 78562–78571
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192955028&doi=10.1109%2fACCESS.2024.3397326&partnerID=40&md5=f6f63d204268411f9f230faa2f168454
M3  - https://doi.org/10.1109/ACCESS.2024.3397326
KW  - Bug report summarization
KW  - contrastive representation
KW  - efficient fine-tuning
KW  - software maintenance
KW  - Computer software maintenance
KW  - Computer software selection and evaluation
KW  - Job analysis
KW  - Quality control
KW  - Bug reports
KW  - Code
KW  - Computer bugs
KW  - Contrastive representation
KW  - Efficient fine-tuning
KW  - Fine tuning
KW  - Language model
KW  - Self-supervised learning
KW  - Task analysis
KW  - Semantics
U1  - 21693536 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: Y. Shao; Zhejiang College Of Security Technology, Zhejiang, Wenzhou, 325000, China; email: shaoyunna2024@163.com
N2  - In software maintenance, concise summaries of bug reports are crucial, significantly enhancing developer efficiency and ultimately improving software quality and user experience. Large language models (LLMs) have become the standard method for bug report summarization due to their powerful representation capabilities. However, LLM-based approaches face two primary challenges: accurately modeling the contextual relationships between various components within a bug report and the risk of overfitting when fine-tuning LLMs on datasets of limited size. To address these challenges, we propose a novel approach, SumLLaMA, which leverages contrastive learning pre-training and parameter-efficient fine-tuning. Contrastive learning pre-training is employed to construct contextual relations between components in a single bug report, enabling SumLLaMA to learn sequence-level representations. For parameter-efficient fine-tuning, we fine-tune a smaller adapter instead of the entire LLM, reducing the number of parameters trained to about 1/1500 of the original model, effectively mitigating the risk of overfitting. To evaluate the effectiveness of SumLLaMA, we compare it against five baseline models, including a state-of-the-art model, on a publicly available dataset. The experimental results show that SumLLaMA outperforms all baselines by up to 26.66, 17.10, and 24.01 points in ROUGE-1, ROUGE-2, and ROUGE-L metrics, respectively, achieving a state-of-the-art result for automated bug report summarization. © 2013 IEEE.
ER  -
TY  - JOUR
AU  - Kruchten, P.
AU  - Gregory, P.
T1  - workshops presented at 23rd International Conferences on Agile Software Development, XP 2022 and 24th International Conferences on Agile Software Development, XP 2023
JO  - Lecture Notes in Business Information Processing
Y1  - 2024
VL  - 489 LNBIP
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182005065&partnerID=40&md5=d01a03e73129169357f5c2e826e1cdea
U1  - 18651348 (ISSN); 978-303148549-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: workshops presented at 23rd International Conferences on Agile Software Development, XP 2022 and 24th International Conferences on Agile Software Development, XP 2023; Conference date: 13 June 2022 through 16 June 2022; Conference code: 306449
N2  - The proceedings contain 22 papers. The special focus in this conference is on Agile Software Development. The topics include: Towards a X-as-a-Service Application in Industrial Laundry - A Case Study of Information Requirement Engineering in Emerging Data Ecosystems; software Startup Ecosystem in Namibia; industry Expectations for Product Ops Professionals: A Review of Job Advertisements; unveiling the Spectrum of Hybrid Work in Software Engineering: Research Directions; defining a Remote Work Policy: Aligning Actions and Intentions; business Development in Large-Scale Agile Software Development: Barriers and Enablers; ChatGPT as a Tool for User Story Quality Evaluation: Trustworthy Out of the Box?; Survey of AI Tool Usage in Programming Course: Early Observations; Turning Large Language Models into AI Assistants for Startups Using Prompt Patterns; enhancing Agile Software Development Sustainability Through the Integration of User Experience and Gamification; ChatGPT as a Fullstack Web Developer - Early Results; reviewing Crypto-Agility and Quantum Resistance in the Light of Agile Practices; empirical Investigation of Quantum Computing on Solving Complex Problems; Sustainable IT in an Agile DevOps Setup Leads to a Shift Left in Sustainability Engineering; improving the Implementation of Microservice-Based Systems with Static Code Analysis; towards an Architecture-Centric Methodology for Migrating to Microservices; being Agile in a Data Science Project; the Future of Work: Agile in a Hybrid World; organizational Debt in Large-Scale Hybrid Agile Software Development: A Case Study on Coordination Mechanisms; the Know-How of Agile Retrospectives in Software Startups.
ER  -
TY  - CONF
AU  - Joshi, I.
AU  - Shahid, S.
AU  - Venneti, S. M.
AU  - Vasu, M.
AU  - Zheng, Y.
AU  - Li, Y.
AU  - Krishnamurthy, B.
AU  - Chan, G. Y.-Y.
T1  - CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering
PB  - Association for Computing Machinery
AD  - Adobe Research, San Jose, CA, United States
Y1  - 2025
SP  - 341–365
EP  - 341–365
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001919599&doi=10.1145%2f3708359.3712102&partnerID=40&md5=67365ab7c58c465450c5ee36728082ac
M3  - https://doi.org/10.1145/3708359.3712102
KW  - HCI
KW  - LLM Evaluation
KW  - Prompt Optimization
KW  - User interfaces
KW  - Application development
KW  - Industry professionals
KW  - Language model
KW  - Large language model' evaluation
KW  - Model evaluation
KW  - Model response
KW  - Optimisations
KW  - Prompt optimization
KW  - Trial and error
KW  - User-centric evaluations
KW  - Requirements engineering
U1  - 979-840071306-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: I. Joshi; Adobe, Noida, India; email: ishika19310@iiitd.ac.in; S. Shahid; Adobe, Noida, Uttar Pradesh, India; email: simra.sshahid@gmail.com; Conference name: 30th International Conference on Intelligent User Interfaces, IUI 2025; Conference date: 24 March 2025 through 27 March 2025; Conference code: 207770
N2  - Ensuring large language models' (LLMs) responses align with prompt instructions is crucial for application development. Based on our formative study with industry professionals, the alignment requires heavy human involvement and tedious trial-and-error especially when there are many instructions in the prompt. To address these challenges, we introduce CoPrompter, a framework that identifies misalignment based on assessing multiple LLM responses with criteria. It proposes a method to generate evaluation criteria questions derived directly from prompt requirements and an interface to turn these questions into a user-editable checklist. Our user study with industry prompt engineers shows that CoPrompter improves the ability to identify and refine instruction alignment with prompt requirements over traditional methods, helps them understand where and how frequently models fail to follow user's prompt requirements, and helps in clarifying their own requirements, giving them greater control over the response evaluation process. We also present the design lessons to underscore our system's potential to streamline the prompt engineering process. © 2025 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Zhao, G.
AU  - Zhang, X.
AU  - Lu, C.
AU  - Zhao, H.
AU  - Wu, T.
AU  - Wang, P.
AU  - Xu, J.
AU  - Zheng, B.
T1  - Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning
PB  - Association for Computing Machinery, Inc
AD  - Taobao & Tmall Group of Alibaba, Beijing, China
Y1  - 2025
SP  - 631–640
EP  - 631–640
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009216090&doi=10.1145%2f3701716.3715222&partnerID=40&md5=0411b5b443e5fe1864a2ddcd44bf01fb
M3  - https://doi.org/10.1145/3701716.3715222
KW  - E-Commerce
KW  - Knowledge Distillation
KW  - Large Language Model
KW  - Semantic Matching
KW  - Distillation
KW  - E-learning
KW  - Learning systems
KW  - Search engines
KW  - Semantics
KW  - User experience
KW  - User interfaces
KW  - E- commerces
KW  - Knowledge distillation
KW  - Language model
KW  - Large language model
KW  - Model-driven
KW  - Multi dimensional
KW  - Performance
KW  - Relevance learning
KW  - Relevance models
KW  - Semantic matching
KW  - Electronic commerce
U1  - 979-840071331-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: B. Zheng; Taobao & Tmall Group of Alibaba, Beijing, China; email: bozheng@alibaba-inc.com; Conference name: 34th ACM Web Conference, WWW Companion 2025; Conference date: 28 April 2025 through 2 May 2025; Conference code: 209280
N2  - Effective query-item relevance modeling is pivotal for enhancing user experience and safeguarding user satisfaction in e-commerce search systems. Recently, benefiting from the vast inherent knowledge, Large Language Model (LLM) approach demonstrates strong performance and long-tail generalization ability compared with previous neural-based specialized relevance learning methods. Though promising, current LLM-based methods encounter the following inadequacies in practice: First, the massive parameters and computational demands make it difficult to be deployed online. Second, distilling LLM models to online models is a feasible direction, but the LLM relevance modeling is a black box, and its rich intrinsic knowledge is difficult to extract and apply online. To improve the interpretability of LLM and boost the performance of online relevance models via LLM, we propose an Explainable LLM-driven Multi-dimensional Distillation framework for e-commerce relevance learning, which comprises two core components: (1) An Explainable LLM for relevance modeling (ELLM-rele), which decomposes the relevance learning into intermediate steps and models relevance learning as a Chain-of-Thought (CoT) reasoning, thereby enhancing both interpretability and performance of LLM. (2) A Multi-dimensional Knowledge Distillation (MKD) architecture that transfers the knowledge of ELLM-rele to current deployable interaction-based and representation-based student models from both the relevance score distribution and CoT reasoning aspects. Through distilling the probabilistic and CoT reasoning knowledge, MKD improves both the semantic interaction and long-tail generalization abilities of student models. Extensive offline evaluations and online experiments on Taobao search ad scene demonstrate that our proposed framework significantly enhances e-commerce relevance learning performance and user experience. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ER  -
TY  - CONF
AU  - Lukianova, E.
AU  - Jeong, J.-Y.
AU  - Jeong, J.-W.
T1  - A picture is worth a thousand words? Investigating the Impact of Image Aids in AR on Memory Recall for Everyday Tasks
PB  - Association for Computing Machinery
AD  - Department of Data Science, Seoul National University of Science and Technology, Seoul, South Korea
Y1  - 2025
SP  - 106–126
EP  - 106–126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001919087&doi=10.1145%2f3708359.3712087&partnerID=40&md5=42711ba09f611ea542ee921a81a8ba4f
M3  - https://doi.org/10.1145/3708359.3712087
KW  - Cognitive Offloading
KW  - Memory Augmentation
KW  - Visualization in AR
KW  - Human form models
KW  - Memory management
KW  - Systems analysis
KW  - Augmentation systems
KW  - Cognitive offloading
KW  - Human-computer interaction researches
KW  - Image texts
KW  - Language model
KW  - Memory aids
KW  - Memory augmentation
KW  - Multi-modal
KW  - User interface designs
KW  - Visualization in augmented reality
KW  - Augmented reality
U1  - 979-840071306-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J.-W. Jeong; Department of Data Science, Seoul National University of Science and Technology, Seoul, South Korea; email: jinw.jeong@seoultech.ac.kr; Conference name: 30th International Conference on Intelligent User Interfaces, IUI 2025; Conference date: 24 March 2025 through 27 March 2025; Conference code: 207770
N2  - Memory augmentation has long been a central field in Human-Computer Interaction (HCI) research. Recently, emerging multimodal large language models (MLLMs) have extended research on memory augmentation by enabling the retrieval of information stored in multiple formats (e.g., text and image) through free-form queries. However, literature has focused on text-based memory aids, there has been surprisingly limited research on image-based assistance, despite humans' superior efficiency in processing visual information. Therefore, in this work, we explore the effect of image aids on memory augmentation. To this end, we first design and implement an augmented reality (AR) memory augmentation system, informed by human evaluation of MLLM performance (GPT-4o, LLaVA, and Mini-Gemini) and insights from user interface (UI) design workshops. As a result, we found that GPT-4o is most suitable for our system, images complemented with text (i.e., Image+text) are the most preferred format of memory aids. We also identified optimal UI design parameters for AR-based memory augmentation. With a finalized version of the system prototype, we conduct a user study (N=20) consisting of two tasks that simulate real-life memory-related challenges. We found that Image+text significantly enhanced both recall performance and memory vividness. Additionally, from a user experience perspective, Image+text was considered the most helpful and easiest to use for memory augmentation. Our findings showed that images are a powerful modality for enhancing memory recall, extending beyond traditional text-based approaches. We expect that insights gained from this work will contribute to the development of practical, everyday memory augmentation systems. © 2025 Copyright held by the owner/author(s).
ER  -
TY  - CONF
AU  - Saffron Dionysius, T.
AU  - Rakesh, G.
AU  - Jasmine Mystica, K.
T1  - AI Circuit Builder: Bridging Language & Logic
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - St. Joseph's College of Engineering, Chennai, India
Y1  - 2025
SP  - 506–512
EP  - 506–512
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004415170&doi=10.1109%2fICVADV63329.2025.10960965&partnerID=40&md5=1b2b21620e3bb537f2102573780f53eb
M3  - https://doi.org/10.1109/ICVADV63329.2025.10960965
KW  - AI-Assisted Design
KW  - Artificial intelligence
KW  - Circuit Generation
KW  - Eclipse Layout KernelJS
KW  - GPT-3.5
KW  - Natural Language Processing
KW  - NodeJS
KW  - ReactJS
KW  - Web Application
KW  - Electronic design automation
KW  - Logic circuits
KW  - Report generators
KW  - Timing circuits
KW  - AI-assisted design
KW  - Circuit generation
KW  - Eclipse layout kerneljs
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Nodejs
KW  - Reactjs
KW  - WEB application
KW  - Web applications
KW  - Printed circuit design
U1  - 979-833152139-4 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 2025 International Conference on Visual Analytics and Data Visualization, ICVADV 2025; Conference date: 4 March 2025 through 6 March 2025; Conference code: 208382
N2  - Circuit design is one of the initial stages of any electronic & electrical projects. It has been a hard part of building a product since, it can be easily solved only by professional developers. In order to solve this part, a tool is build that combines both the core knowledge of electronics & artificial intelligence. AI circuit builder is a tool that can be used by people who are beginners and don't really have a particular knowledge in a specific part of the circuit. The user can easily prompt the LLM in the form of natural language. The tool then generates a digital format for the desired output. The output is processed with a routing algorithm in order to sort out the routes. With the help of various modern technologies for the web the tool provides a seamless user experience overall. © 2025 IEEE.
ER  -
TY  - JOUR
AU  - Siau, K. L.
AU  - F.F.-H., Nah
T1  - 12th International Conference on HCI in Business, Government and Organizations, held as part of the 27th HCI International Conference, HCII 2025
JO  - Lecture Notes in Computer Science
Y1  - 2025
VL  - 15805 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007143427&partnerID=40&md5=de4ce4c850bdc6d0606868cf982ed7f1
U1  - 03029743 (ISSN); 978-303192825-3 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 12th International Conference on HCI in Business, Government and Organizations, held as part of the 27th HCI International Conference, HCII 2025; Conference date: 22 June 2025 through 27 June 2025; Conference code: 332659
N2  - The proceedings contain 41 papers. The special focus in this conference is on HCI in Business, Government and Organizations. The topics include: Apology or Gratitude: The Impact of Verbal Recovery Strategies of AI-Powered Virtual Tourism Assistants on Tourists’ Post-Recovery Satisfaction; enhancing the Omnichannel Retailing Customer Experience in the Pre-purchase Phase: Evaluation and Improvement of a Digital Grocery Brochure; investigating Users’ Responses to Blurred Boundary Advertisements in Short Video Websites; RoBuddy - An Innovative Research Project on AI in Office Environments; a Method of Assembly Guidance Information Delivery in Augmented Reality Considering Users’ Proficiency Levels; Once More with (the Right) Feeling: How Historical Fiction Writing Processes of Character Design, Plot Outline, and Context Checking Are Affected by Co-Writing with ChatGPT; Using CNN Models to Predict the Future Trends of Listed Stocks on the Taiwan Stock Exchange; Integrating AI-Driven Personas and Procedural Visualization for Complete Communities Design and Urban Planning for Large-Scale Urban Development; Generative AI and Changing Work: Systematic Review of Practitioner-Led Work Transformations Through the Lens of Job Crafting; Follow My Logic: Generative AI Workflows in Designing for Serious Table-Top Games; AI Integration in ERP Systems: Optimizing Knowledge Management and Business Process Re-engineering for Strategic Outcomes; rethink the Way of Conducting Research: It is Time to Change to the Artificial Intelligence Era for Reliable Outcomes; augmenting Student Startups’ Customer Validation Efforts Through Adaptive Coaching Using Large Language Models; skilled Labor Shortage and Artificial Intelligence: Challenges and Opportunities for the Regional Labor Market; LLM-Assisted Collaborative Change Specification of Industrial Control Software; ChatGPT and Financial Investing: The Advantages, the Disadvantages, and the Perils.
ER  -
TY  - JOUR
AU  - Rangaswamy, N.
AU  - Sim, G. R.
AU  - Borah, P. P.
T1  - 15th Indian Conference on Human-Computer Interaction Design and Research, IndiaHCI 2024
JO  - Communications in Computer and Information Science
Y1  - 2025
VL  - 2337 CCIS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219213492&partnerID=40&md5=c15be417b2721d7d2cba312c1c56575e
U1  - 18650929 (ISSN); 978-303180828-9 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 15th Indian Conference on Human-Computer Interaction Design and Research, IndiaHCI 2024; Conference date: 7 November 2024 through 9 November 2024; Conference code: 327009
N2  - The proceedings contain 41 papers. The special focus in this conference is on Human-Computer Interaction Design and Research. The topics include: Fogg Behavioural Model Based Cybersecurity Awareness Framework: An Empirical Analysis; could You Hear That? Identifying Marathi Phrases Suitable for Aural Transcription Tasks; IDCText: An Application for Conducting Text Input Research Studies in Indian Languages; comparative Evaluation of Speech Interfaces of Conversational Agents in Hindi; exploring the Impact of Foot-Based Haptic Feedback on User Experience in Virtual Reality Navigation; investigating Contextual Factors in Technology-Based Solutions Designed to Support Health and Fitness Routines for Older Adults: A Systematic Review; Is ChatGPT Ready for Indian-Language Speakers? Findings From a Preliminary Mixed Methods Study; using Graph Analysis for Evaluating Usability of Software-Based Keyboard for Password Creation; Spatial Audio Training for Visually Impaired Users Navigation in VR: An Analytical Approach; culturally Relevant Novel Interaction Methods for Immersive Video Streaming Experience in Virtual Reality; allerGuard: An Innovative mHealth Solution for Food Allergy Management in India; visual Feedback Interface for Audio Communication Over Lossy and High Delay Networks.
ER  -
TY  - CONF
AU  - de Wit, J.
AU  - Følstad, A.
AU  - Araujo, T.
AU  - Papadopoulos, S.
AU  - E.L.-C., Law
AU  - Luger, E.
AU  - Goodwin, M.
AU  - Hobert, S.
AU  - Brandtzaeg, P. B.
T1  - Leveraging Large Language Models as Simulated Users for Initial, Low-Cost Evaluations of Designed Conversations
PB  - Springer Science and Business Media Deutschland GmbH
AD  - Department of Communication and Cognition, Tilburg University, Warandelaan 2, Tilburg, 5037 AB, Netherlands
Y1  - 2024
VL  - 14524 LNCS
SP  - 77–93
EP  - 77–93
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190376004&doi=10.1007%2f978-3-031-54975-5_5&partnerID=40&md5=9a918ab3cb430c0852d139d6bf988431
M3  - https://doi.org/10.1007/978-3-031-54975-5_5
KW  - Automatic evaluation
KW  - Conversational agents
KW  - Large language models
KW  - Computational linguistics
KW  - User interfaces
KW  - Background knowledge
KW  - Cost evaluations
KW  - Language model
KW  - Large language model
KW  - Low cost methods
KW  - Low-costs
KW  - Performance
KW  - Rule based
KW  - Costs
U1  - 03029743 (ISSN); 978-303154974-8 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: J. de Wit; Department of Communication and Cognition, Tilburg University, Tilburg, Warandelaan 2, 5037 AB, Netherlands; email: j.m.s.dewit@tilburguniversity.edu; Conference name: 7th International Workshop on Chatbot Research and Design, CONVERSATIONS 2023; Conference date: 22 November 2023 through 23 November 2023; Conference code: 310239
N2  - In this paper, we explore the use of large language models, in this case the ChatGPT API, as simulated users to evaluate designed, rule-based conversations. This type of evaluation can be introduced as a low-cost method to identify common usability issues prior to testing conversational agents with actual users. Preliminary findings show that ChatGPT is good at playing the part of a user, providing realistic testing scenarios for designed conversations even if these involve certain background knowledge or context. GPT-4 shows vast improvements over ChatGPT (3.5). In future work, it is important to evaluate the performance of simulated users in a more structured, generalizable manner, for example by comparing their behavior to that of actual users. In addition, ways to fine-tune the LLM could be explored to improve its performance, and the output of simulated conversations could be analyzed to automatically derive usability metrics such as the number of turns needed to reach the goal. Finally, the use of simulated users with open-ended conversational agents could be explored, where the LLM may also be able to reflect on the user experience of the conversation. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
ER  -
TY  - JOUR
AU  - Degen, H.
AU  - Ntoa, S.
T1  - 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th HCI International Conference, HCII 2024
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Y1  - 2024
VL  - 14735 LNAI
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196310333&partnerID=40&md5=edee6b85077e31bd4407f6a6ce93897c
U1  - 03029743 (ISSN); 978-303160613-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 5th International Conference on Artificial Intelligence in HCI, AI-HCI 2024, held as part of the 26th HCI International Conference, HCII 2024; Conference date: 29 June 2024 through 4 July 2024; Conference code: 313149
N2  - The proceedings contain 90 papers. The special focus in this conference is on Artificial Intelligence in HCI. The topics include: What Makes People Say Thanks to AI; PyFlowML: A Visual Language Framework to Foster Participation in ML-Based Decision Making; A Three-Year Analysis of Human Preferences in Delegating Tasks to AI; qualitative User-Centered Requirements Analysis for a Recommender System for a Project Portfolio Platform in Higher Education Institutions; foreword; surveying Computational Theory of Mind and a Potential Multi-agent Approach; What Is the Focus of XAI in UI Design? Prioritizing UI Design Principles for Enhancing XAI User Experience; Evaluation of Generative AI-Assisted Software Design and Engineering: A User-Centered Approach; time Series Representation Learning: A Survey on Deep Learning Techniques for Time Series Forecasting; uncertainty of Information Applied to Network Monitoring Metrics; semi-supervised Sorting via Deep Feature Extraction and Density Based Clustering with User Feedback; towards a Framework for Interdisciplinary Studies in Explainable Artificial Intelligence; exploring the Impact of Explainability on Trust and Acceptance of Conversational Agents - A Wizard of Oz Study; WisCompanion: Integrating the Socratic Method with ChatGPT-Based AI for Enhanced Explainability in Emotional Support for Older Adults; how to Explain It to System Testers?; A Multidisciplinary Heuristic Evaluation of AI-Enhanced Web Tools: Insights and Implications for Legal Contract Management Systems; Evaluating the Effectiveness of the Peer Data Labelling System (PDLS); preface; reducing Human Annotation Effort Using Self-supervised Learning for Image Segmentation; Enhancing Historical Understanding in School Students: Designing a VR Application with AI-Animated Characters; Examining User Perceptions to Vocal Interaction with AI Bots in Virtual Reality and Mobile Environments: A Focus on Foreign Language Learning and Communication Dynamics; Human-Aligned GAI Driven by Conceptual Knowledge: System, Framework, and Co-creation; iterative Visual Interaction with Latent Diffusion Models; evidential Representation Proposal for Predicate Classification Output Logits in Scene Graph Generation.
ER  -
TY  - CONF
AU  - Cuadra, A.
AU  - Wang, M.
AU  - Stein, L. A.
AU  - Jung, M. F.
AU  - Dell, N.
AU  - Estrin, D.
AU  - Landay, J. A.
T1  - The Illusion of Empathy? Notes on Displays of Emotion in Human-Computer Interaction
PB  - Association for Computing Machinery
AD  - Cornell Tech, New York, NY, United States
Y1  - 2024
IS  - 446
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189492830&doi=10.1145%2f3613904.3642336&partnerID=40&md5=f8e16dc144ba7f57423565d7148ab39b
M3  - https://doi.org/10.1145/3613904.3642336
KW  - Affective Computing
KW  - AI
KW  - Automation
KW  - Autonomous Agents
KW  - Chatbots
KW  - Conversational Agents
KW  - Conversational User Interfaces
KW  - Disability
KW  - Emotion
KW  - Empathy
KW  - Ethics
KW  - Gender
KW  - Health
KW  - Human-AI Interaction
KW  - Human-Computer Interaction
KW  - Identity
KW  - LLMs
KW  - Marginalization
KW  - Mental Health
KW  - Natural Language Processing
KW  - Personalization
KW  - Power and Privilege
KW  - Religion
KW  - Social Robots
KW  - Technological Harm
KW  - Ubiquitous Computing
KW  - User Experience Design
KW  - Values in Design
KW  - Voice Assistants
KW  - Wellbeing
KW  - Human computer interaction
KW  - Human robot interaction
KW  - Machine design
KW  - Natural language processing systems
KW  - Ubiquitous computing
KW  - User interfaces
KW  - Conversational agents
KW  - Conversational user interface
KW  - Human-AI interaction
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Mental health
KW  - Natural language processing
KW  - Natural languages
KW  - Personalizations
KW  - Power
KW  - Power and privilege
KW  - Social robots
KW  - Technological harm
KW  - User experience design
KW  - Value in designs
KW  - Voice assistant
KW  - Autonomous agents
U1  - 979-840070330-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 19; Conference name: 2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024; Conference date: 11 May 2024 through 16 May 2024; Conference code: 199441
N2  - From ELIZA to Alexa, Conversational Agents (CAs) have been deliberately designed to elicit or project empathy. Although empathy can help technology better serve human needs, it can also be deceptive and potentially exploitative. In this work, we characterize empathy in interactions with CAs, highlighting the importance of distinguishing evocations of empathy between two humans from ones between a human and a CA. To this end, we systematically prompt CAs backed by large language models (LLMs) to display empathy while conversing with, or about, 65 distinct human identities, and also compare how different LLMs display or model empathy. We find that CAs make value judgments about certain identities, and can be encouraging of identities related to harmful ideologies (e.g., Nazism and xenophobia). Moreover, a computational approach to understanding empathy reveals that despite their ability to display empathy, CAs do poorly when interpreting and exploring a user's experience, contrasting with their human counterparts. © 2024 Copyright held by the owner/author(s)
ER  -
TY  - CONF
AU  - Lambert, S.
AU  - Mathews, C.
AU  - Jaddoa, A.
AU  - Bohemia, E.
AU  - Bohemia, E.
AU  - Buck, L.
AU  - Grierson, H.
AU  - Kim, J.
AU  - Storer, I.
AU  - Whitehead, T.
T1  - CONCEPT TO PRODUCTION WITH A GEN AI DESIGN ASSISTANT: AIDA
PB  - The Design Society
AD  - Canterbury Christ Church University, United Kingdom
Y1  - 2024
SP  - 235–240
EP  - 235–240
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003908119&doi=10.35199%2fepde.2024.40&partnerID=40&md5=81df8bb901d7d2182c2dfeed199c308b
M3  - https://doi.org/10.35199/epde.2024.40
KW  - 3D CAD
KW  - artificial intelligence (AI)
KW  - design automation
KW  - engineering design
KW  - generative design
KW  - human computer interaction (HCI)
KW  - large language models (LLMs)
KW  - machine language (ML)
KW  - Product design
KW  - Computer aided logic design
KW  - Computer software selection and evaluation
KW  - Curricula
KW  - Design for manufacturability
KW  - High level synthesis
KW  - Integrated circuit layout
KW  - Intellectual property core
KW  - Printed circuit design
KW  - Process design
KW  - Software design
KW  - 3-d cads
KW  - Artificial intelligence
KW  - Computer interaction
KW  - Design automations
KW  - Engineering design
KW  - Generative design
KW  - Human computer interaction
KW  - Language model
KW  - Large language model
KW  - Machine language
KW  - Machine languages
KW  - Machine design
U1  - 978-191225420-0 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 26th International Conference on Engineering and Product Design Education, E and PDE 2024; Conference date: 5 September 2024 through 6 September 2024; Conference code: 208271
N2  - In design research there is a deep interest in how designers solve complex problems using design methods and heuristic shortcuts and in particular how this might relate to Machine Language (ML) to simulate the design process. With the introduction of Large Language Model (LLMs) such as Chat GPT we can appreciate how software with the remarkable capability of Generative AI (Gen AI) and generative design can be used to assist designers in the three-dimensional design of their products. In this paper, we will focus on how AI will impact designing in computing, identify what is relevant and suggest a new development opportunity. Our interest is in examining the potential for better and novel software solutions, making them easier to use during the design synthesis process and capable of adjustment throughout the 3D CAD development stage. The specific problem we aim to resolve is how to optimise a designer's time spent from concept to production using Gen AI & 3D CAD software without affecting the quality of design thinking, methodology and practical process. Gen AI as an evolving platform has the potential to create a design to production productivity shift that industry and academic groups have long predicted. Designing will remain creative and inventive, individualistic or team based and using what we have termed an AI design assistant, AIDA. © 2024 Proceedings of the 26th International Conference on Engineering and Product Design Education: Rise of the Machines: Design Education in the Generative AI Era, E and PDE 2024. All rights reserved.
ER  -
TY  - JOUR
AU  - Alonso, O.
AU  - Cousijn, H.
AU  - Silvello, G.
AU  - Marchesin, S.
AU  - Marrero, M.
AU  - Teixeira Lopes, C.
T1  - 27th International Conference on Theory and Practice of Digital Libraries, TPDL 2023
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Y1  - 2023
VL  - 14241 LNCS
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174636639&partnerID=40&md5=a5c04568c258591876e5b540bdd31858
U1  - 03029743 (ISSN); 978-303143848-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Conference name: 27th International Conference on Theory and Practice of Digital Libraries, TPDL 2023; Conference date: 26 September 2023 through 29 September 2023; Conference code: 302529
N2  - The proceedings contain 32 papers. The special focus in this conference is on International Conference on Theory and Practice of Digital Libraries. The topics include: On Retraction Cascade? Citation Intention Analysis as a Quality Control Mechanism in Digital Libraries; using Semi-automatic Annotation Platform to Create Corpus for Argumentative Zoning; CORE-GPT: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering; a Robust Approach for Hybrid Personalized Recommender Systems; readability Measures as Predictors of Understandability and Engagement in Searching to Learn; classification of Visualization Types and Perspectives in Patents; it’s Not Just GitHub: Identifying Data and Software Sources Included in Publications; a Graph Neural Network Approach for Evaluating Correctness of Groups of Duplicates; synthesizing Web Archive Collections into Big Data: Lessons from Mining Data from Web Archives; a Multilingual Dashboard to Analyse Intercultural Knowledge Circulation; a Comparison of Automated Journal Recommender Systems; Making PDFs Accessible for Visually Impaired Users (and Findable for Everybody Else); multi-view Graph-Based Text Representations for Imbalanced Classification; Large Synthetic Data from the ar χ iv for OCR Post Correction of Historic Scientific Articles; from Textual to Visual Image Searching: User Experience of Advanced Image Search Tool; ranking for Learning: Studying Users’ Perceptions of Relevance, Understandability, and Engagement; image Modification Modeled as a Storytelling Process; From ISAD(G) to Linked Data Archival Descriptions; detection of Redacted Text in Legal Documents; the First Tile for the Digital Onomastic Repertoire of the French Medieval Romance: Problems and Perspectives; known by the Company It Keeps: Proximity-Based Indexing for Physical Content in Archival Repositories; fostering Access to Cultural Heritage Knowledge: Iterative Design for the Visit of Historical Monuments; persistent Identifier Usage by Cultural Heritage Institutions: A Study on the Europeana.eu Dataset; aspect-Driven Structuring of Historical Dutch Newspaper Archives.
ER  -
TY  - CONF
AU  - Kuo, J.-Y.
AU  - Xia, Z.
AU  - Cooper, A.
AU  - Koomsap, P.
AU  - Stjepandic, J.
T1  - Exploring Intelligent User Interfaces from Design Students' Perspectives on Smart Home Products Through Peer Assessment, Focus Group and ChatGPT
PB  - IOS Press BV
AD  - School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore
Y1  - 2023
VL  - 41
SP  - 161–170
EP  - 161–170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184295754&doi=10.3233%2fATDE230608&partnerID=40&md5=4bec51ebb4db7457c9d332c6f1c38e18
M3  - https://doi.org/10.3233/ATDE230608
KW  - Automation
KW  - Curricula
KW  - Digital devices
KW  - Education computing
KW  - Product design
KW  - Systems thinking
KW  - User interfaces
KW  - Design Education
KW  - Focus groups
KW  - Home products
KW  - Intelligent technology
KW  - Intelligent User Interfaces
KW  - Peer assessment
KW  - Smart devices
KW  - Smart homes
KW  - Student perspectives
KW  - Users' experiences
KW  - Students
U1  - 2352751X (ISSN); 978-164368440-6 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: J.-Y. Kuo; Department of Industrial Design, National Taipei University of Technology, Taiwan; email: jyk@ntut.edu.tw; Conference name: 30th ISTE International Conference on Transdisciplinary Engineering: Leveraging Transdisciplinary Engineering in a Changing and Connected World, TE 2023; Conference date: 11 July 2023 through 14 July 2023; Conference code: 196765
N2  - In a changing and connected world, people are surrounded by an increasing number of smart devices in a complex system. Intelligent technology has revolutionised the way we interact with these devices, and has resulted in improved user experiences through the integration of physical status and digital applications. However, this transition has also presented new challenges and demands for transdisciplinary adaptation in traditional approaches to design education. Many existing design methods and frameworks have not kept pace with the level of automation now seen in intelligent interactive products, nor have they addressed human-machine interdependence in a system-thinking context. The aim of this study is to gain insights from the younger generation of design students to inform the development of a more suitable design course. Using smart home products as the case scenario, 39 industrial design students evaluated the user experience with the products through hands-on interaction. The individual product reviews of the robot cleaner, smart speaker and smart lightbulb were then analysed and consolidated. Thus, this study contributes to the elucidation of design students' perspectives on intelligent user interfaces. Furthermore, a comparative analysis of user insights was conducted through peer assessments, focus groups and large language models to explore their potential and difference in terms of the design process. Overall, the goal of this analysis is to advance the field of design practice and education. © 2023 The Authors.
ER  -
TY  - JOUR
AU  - Wang, Y.
AU  - Liu, H.
AU  - Gao, S.
AU  - Tang, X.
T1  - Animation2API: API Recommendation for the Implementation of Android UI Animations
JO  - IEEE Transactions on Software Engineering
Y1  - 2023
VL  - 49
IS  - 9
SP  - 4411–4428
EP  - 4411–4428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167811848&doi=10.1109%2fTSE.2023.3294971&partnerID=40&md5=4e171ab994e9f059240aef36b6d06a5e
M3  - https://doi.org/10.1109/TSE.2023.3294971
KW  - API recommendation
KW  - UI animation
KW  - User interface
KW  - Animation
KW  - Application programming interfaces (API)
KW  - Codes (symbols)
KW  - Database systems
KW  - Information retrieval
KW  - Query processing
KW  - Websites
KW  - Code
KW  - Data resources
KW  - Features extraction
KW  - Operating system
KW  - Spatial feature vector
KW  - Usability of mobile applications
KW  - Users' experiences
KW  - Web searches
KW  - User interfaces
U1  - 00985589 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 6; Correspondence Address: H. Liu; Jilin University, College of Computer Science and Technology, Jilin, 130012, China; email: liuhuaxiao@jlu.edu.cn; CODEN: IESED
N2  - UI animations, such as card movement and menu slide in/out, provide appealing user experience and enhance the usability of mobile applications. In the process of UI animation implementation, it is difficult for developers to identify suitable APIs for the animation to be implemented from a large number of APIs. Fortunately, the huge app market contains millions of apps, and they can provide valuable data resources for solving this problem. By summarizing the API usage for the same or similar animations in apps, reusable knowledge can be mined for the API recommendation. In this paper, we propose a novel method Animation2API, which mines the knowledge about APIs from existing apps and recommends APIs for UI animations. Different from existing text-based API recommendation approaches, Animation2API takes the UI animation in GIF/video format as query input. Firstly, we construct a database containing mappings between UI animations and APIs by analyzing a broad set of apps. Then, we build a UI animation feature extractor, which can be used to gain temporal-spatial feature vectors of UI animations. By comparing the temporal-spatial feature vectors between UI animations, we identify animations that are similar to the query animation from the database. Finally, we summarize the APIs used for implementing these animations and recommend a list of APIs for developers. The empirical evaluation results show that our method can achieve 82.66% Success rate and outperform the baseline Guru by 230.77% and 184.95% in terms of Precision and Recall when considering twenty APIs. In the user study, we take the scenarios of using web search and ChatGPT to implement animations as baselines, and the results show that participants can complete animations faster (14.54%) after using Animation2API. Furthermore, participants' positive feedbacks on the questionnaire indicate the usefulness of Animation2API. © 1976-2012 IEEE.
ER  -
TY  - CONF
AU  - Yin, Z.
AU  - Wang, Y.
AU  - Papatheodorou, T.
AU  - Hui, P.
T1  - Text2VRScene: Exploring the Framework of Automated Text-driven Generation System for VR Experience
PB  - Institute of Electrical and Electronics Engineers Inc.
AD  - The Hong Kong University of Science and Technology, Guangzhou, China
Y1  - 2024
SP  - 701–711
EP  - 701–711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191431035&doi=10.1109%2fVR58804.2024.00090&partnerID=40&md5=5484a5bc3939d003efe68308f56b15a6
M3  - https://doi.org/10.1109/VR58804.2024.00090
KW  - Human computer interaction (HCI)
KW  - Human-centered computing
KW  - Interaction paradigms
KW  - Interaction techniques
KW  - Text input
KW  - Virtual reality
KW  - Automation
KW  - Human computer interaction
KW  - Natural language processing systems
KW  - User interfaces
KW  - Automated systems
KW  - Digital contents
KW  - Generation systems
KW  - Generative model
KW  - Interaction paradigm
KW  - Language model
U1  - 979-835037402-5 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 9; Correspondence Address: Y. Wang; The Hong Kong University of Science and Technology, Guangzhou, China; email: yuyangwang@hkust-gz.edu.cn; Conference name: 31st IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2024; Conference date: 16 March 2024 through 21 March 2024; Conference code: 198921
N2  - With the recent development of the Virtual Reality (VR) industry, the increasing number of VR users pushes the demand for the massive production of immersive and expressive VR scenes in related industries. However, creating expressive VR scenes involves the reasonable organization of various digital content to express a coherent and logical theme, which is time-consuming and labor-intensive. In recent years, Large Language Models (LLMs) such as ChatGPT 3.5 and generative models such as stable diffusion have emerged as powerful tools for comprehending natural language and generating digital contents such as text, code, images, and 3D objects. In this paper, we have explored how we can generate VR scenes from text by incorporating LLMs and various generative models into an automated system. To achieve this, we first identify the possible limitations of LLMs for an automated system and propose a systematic framework to mitigate them. Subsequently, we developed Text2VRScene, a VR scene generation system, based on our proposed framework with well-designed prompts. To validate the effectiveness of our proposed framework and the designed prompts, we carry out a series of test cases. The results show that the proposed framework contributes to improving the reliability of the system and the quality of the generated VR scenes. The results also illustrate the promising performance of the Text2VRScene in generating satisfying VR scenes with a clear theme regularized by our well-designed prompts. This paper ends with a discussion about the limitations of the current system and the potential of developing similar generation systems based on our framework. © 2024 IEEE.
ER  -
TY  - CONF
AU  - Huo, S.
AU  - Mukherjee, K.
AU  - Bandlamudi, J.
AU  - Isahagian, V.
AU  - Muthusamy, V.
AU  - Rizk, Y.
AU  - Köpke, J.
AU  - López-Pintado, O.
AU  - Plattfaut, R.
AU  - J.-R., Rehse
AU  - Gdowska, K.
AU  - Gonzalez-Lopez, F.
AU  - Munoz-Gama, J.
AU  - Smit, K.
AU  - van der Werf J.M.E.M.
T1  - Accelerating the Support of Conversational Interfaces for RPAs Through APIs
PB  - Springer Science and Business Media Deutschland GmbH
AD  - IBM Research, Austin, TX, United States
Y1  - 2023
VL  - 491 LNBIP
SP  - 165–180
EP  - 165–180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586436&doi=10.1007%2f978-3-031-43433-4_11&partnerID=40&md5=d158f7da1fd079229b07528bae782071
M3  - https://doi.org/10.1007/978-3-031-43433-4_11
KW  - API Specification
KW  - Conversational Assistant
KW  - Language Model
KW  - Paraphrase Generation
KW  - Robotic Process Automation
KW  - Application programming interfaces (API)
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Public relations
KW  - Robotics
KW  - API specifications
KW  - Business-users
KW  - Chatbots
KW  - Conversational assistant
KW  - Intent recognition
KW  - Language model
KW  - Paraphrase generation
KW  - Process automation
KW  - Recognition models
KW  - Robotic process automation
KW  - Specifications
U1  - 18651348 (ISSN); 978-303143432-7 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: S. Huo; IBM Research, Yorktown Heights, United States; email: siyu.huo@ibm.com; Conference name: Proceedings of the 21st International Conference on Business Process Management, BPM 2023; Conference date: 11 September 2023 through 15 September 2023; Conference code: 300139
N2  - In the business automation world, APIs are everywhere. They provide access to enterprise tools such as customer relationship management solutions, or custom automations such as unattended RPA bots that automate repetitive tasks. Unfortunately, they may not be accessible to the business users who need them but are not equipped with the necessary technical skills to leverage them. Most recently, chatbots are becoming the go-to medium to make automation software accessible to business users. Since API specifications aren’t written with a chatbot use in mind, additional work is needed to make APIs accessible through a natural language interface. Making this process scalable to many APIs requires an automated data training pipeline for intent recognition models, a crucial component within chatbots to understand natural language utterances from users. More accurate intent recognition models lead to better user experience and satisfaction. Prior work proposed approaches to extracting intents from OpenAPI specifications. However, the resulting models tend to be brittle due to weaknesses in training data. In this work, we propose a data augmentation approach based on paraphrasing using large language models and propose a system to generate sentences to train intent recognition models. Experimental results highlight the effectiveness of our approach. Our system is deployed in a real world setting. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
ER  -
TY  - CONF
AU  - Packer, B.
AU  - Keates, S.
AU  - Antona, M.
AU  - Stephanidis, C.
T1  - Designing AI Writing Workflow UX for Reduced Cognitive Loads
PB  - Springer Science and Business Media Deutschland GmbH
AD  - University of Chichester, Bognor Regis, Chichester, PO21 1HR, United Kingdom
Y1  - 2023
VL  - 14021 LNCS
SP  - 306–325
EP  - 306–325
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173016253&doi=10.1007%2f978-3-031-35897-5_23&partnerID=40&md5=86e2f946b9b870e0d31d9ba3166e93b7
M3  - https://doi.org/10.1007/978-3-031-35897-5_23
KW  - accessibility
KW  - AI pedagogy
KW  - artificial intelligence
KW  - cognitive load
KW  - high-order writing
KW  - Human-centered design
KW  - user experience
KW  - Diseases
KW  - Human computer interaction
KW  - Natural language processing systems
KW  - Students
KW  - Taxonomies
KW  - User interfaces
KW  - Accessibility
KW  - Cognitive loads
KW  - High-order
KW  - High-order writing
KW  - Higher-order
KW  - Higher-order thinkings
KW  - Human-centred designs
KW  - Users' experiences
KW  - Work-flows
KW  - Artificial intelligence
U1  - 03029743 (ISSN); 978-303135896-8 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 1; Correspondence Address: B. Packer; University of Chichester, Chichester, Bognor Regis, PO21 1HR, United Kingdom; email: b.packer@chi.ac.uk; Conference name: 17th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2023, held as part of the 25th International Conference on Human-Computer Interaction, HCII 2023; Conference date: 23 July 2023 through 28 July 2023; Conference code: 297839
N2  - This paper explores how Large-Language Model Artificial Intelligences (LLM-AIs) can be used to support people with Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD), and other learning differences which effect cognition and self-regulation. It examines the cognitive load associated with complex writing tasks and how it affects users who have trouble with high-order thinking and planning. OpenAI’s GPT-3 API is used to analyze how AI can help with complex language-based tasks. The paper first reflects on how GPT-3 uses natural language processing to generate text, translate, summarize, answer questions, and caption images, as well as how it adapts to respond to different situations and tasks to accurately classify them. Bloom’s Taxonomy and SOLO Taxonomy are highlighted as language-driven methods of assessing learner understanding and to design activities and assessments that encourage high-order thinking. Literature is reviewed which suggests that students with disorders which effect executive functions need extra help with their writing skills to do well in school, and that early and accessible interventions such as digital self-management tools already help these learners. A model of executive-cognitive capacity is proposed to assess how best to manage the cognition of tasks and workloads, and to support a design matrix for assistive tools and processes. Finally, the Social Cognitive Theory (SCT) model for writing is evaluated for use as a procedural high-order writing process by which the tools can be designed and against which their efficacy can be validated. This review illustrates a universal design method for the development and evaluation of future AI writing tools for all users, with specific consideration towards users with atypical cognitive and sensory processing needs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
ER  -
TY  - CONF
AU  - Turhan, G. D.
AU  - Dokonal, W.
AU  - Hirschberg, U.
AU  - Wurzer, G.
AU  - Wurzer, G.
T1  - Life Cycle Assessment for the Unconventional Construction Materials in Collaboration with a Large Language Model
PB  - Education and research in Computer Aided Architectural Design in Europe
AD  - Izmir University of Economics, Turkey
Y1  - 2023
VL  - 2
SP  - 39–48
EP  - 39–48
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172457872&doi=10.52842%2fconf.ecaade.2023.2.039&partnerID=40&md5=e0d0546b9e2ea52989980b72f979c46c
M3  - https://doi.org/10.52842/conf.ecaade.2023.2.039
KW  - Human-computer Interaction (HCI)
KW  - Large Language Models (LLMs)
KW  - Life Cycle Assessment (LCA)
KW  - Machine Learning (ML)
U1  - 26841843 (ISSN); 978-949120735-8 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 6; Correspondence Address: G.D. Turhan; Izmir University of Economics, Turkey; email: gozde.turhan@ieu.edu.tr; Conference name: 41st Conference on Education and Research in Computer Aided Architectural Design in Europe, eCAADe 2023; Conference date: 20 September 2023 through 22 September 2023; Conference code: 300449
N2  - In this paper, developing an online tool for the Life Cycle Assessment (LCA) of unconventional construction materials in collaboration with Large Language Models (LLMs) is proposed. The LCA provides information on the environmental impact of a product throughout its entire life cycle, from the extraction of raw materials to disposal or recycling. The LLMs are neural network architectures, typically utilizing variants of recurrent neural networks such as the transformer, which are trained on large bodies of textual data using techniques such as pre-training and fine-tuning. This study focuses on the use of bacterial cellulose composites as a biobased unconventional construction material. The methodology of developing an LLM-aided LCA tool is divided into five stages: Defining the functional unit; identifying the life cycle stages; collecting environmental and social impact data; interpreting and evaluating; developing a web-based tool. The results of this study have shown that the designers can incorporate sustainable thinking in the design process by using LLMs integrated to LCA, ultimately contributing to a more sustainable future against the impacts of the Anthropocene. Overall, the outcomes demonstrated the value of human-computer interaction (HCI) as a tool for exploring new possibilities with biobased materials and for inspiring designers to reconsider the material evaluation in their work. Future studies can delve into the integration of this tool into building information modeling software or computational design software in order to perform LCA for 3D structures. Different scales of such applications in design practices, such as fashion design, product design or service design can also be conducted by questioning how LCA can be combined with LLMs to leverage novel sustainable design solutions. © 2023, Education and research in Computer Aided Architectural Design in Europe. All rights reserved.
ER  -
TY  - JOUR
AU  - Bale, A. S.
AU  - Vada, Y. R.
AU  - Oshiojum, B. E.
AU  - Lakkineni, U. K.
AU  - Rao, C.
AU  - Venkatesh, K.
AU  - Rani, I.
T1  - ChatGPT in Software Development: Methods and Cross-Domain Applications
JO  - International Journal of Intelligent Systems and Applications in Engineering
Y1  - 2023
VL  - 11
IS  - 9s
SP  - 636–643
EP  - 636–643
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171329846&partnerID=40&md5=4283848fd2a7f64e2f54575369e84b6a
KW  - AI
KW  - Automation
KW  - Generative images
KW  - Generative videos
KW  - UI
KW  - UX
U1  - 21476799 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 7; Correspondence Address: A.S. Bale; Dept. of ECE, New Horizon College of Engineering, India; email: ajaysudhirbale@gmail.com
N2  - Through the use of natural language processing (NLP) to analyze and synthesize data, ChatGPT has the potential to advance Software Engineering (SE) studies. Yet, it may provide moral challenges such as the possibility for data bias, leakage of private information, and the compromise of sensitive information. Utilizing ChatGPT, we highlight such current developments in SE. We also explore ChatGPT's potential outside of the gaming industry. Finally, we suggest leveraging ChatGPT to simplify UI and UX interaction. This article will provide an established protocol for using ChatGPT applications within SE investigation while keeping ethical concerns in mind. © 2023, Ismail Saritas. All rights reserved.
ER  -
TY  - JOUR
AU  - Retzlaff, C. O.
AU  - Das, S.
AU  - Wayllace, C.
AU  - Mousavi, P.
AU  - Afshari, M.
AU  - Yang, T.
AU  - Saranti, A.
AU  - Angerschmid, A.
AU  - Taylor, M. E.
AU  - Holzinger, A.
T1  - Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities
JO  - Journal of Artificial Intelligence Research
Y1  - 2024
VL  - 79
SP  - 359–415
EP  - 359–415
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184656775&doi=10.1613%2fjair.1.15348&partnerID=40&md5=7d43b0336a7075eb678e93ab7d56a7bf
M3  - https://doi.org/10.1613/jair.1.15348
KW  - Autonomous agents
KW  - Iterative methods
KW  - Learning systems
KW  - Robots
KW  - Fine tuning
KW  - Human-in-the-loop
KW  - Iterative update
KW  - Learn+
KW  - Learning process
KW  - Machine-learning
KW  - Performance
KW  - Reinforcement learnings
KW  - Reward function
KW  - Users' experiences
KW  - Reinforcement learning
U1  - 10769757 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 50; CODEN: JAIRF
N2  - Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively. ©2024 The Authors. Published by AI Access Foundation under Creative Commons Attribution License CC BY 4.0.
ER  -
TY  - JOUR
AU  - Bandi, A.
AU  - Adapa, P. V. S. R.
AU  - Kuchi, Y. E. V. P. K.
T1  - The Power of Generative AI: A Review of Requirements, Models, Input-Output Formats, Evaluation Metrics, and Challenges
JO  - Future Internet
Y1  - 2023
VL  - 15
IS  - 260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169062394&doi=10.3390%2ffi15080260&partnerID=40&md5=371a8a3f70afb5c8dc6038278a12c1b2
M3  - https://doi.org/10.3390/fi15080260
KW  - AIGC
KW  - AIGC models
KW  - ChatGPT
KW  - generative adversarial networks
KW  - generative AI models
KW  - generative AI survey
KW  - GPT-3
KW  - GPT-4
KW  - transformers
KW  - user experience
KW  - Quality control
KW  - Taxonomies
KW  - AIGC model
KW  - Generative artificial intelligence model
KW  - Generative artificial intelligence survey
KW  - Intelligence models
KW  - Transformer
KW  - Users' experiences
KW  - Generative adversarial networks
U1  - 19995903 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 287; Correspondence Address: A. Bandi; School of Computer Science and Information Systems, Northwest Missouri State University, Maryville, 64468, United States; email: ajay@nwmissouri.edu
N2  - Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input-output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input-output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance. © 2023 by the authors.
ER  -
TY  - CONF
AU  - Choe, M.
AU  - Bosch, E.
AU  - Dong, J.
AU  - Alvarez, I.
AU  - Oehl, M.
AU  - Jallais, C.
AU  - Alsaid, A.
AU  - Nadri, C.
AU  - Jeon, M.
T1  - Emotion GaRage Vol. IV: Creating Empathic In-Vehicle Interfaces with Generative AIs for Automated Vehicle Contexts
PB  - Association for Computing Machinery
AD  - University of Michigan-Dearborn, United States
Y1  - 2023
SP  - 234–236
EP  - 234–236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173674777&doi=10.1145%2f3581961.3609828&partnerID=40&md5=37010600cfec5f4cd85e1ba6908eb9ce
M3  - https://doi.org/10.1145/3581961.3609828
KW  - affective computing
KW  - ChatGPT
KW  - emotions
KW  - empathic vehicles
KW  - interaction design
KW  - Automation
KW  - Design
KW  - User interfaces
KW  - Affective Computing
KW  - Artificial intelligence tools
KW  - Automated vehicles
KW  - Emotion
KW  - Empathic vehicle
KW  - Interaction design
KW  - Model based approach
KW  - Vehicle display
KW  - Vehicle interface
KW  - Vehicles
U1  - 979-840070112-2 (ISBN)
N1  - Export Date: 11 July 2025; Cited By: 4; Correspondence Address: M. Jeon; Grado Department of Industrial and Systems Engineering, Virginia Tech, United States; email: myounghoonjeon@vt.edu; Conference name: 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2023; Conference date: 18 September 2023 through 21 September 2023; Conference code: 192847
N2  - This workshop aims to design advanced empathic user interfaces for in-vehicle displays, particularly for high-level automated vehicles (SAE level 3 or higher). Incorporating model-based approaches for understanding human emotion regulation, it seeks to enhance the user-vehicle interaction. A unique aspect of this workshop is the integration of generative artificial intelligence (AI) tools in the design process. The workshop will explore generative AI's potential in crafting contextual responses and its impact on user experience and interface design. The agenda includes brainstorming on various driving scenarios, developing emotion-oriented intervention methods, and rapid prototyping with AI tools. The anticipated outcome includes practical prototypes of affective user interfaces and insights on the role of AI in designing human-machine interactions. Through this workshop, we hope to contribute to making automated driving more accessible and enjoyable. © 2023 Owner/Author.
ER  -
TY  - CONF
AU  - Shoeibi, N.
AU  - Balderas, A.
AU  - Martinez-Mones, A.
AU  - Dodero, J. M.
AU  - Ros, S.
T1  - Cross-lingual Transfer in Generative AI-Based Educational Platforms for Equitable and Personalized Learning
PB  - CEUR-WS
AD  - University of Salamanca, Salamanca, Spain
Y1  - 2023
VL  - 3542
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177038444&partnerID=40&md5=4e4bb27e6325f4cfb762c25218c46ff3
KW  - Bias1
KW  - Diffusion Model
KW  - Education
KW  - Equality
KW  - Generative AI
KW  - language learning
KW  - LLM
KW  - Personalized Learning
KW  - Diffusion
KW  - Ethical technology
KW  - Learning systems
KW  - User interfaces
KW  - Cross-lingual
KW  - Diffusion model
KW  - Educational platforms
KW  - Language learning
KW  - Language model
KW  - Large language model
KW  - Personalized learning
KW  - Search engines
U1  - 16130073 (ISSN)
N1  - Export Date: 11 July 2025; Cited By: 0; Correspondence Address: N. Shoeibi; University of Salamanca, Salamanca, Spain; email: Nastaran@usal.es; Conference name: 11th Learning Analytics Summer Institute Spain, LASI Spain 2023; Conference date: 29 June 2023 through 30 June 2023; Conference code: 194093
N2  - This doctoral thesis explores the integration of Generative AI, specifically Large Language Models (LLMs) and diffusion models, in educational platforms. Emphasis is placed on cross-lingual transfer techniques to overcome language barriers and create personalized content. The study addresses the impact of Generative AI on personalized learning experiences and ethical concerns. A mixed-methods approach combines quantitative usage metrics with qualitative insights from interviews and surveys. Initial results indicate improved task performance and user engagement, but ongoing refinement is needed to address biases and ethics. The LATILL platform, a web search engine for German as Foreign Language teachers, is a case study. It leverages Generative AI to provide level-Appropriate texts, translations, and image generation. The research aims to determine this technology s impact and future potential on user experience, focusing on equitable access to personalized learning across diverse geolocations. © 2023 CEUR-WS. All rights reserved.
ER  -
TY  - CONF
AU  - Abdelraheem, Ahmed
AU  - Elbanna, Malak
AU  - Elnaggar, Mohamed
AU  - Shawky, Doaa
T1  - Defining a New Metric for Detecting Bias in Software Systems: Towards Ethical Software Engineering
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICEENG64546.2025.11031392
KW  - and program slicing;bias detection;Codes;control flow analysis;ethical software systems;Ethics;Ethnicity;Machine Learning;machine learning (ML);Measurement;Software engineering;Software systems;Software testing;Static analysis;Systematics;User experience
N2  - Bias in software systems poses ethical concerns that may lead to unintended discrimination, particularly when sensitive variables (e.g., gender, ethnicity) influence decision-making processes. While bias detection in machine learning models has been extensively studied, traditional software systems remain largely unexplored. However, implicit bias can manifest in conditional logic, user role definitions, or static decision trees, which directly influences user experience and access equity in real-world applications (e.g., government services or healthcare platforms). This paper presents a novel static analysis methodology for detecting and quantifying bias in general-purpose software systems. The proposed approach leverages static backward slicing to isolate relevant code, builds a Control Flow Graph (CFG) to trace sensitive variables in conditional branches, and utilizes a Control Dependency Graph (CDG) to assess bias propagation in a weighted-analysis format. Two bias metrics are inferred from the process: Bias Impact Score (BIS), which quantifies how the detected bias influences code execution, and the Bias Severity Score (BSS), which measures the broader implications of the impact. A final composite metric is introduced combining static code structure and ML-based sensitivity analysis. The proposed methodology is evaluated using an LLM-generated dataset of 3920 code snippets from prior research, covering different demographic bias directions such as ethnicity, gender, religion, and occupation. Results, achieving 94.6% accuracy in bias detection, show that the proposed methodology effectively identifies and quantifies bias, allowing developers to mitigate ethical risks early in the software development lifecycle. This research paper provides a foundation for ethical software engineering by offering a systematic and scalable approach to bias detection not only limited to AI-driven models. The methodology currently focuses on Python code and may require adaptation for multi-file projects, reflecting scalability trade-offs. This opens future work opportunities including advanced contextual and implicit bias detection in several frameworks.
ER  -
TY  - CONF
AU  - Pourasad, Ali Ebrahimi
AU  - Maalej, Walid
T1  - Does GenAI Make Usability Testing Obsolete?
SP  - 437–449
EP  - 437–449
M3  - https://doi.org/10.1109/ICSE55347.2025.00138
KW  - AI4SE;AI-Inspired Design;App Development;Foundation models;Large Language Model;Large language models;Mobile applications;Predictive models;Quality Requirements;Recommender systems;Reviews;Software engineering;Source coding;Testing;Usability;Usability Engineering;User experience
N2  - Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM, we predicted usability issues in two open-source apps of a medium complexity and asked two usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.
ER  -
TY  - CONF
AU  - Qasse, I.
AU  - Mishra, S.
AU  - \th. Jónsson, B.
AU  - Khomh, F.
AU  - Hamdaqa, M.
T1  - Chat2Code: A Chatbot for Model Specification and Code Generation, The Case of Smart Contracts
SP  - 50–60
EP  - 50–60
M3  - https://doi.org/10.1109/SSE60056.2023.00018
KW  - Automatic Code Generation;Blockchain;Chatbots;Codes;Model-driven Engineering;Natural language processing;Oral communication;Smart contracts;Surveys;Syntactics;Vocabulary
N2  - The potential of automatic code generation through Model-Driven Engineering (MDE) frameworks has yet to be realized. Beyond their ability to help software professionals write more accurate, reusable code, MDE frameworks could make programming accessible for a new class of domain experts. However, domain experts have been slow to embrace these tools, as they still need to learn how to specify their applications' requirements using the concrete syntax (i.e., textual or graphical) of the new and unified domain-specific language. Conversational interfaces (chatbots) could smooth the learning process and offer a more interactive way for domain experts to specify their application requirements and generate the desired code. If integrated with MDE frameworks, chatbots may offer domain experts with richer domain vocabulary without sacrificing the power of agnosticism that unified modelling frameworks provide. In this paper, we discuss the challenges of integrating chatbots within MDE frameworks and then examine a specific application: the auto-generation of smart contract code based on conversational syntax. We demonstrate how this can be done and evaluate our approach by conducting a user experience survey to assess the usability and functionality of the chatbot framework. The paper concludes by drawing attention to the potential benefits of leveraging Language Models (LLMs) in this context.
ER  -
TY  - JOUR
AU  - Qin, Yihao
AU  - Wang, Shangwen
AU  - Lou, Yiling
AU  - Dong, Jinhao
AU  - Wang, Kaixin
AU  - Li, Xiaoling
AU  - Mao, Xiaoguang
T1  - SoapFL: A Standard Operating Procedure for LLM-Based Method-Level Fault Localization
JO  - IEEE Transactions on Software Engineering
Y1  - 2025
VL  - 51
IS  - 4
SP  - 1173–1187
EP  - 1173–1187
M3  - https://doi.org/10.1109/TSE.2025.3543187
KW  - Benchmark testing;Codes;Computer bugs;Debugging;Electronic mail;Fault Localization;Large Language Model;Large language models;Location awareness;Navigation;Standards;Usability
U1  - 1939-3520
N2  - Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to LLMs' limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system). To address the limitation, this paper presents SoapFL, which builds an LLM-driven standard operating procedure (SOP) to automatically localize buggy methods from the entire software. By simulating the behavior of a human developer, SoapFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within specific steps, SoapFL provides useful test behavior or coverage information to LLM through program analysis. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that SoapFL can localize 175 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in SoapFL with the ablation study and demonstrate the usability of SoapFL through a user study. Finally, the cost analysis shows that SoapFL spends an average of only 0.081 dollars and 92 seconds for a single bug.
ER  -
TY  - CONF
AU  - Raghi, K. R.
AU  - Sudha, K.
AU  - Sreeram A, M.
AU  - Joshua S, Steve
T1  - Software Development Automation Using Generative AI
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICERCS63125.2024.10894980
KW  - AI-driven software development;Automation;Code Generation;Generative AI;Generative AI (GenAI);GPT;LangChain;Large language models (LLMs);Manuals;Planning;Scalability;Scientific computing;SDLC automation;Software;Software Development Lifecycle (SDLC);Software development management;Software engineering;Testing
N2  - The Software Development Lifecycle (SDLC) is a structured process that guides the development of software projects, encompassing phases from planning to deployment. Traditionally, the SDLC has relied on manual input, making it prone to delays, errors, and inefficiencies. With the recent advancements in Generative AI (GenAI) and Large Language Models (LLMs) such as GPT, it is now feasible to automate substantial portions of the SDLC. This paper presents a novel approach to automating the SDLC using LLMs and the Langchain framework, aiming to streamline the entire software development process. By au-tomating key phases, including project planning, requirements gathering, code generation, testing, and deployment, this research explores how AI can minimize human intervention and accelerate software development timelines. The paper also discusses the potential advantages of AI-driven SDLC automation, such as improved efficiency, consistency, and scalability, while addressing challenges related to its integration. The proposed approach offers a glimpse into the future of software engineering, where AI plays a central role in transforming how software is developed and delivered.
ER  -
TY  - CONF
AU  - Rasool, Z.
AU  - Barnett, S.
AU  - Willie, D.
AU  - Kurniawan, S.
AU  - Balugo, S.
AU  - Thudumu, S.
AU  - Abdelrazek, M.
T1  - LLMs for Test Input Generation for Semantic Applications
SP  - 160–165
EP  - 160–165
KW  - Calibration;Costs;Large Language Model;Query Evaluation;question answering;Semantic Cache;Semantic search;Semantics;Software reliability;Software systems;Test Input Generation;User experience
N2  - Large language models (LLMs) enable state-of-the-art semantic capabilities to be added to software systems such as semantic search of unstructured documents and text generation. However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience. To address this problem, semantic caches are used to check for answers to similar queries (that may have been phrased differently) without hitting the LLM service. Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system. Adopting semantic cache techniques usually requires testing the effectiveness of a semantic cache (accurate cache hits and misses) which requires a labelled test set of similar queries and responses which is often unavailable. In this paper, we present VaryGen, an approach for using LLMs for test input generation that produces similar questions from unstructured text documents. Our novel approach uses the reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise subtle variations to queries, and 3) evaluate the synthesised test dataset. We evaluated our approach in the domain of a student question and answer system by qualitatively analysing 100 generated queries and result pairs, and conducting an empirical case study with an open source semantic cache. Our results show that query pairs satisfy human expectations of similarity and our generated data demonstrates failure cases of a semantic cache. Additionally, we also evaluate our approach on Qasper dataset. This work is an important first step into test input generation for semantic applications and presents considerations for practitioners when calibrating a semantic cache.CCS CONCEPTS• Software and its engineering → Empirical software validation.
ER  -
TY  - CONF
AU  - Parnin, Chris
AU  - Soares, Gustavo
AU  - Pandita, Rahul
AU  - Gulwani, Sumit
AU  - Rich, Jessica
AU  - Henley, Austin Z.
T1  - Building Your Own Product Copilot: Challenges, Opportunities, and Needs
SP  - 338–348
EP  - 338–348
M3  - https://doi.org/10.1109/SANER64311.2025.00039
KW  - AI;Artificial intelligence;Best practices;Buildings;Companies;intelligent applications;Interviews;large-language models;Pain;pain points;Software;Software development management;Software engineering;Testing
N2  - A race is underway to embed advanced AI capabil-ities into products. These product ``copilots'' enable users to ask questions in natural language and receive relevant responses that are specific to the user's context. In fact, virtually every large technology company is looking to add these capabilities to their software products. However, for most software engineers, this is often their first encounter with integrating AI-powered technol-ogy. Furthermore, software engineering processes and tools have not caught up with the challenges and scale involved with building AI-powered applications. In this work, we present the findings of an interview study with 26 professional software engineers responsible for building product copilots at various companies. From our interviews, we found pain points at every step of the engineering process and the challenges that strained existing development practices. We then conducted group brainstorming sessions to collaborative on opportunities and tool designs for the broader software engineering community.
ER  -
TY  - CONF
AU  - Richards, Jonan
AU  - Wessel, Mairieli
T1  - Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants
SP  - 6–10
EP  - 6–10
M3  - https://doi.org/10.1109/BotSE67031.2025.00009
KW  - Conferences;conversational agent;evaluation;HCI;Human computer interaction;Large language models;LLM;Roads;SE;Software engineering
N2  - As Large Language Models (LLMs) are increasingly adopted in software engineering, recently in the form of conversational assistants, ensuring these technologies align with developers' needs is essential. The limitations of traditional human-centered methods for evaluating LLM-based tools at scale raise the need for automatic evaluation. In this paper, we advocate combining insights from human-computer interaction (HCI) and artificial intelligence (AI) research to enable human-centered automatic evaluation of LLM-based conversational SE assistants. We identify requirements for such evaluation and challenges down the road, working towards a framework that ensures these assistants are designed and deployed in line with user needs.
ER  -
TY  - CONF
AU  - Sahoo, Priyam
AU  - Pujar, Saurabh
AU  - Nalawade, Ganesh
AU  - Gebhardt, Richard
AU  - Mandel, Louis
AU  - Buratti, Luca
T1  - Ansible Lightspeed: A Code Generation Service for IT Automation
SP  - 2148–2158
EP  - 2148–2158
KW  - ansible;Automation;code completion;Codes;Domain specific languages;generative models;ide;Large language models;Machine Learning;Natural languages;Productivity;Sentiment analysis;Software;Software engineering;User Study
N2  - The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been re-leased, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.
ER  -
TY  - CONF
AU  - Sai Kumar, Pusarla Venkata
AU  - Yadav, Mende Nikhil
AU  - Reddy, Eppa Chaithanya
AU  - Sathya, V.
T1  - Smart-Shield: A Hybrid ML-AI Framework for Advanced Phishing Detection using Gradient Boosting and LLM Analysis
SP  - 1467–1473
EP  - 1467–1473
M3  - https://doi.org/10.1109/ICPCSN65854.2025.11035754
KW  - Accuracy;Artificial intelligence;Boosting;Google's Gemini LLM;Gradient Boosting Classifier;Java;Java Server Pages;Machine Learning;NetBeans;Phishing;Phishing Detection;Portals;Protection;Real-time systems;Servers;Social networking (online);Uniform resource locators;URL Classification
N2  - Quick changes in phishing attempts make sophisticated detection techniques absolutely vital. This work presents a phishing detection system aiming at increasing accuracy and adaptability by combining machine learning with artificial intelligence-driven analysis. Along with Google's Gemini 2.0 Flash LLM for more in-depth contextual analysis, it makes use of a Gradient Boosting Classifier (GBC) taught on 30 URL characteristics. SMote for class balance and tuned hyperparameters help to improve the model by means of fine-tuning of the detection performance. Two parts characterize the development process: the first phase concentrates on establishing the ML model, which comprises URL categorization, shortlink identification, and phishing pattern analysis; the second phase employs JSP and NetBeans to build a Java-based web application. Three portals comprise the system: an administrative portal for user management, phishing trend analysis and URL classification; a user portal for secure web browsing, suggestions, and automatic phishing alerts; and an attacker simulation site to test phishing detection capability. Python is connected with NetBeans to provide real-time classification and security analysis; an automatic access control method warns or temporarily limits users who surpass limits on browsing harmful URLs. This research presents a very accurate phishing detection system with real-time alarms and enhanced protection against changing threats by combining machine learning with AI-driven analysis.
ER  -
TY  - CONF
AU  - Sarda, Komal
AU  - Namrud, Zakeya
AU  - Watts, Ian
AU  - Shwartz, Larisa
AU  - Nagar, Seema
AU  - Mohapatra, Prateeti
AU  - Litoiu, Marin
T1  - Augmenting Automatic Root-Cause Identification with Incident Alerts Using LLM
SP  - 1–10
EP  - 1–10
M3  - https://doi.org/10.1109/CASCON62161.2024.10838171
KW  - Accuracy;Cloud native applications;Complexity theory;Computational modeling;Incident management;Large language models;Manuals;Measurement;Reliability engineering;Root cause analysis;Runtime;Soft sensors
N2  - Ensuring the reliability and availability of cloud services relies heavily on efficient root cause analysis (RCA) for cloud incidents. Traditionally, RCA involved labor-intensive manual investigations across various data sources, such as logs, metrics, and traces. Despite the increasing adoption of AI-driven assistants for RCA, their effectiveness for Site Reliability Engineers (SREs) is often hindered by low accuracy due to the inherent complexity of the task. This study introduces an on-call system powered by a large language model (LLM) designed to automate RCA processes for cloud incidents in practical, privacy-aware industrial settings. Our approach integrates incoming multimodal datasets (Logs, Metrics, Traces, Alerts-LMTA), aggregates critical runtime diagnostic information, including developer set alerts with precise offset values and thresholds, and utilizes LMTA data to predict the root cause category of incidents. We evaluated the effectiveness of our approach using two open-source, real-world-like datasets, demonstrating that the proposed LLM based approach can achieve an RCA accuracy of up to 97%.
ER  -
TY  - JOUR
AU  - Sasaki, Yuya
AU  - Washizaki, Hironori
AU  - Li, Jialong
AU  - Yoshioka, Nobukazu
AU  - Ubayashi, Naoyasu
AU  - Fukazawa, Yoshiaki
T1  - Landscape and Taxonomy of Prompt Engineering Patterns in Software Engineering
JO  - IT Professional
Y1  - 2025
VL  - 27
IS  - 1
SP  - 41–49
EP  - 41–49
M3  - https://doi.org/10.1109/MITP.2024.3525458
KW  - Large language models;Performance evaluation;Prompt engineering;Quality assessment;Software engineering;Taxonomy;Usability;User experience;User-generated content
U1  - 1941-045X
N2  - Advancements in large language models (LLMs) have enhanced their ability to handle ambiguous user instructions. However, effective prompt patterns remain crucial for usability and comprehension. This article presents a taxonomy of prompt engineering patterns for software engineering. It is based on a systematic literature review that was conducted in early 2023, when LLMs still faced significant limitations in context length and inference capabilities. Our study explores techniques that enhance the usability and reliability of LLMs, emphasizing the ongoing importance of well-designed prompts in optimizing task performance. Our findings highlight the critical role of prompt patterns in maximizing LLM's potential, even as their capabilities continue to evolve.
ER  -
TY  - CONF
AU  - Senthamarai, N.
AU  - Jeyaselvi, M.
AU  - Hemamalini, V.
T1  - Automatic Cloud Formation Using LLM
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICoICC64033.2025.11052114
KW  - Automation;Best practices;Cloud computing;Codes;Collaboration;Generation;Generators;IAC;Industries;Manuals;Software development management;Software reliability;Template;Terraform
N2  - Introduces a groundbreaking IAC Code Generator for automated Terraform script creation. Empowers developers by treating infrastructure as a versioned, programmable artifact. Enhances efficiency, reduces time-to-market, ensures consistency, and promotes collaboration between development and operations teams. Features an intuitive interface, customizable templates, and integrates industry best practices for accessible and accelerated development cycles.
ER  -
TY  - CONF
AU  - Sergeyuk, A.
AU  - Lvova, O.
AU  - Titov, S.
AU  - Serova, A.
AU  - Bagirov, F.
AU  - Kirillova, E.
AU  - Bryksin, T.
T1  - Reassessing Java Code Readability Models with a Human-Centered Approach
SP  - 225–235
EP  - 225–235
KW  - AI-Generated Code;Code Readability;Code Readability Models;Codes;Correlation;Data models;Human-Computer Interaction;Java;Predictive models;Productivity;Repertory Grid Technique;Surveys
N2  - To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it's often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.
ER  -
TY  - CONF
AU  - Shahariar, G. M.
AU  - Hasan, T.
AU  - Iqbal, A.
AU  - Uddin, G.
T1  - Contrastive Learning for API Aspect Analysis
SP  - 637–648
EP  - 637–648
M3  - https://doi.org/10.1109/ASE56229.2023.00064
KW  - API aspects;API review;Aspect detection;Contrastive learning;Documentation;LIME;Linear programming;Performance analysis;Security;Semantics;Training;Transformers
N2  - We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.
ER  -
TY  - CONF
AU  - Rogozinnikov, E. I.
AU  - Doskalesku, K. V.
AU  - Ivanov, A. A.
AU  - Marinov, Y. A.
AU  - Morozov, S. M.
AU  - Yakunin, A. V.
T1  - Training Generative Models for the Task of Object Description in A Strict Format: Analysis of the Efficiency of Their Architectures
SP  - 1–15
EP  - 1–15
M3  - https://doi.org/10.1109/RPA65165.2024.10932905
KW  - AI Model Training;Analytical models;Computational modeling;Computer architecture;Digital Models;Generative AI;GPT;Industries;LLM;Recurrent Neural Networks;Software packages;Substations;Training;Transformers;Visualization
N2  - In the modern world, digitalization is increasingly penetrating into people's daily lives and covers a wide variety of areas of human activity. Electric power industry is no exception. The introduction of SCADA systems has increased the observability of processes occurring in the network and increased the efficiency and speed of decision-making aimed at maintaining a stable operating mode of the EPS. The next step in the development of digital electric power industry is digital twins, which could be integrated into digital services for their visual display or, for example, for conducting processes of modeling the operating mode of the EPS. The introduction of artificial intelligence (AI) is also gaining popularity. More and more advanced AI models are constantly emerging, capable of solving increasingly complex problems. The introduction of generative artificial intelligence can significantly speed up the process of building digital models. This article provides a comparative review of various generative AI models that are further trained to generate descriptions of electric power substations in a strictly formalized form that can be further computer processed.
ER  -
TY  - JOUR
AU  - Shang, Y.
AU  - Liu, Z.
AU  - Kang, J.
AU  - Hossain, M. S.
AU  - Wu, Y.
T1  - Adversarial Attacks on Vision-Language Model-Empowered Chatbots in Consumer Electronics
JO  - IEEE Transactions on Consumer Electronics
Y1  - 2024
VL  - 70
IS  - 3
SP  - 6075–6083
EP  - 6075–6083
M3  - https://doi.org/10.1109/TCE.2024.3417688
KW  - adversarial attacks;AIGC;Artificial intelligence;Chatbots;Consumer electronics;Data models;Media;multimodal attacks;Security;vision-language models;Visualization
N2  - Artificial Intelligence-Generated Content (AIGC) technology has revolutionized content creation, distribution, and engagement in the consumer electronics sector, propelling its applications to unprecedented heights. Within this landscape, AIGC-driven conversational agents, exemplified by renowned chatbots like ChatGPT, have gained widespread popularity globally. These advanced conversational agents are instrumental in significantly enhancing user efficiency and overall experience within consumer electronics applications. However, with the increasing integration of vision-language models (VLMs, a representative of AIGC) in consumer electronics, the vulnerability of chatbots to adversarial attacks has become a critical concern. This paper investigates and analyzes the susceptibility of VLMs-empowered chatbots to adversarial manipulation, particularly in the context of consumer electronics applications. The study employs a comprehensive approach, combining vision and language modalities, to explore potential attack vectors and vulnerabilities. Specifically, we designed three adversarial attacks, which all exploited the insufficient alignment of VLMs on multi-modal data to implement effective attacks and make chatbots output harmful content. A series of experiments demonstrate the efficacy of adversarial attacks on three popular chatbot systems, revealing vulnerabilities that may compromise the reliability and security of these systems in real-world scenarios. The findings emphasize the importance of robust defenses against adversarial attacks in VLMs-driven chatbots, urging the development of enhanced security measures to safeguard users and prevent malicious exploitation of consumer electronics. Our data and code are available at https://github.com/yxc0731/VLM-Adversarial-Attacks.
ER  -
TY  - CONF
AU  - Panchal, D.
AU  - Verma, P.
AU  - Baran, I.
AU  - Musgrove, D.
AU  - Lu, D.
T1  - Simplifying Network Orchestration using Conversational AI
SP  - 84–89
EP  - 84–89
M3  - https://doi.org/10.1109/ICOIN59985.2024.10572160
KW  - 5G;6G;Automation;Companies;Complexity theory;Intent Driven Networking;Intent recognition;Intent-Based Networking;Large language models (LLMs);Machine Learning;Natural language processing;Network Function Virtualization;Network Orchestration;Next Generation Networks;Open Network Automation Platform (ONAP);Open Source;Operations support systems (OSS);Oral communication;Software Defined Net-working;Task analysis;User experience
N2  - ONAP is a comprehensive platform for orchestration, management and automation of network and edge computing services for 5G, 6G and Next Generation Networks. Unlike traditional OSSs, it is an open-source project where companies all over the world are collaborating to build different functionalities of an end-to-end Network operating system. For this reason, the ONAP platform has several different sub projects and APIs each performing a specific function to achieve Network Management. There is some complexity associated with using these APIs and knowing and understanding the many parameters associated with them, which impedes adoption. This not only prevents an end- to-end cloud service orchestration like experience for network services, but also increases the time and money spent on network orchestration. This paper proposes and discusses the design of a conversational AI solution that can interface with some significant APIs in ONAP to solve these problems. The conversational AI solution has the potential to significantly simplify network orchestration tasks. This work is being further extended to using Large Language Models (LLMs) to achieve simplified Intent-Based management and orchestration paradigms within ONAP.
ER  -
TY  - CONF
AU  - Ott, N.
AU  - Horst, R.
AU  - Dörner, R.
T1  - Towards Reducing Latency Using Beam Search in an Interactive Conversational Speech Agent
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/GEM61861.2024.10585772
KW  - Beam Search;Computer architecture;Conversational Artificial Intelligence;Entertainment industry;Human-Computer Interaction;Interactive Podcasts;Large language models;Latency Reduction;Media;Oral communication;Semantics;Surveys
N2  - The rapid advancement of generative artificial in-telligence (AI) has led to groundbreaking developments in large language models. As large language models generate textual sequences autoregressively, mitigating latency becomes imper-ative for providing a highly immersive interaction experience within a realtime conversation, for example, providing fast and accurate responses to users' questions. Current efforts focus on accelerating inference processes, yet often at the expense of model architecture alterations, leading to compromised quality. In this paper, we explore latency reduction in the case of speech-based conversational agents. We leverage mathematical functions based on Beam Search to analyze autoregressive textual sequences, enabling a nuanced evaluation of semantic quality during auditory interaction, for example, for use within interactive web podcasts. We implemented our concepts and used the software to evaluate the concepts within (1) an automated evaluation of 1000 question-answer pairs and (2) a user survey. The results show that the semantic quality of autoregressive textual sequences could be assessed successfully by our proposed mathematical terms.
ER  -
TY  - CONF
AU  - Mahto, Manoj Kumar
AU  - Srivastava, Durgesh
AU  - Kumar, Ranjit
AU  - Sah, Basant
AU  - Singh, Hare Ram
AU  - Maakar, Sunil Kr.
T1  - Personalized User Interaction in Web Applications using Adaptive LLM Model
SP  - 962–966
EP  - 962–966
M3  - https://doi.org/10.1109/ICPCT64145.2025.10940477
KW  - Adaptation models;Analytical models;Behavioural Analytics;Computational modeling;Context modeling;Context-Aware Language Models;Contextual Embedding Strategies;Data privacy;Large language models;LLM Model;Natural language processing;NLP;Reviews;Scalability;Solid modeling;User Interaction Modeling;Web Application Personalization
N2  - The rapid evolution of web applications has made personalization and intuitive user experiences increasingly essential. The aim of this work is to provide a tailored large language model (LLM) architecture allowing context-aware interactions within online contexts. Using real-world datasets like as Microsoft COCO and Amazon Customer Reviews enhanced with contextual metadata and behavioural analytics, we taught LLMs to grasp customer wants via explicit patterns and contextual signals. To allow the models to dynamically modify their replies, improve user engagement, and expedite processes, sophisticated natural language processing methods and contextual embedding strategies were used. In comparison to conventional static models, extensive experimentation and user testing exposed notable increases in user happiness and interaction efficiency. Significant problems like data privacy, model scalability, and real-time processing were resolved to guarantee the feasibility of these bespoke LLMs throughout several web application environments.
ER  -
TY  - CONF
AU  - Mailach, Alina
AU  - Simon, Sebastian
AU  - Dorn, Johannes
AU  - Siegmund, Norbert
T1  - Themes of Building LLM-Based Applications for Production: A Practitioner's View
SP  - 18–30
EP  - 18–30
M3  - https://doi.org/10.1109/CAIN66642.2025.00011
KW  - agents;Computer architecture;evaluation;fine-tuning;Large language models;LLM in production;Manuals;Production;Prompt engineering;RAG;Retrieval augmented generation;retrieval-augmented generation;Software engineering;Systematics;Tuning;Videos
N2  - Background: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications. Method: We collected 189 videos from 2022 to 2024 by practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes. Results: The most prevalent topics fall within the theme Design & Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., finetuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges. Implications: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further highlight topics of interest for academics where further research is needed.
ER  -
TY  - CONF
AU  - Mao, Ziyu
AU  - Wang, Jingyi
AU  - Sun, Jun
AU  - Qin, Shengchao
AU  - Xiong, Jiawen
T1  - LLM-Aided Automatic Modeling for Security Protocol Verification
SP  - 642–654
EP  - 642–654
M3  - https://doi.org/10.1109/ICSE55347.2025.00197
KW  - Analytical models;Automatic modeling;Codes;Large language models;LLMs;Manuals;Natural languages;Protocols;Security;Semantics;Software engineering;Symbolic analysis;Transforms
N2  - Symbolic protocol analysis serves as a pivotal technique for protocol design, security analysis, and the safeguarding of information assets. Several modern tools such as Tamarin and ProVerif have been proven successful in modeling and verifying real-world protocols, including complex protocols like TLS 1.3 and 5G AKA. However, developing formal models for protocol verification is a non-trivial task, which hinders the wide adoption of these powerful tools in practical protocol analysis. In this work, we aim to bridge the gap by developing an automatic method for generating symbolic protocol models using Large Language Models (LLMs) from protocol descriptions in natural language document. Although LLMs are powerful in various code generation tasks, it is shown to be ineffective in generating symbolic models (according to our empirical study). Therefore, rather than applying LLMs naively, we carefully decompose the symbolic protocol modeling task into several stages so that a series of formal models are incrementally developed towards generating the final correct symbolic model. Specifically, we apply LLMs for semantic parsing, enable lightweight manual interaction for disambiguation, and develop algorithms to transform the intermediate models for final symbolic model generation. To ensure the correctness of the generated symbolic model, each stage is designed based on a formal execution model and the model transformations are proven sound. To the best of our knowledge, this is the first work aiming to generate symbolic models for protocol verification from natural language documents. We also introduce a benchmark for symbolic protocol model generation, with 18 real-world security protocol's text description and their corresponding symbolic models. We then demonstrate the potential of our tool, which successfully generated correct models of moderate scale in 10 out of 18 cases. Our tool is released at [1].
ER  -
TY  - CONF
AU  - Marini, L.
AU  - Gisslén, L.
AU  - Sestini, A.
T1  - Leveraging Large Language Models for Efficient Failure Analysis in Game Development
SP  - 1–8
EP  - 1–8
M3  - https://doi.org/10.1109/CoG60054.2024.10645540
KW  - Accuracy;Codes;Computer bugs;Failure analysis;Games;Large language models;Natural language processing;Natural languages;Software development;Software quality;Tracing;Validation
N2  - In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product. Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes. However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71 % in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time - up to 60 % - spent investigating issues.
ER  -
TY  - CONF
AU  - Meißner, N.
AU  - Speth, S.
AU  - Becker, S.
T1  - Automated Programming Exercise Generation in the Era of Large Language Models
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/CSEET62301.2024.10662984
KW  - AI-Generated Exercises;Chatbots;Engineering education;Graphical user interfaces;Internet;Large language models;Manuals;Programming Exercises;Programming profession;Refining;Software engineering;Software Engineering Education;Usability
N2  - Lecturers are increasingly attempting to use large language models (LLMs) to simplify and make the creation of exercises for students more efficient. Efforts are also being made to automate the exercise creation process in software engineering (SE) education. This study explores the use of advanced LLMs, including GPT-4 and LaMDA, for automated programming exercise creation in higher education and compares the results with related work using GPT-3.5-turbo. Utilizing applications such as ChatGPT, Bing AI Chat, and Google Bard, we identify LLMs capable of initiating different exercise designs. However, manual refinement is crucial for accuracy. Common error patterns across LLMs highlight challenges in complex programming concepts, while specific strengths in various topics showcase model distinctions. This research underscores LLMs' value in exercise generation, emphasizing the critical role of human supervision in refining these processes. Our concise insights cater to educators, practitioners, and other researchers seeking to enhance SE education through LLM applications.
ER  -
TY  - CONF
AU  - Meng, F.
AU  - Qi, M.
AU  - Bao, S.
T1  - Comprehensive Evaluation of English Learning Apps Based on Natural Language Processing
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/CICN59264.2023.10402171
KW  - Communication networks;Comprehensive Evaluation;Computational intelligence;Education;English Learning Apps;Market research;Natural language processing;Online Classroom;Software
N2  - Mobile app-based learning is considered a futureoriented educational paradigm and an indispensable mode of learning in the future. Objective and effective evaluation of English learning apps holds significant importance in enhancing teaching effectiveness. The main work of this paper includes the following content. It elucidates the understanding of natural language processing technology, outlines the principles for constructing evaluation criteria for English learning apps, and extensively explores both rational and emotional aspects of evaluation criteria for English learning apps. Finally, it analyzes the developmental trends of natural language processing technology in the comprehensive evaluation of English learning apps. The primary aim of this work is to promote education in a more rational direction by offering a comprehensive set of evaluation criteria for English learning apps, which can be utilized by third-party assessment agencies or individuals. This facilitates a more efficient selection of suitable learning software for end-users and provides valuable recommendations for software developers to enhance their products.
ER  -
TY  - CONF
AU  - Ouaazki, A.
AU  - Bergram, K.
AU  - Holzer, A.
T1  - Leveraging ChatGPT to Enhance Computational Thinking Learning Experiences
SP  - 1–7
EP  - 1–7
M3  - https://doi.org/10.1109/TALE56641.2023.10398358
KW  - Chatbots;Collaborative Learning;Computational modeling;Education;Human-Computer Interaction;Interactive Learning Environments;Large language models;Software;Springs;Task analysis;User experience
N2  - Given the pervasive reliance on technology in modern society, teaching Computational Thinking (CT) abilities is becoming increasingly relevant. These abilities, such as modeling and coding, have become crucial for a larger audience of students, not only those who wish to become software engineers or computer scientists. Recent advances in Large Language Models (LLMs), such as ChatGPT, provide powerful assistance to complete computational tasks, by simplifying code generation and debugging, and potentially enhancing interactive learning. However, it is not clear if these advances make CT tasks more accessible and inclusive for all students, or if they further contribute to a digital skills divide, favoring the top students. To address this gap, we have created and evaluated a novel learning scenario for transversal CT skills that leveraged LLMs as assistants. We conducted an exploratory field study during the spring semester of 2022, to assess the effectiveness and user experience of LLM-augmented learning. Our results indicate that the usage of ChatGPT as a learning assistant improves learning outcomes. Furthermore, contrary to our predictions, the usage of ChatGPT by students does not depend on prior CT capabilities and as such does not seem to exacerbate prior inequalities.
ER  -
TY  - BOOK
AU  - Miller, Richard H.
AU  - Johnson, Jeff
PB  - Packt Publishing
Y1  - 2024
UR  - https://ieeexplore.ieee.org/document/10769348
N2  - Create engaging AI experiences by mastering ChatGPT for business and leveraging user interface design practices, research methods, prompt engineering, the feeding lifecycle, and moreKey FeaturesLearn in-demand design thinking and user research techniques applicable to all conversational AI platformsMeasure the quality and evaluate ChatGPT from a customer's perspective for optimal user experienceSet up and use your secure private data, documents, and materials to enhance your ChatGPT modelsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany enterprises grapple with new technology, often hopping on the bandwagon only to abandon it when challenges emerge. This book is your guide to seamlessly integrating ChatGPT into enterprise solutions with a UX-centered approach. UX for Enterprise ChatGPT Solutions empowers you to master effective use case design and adapt UX guidelines through an engaging learning experience. Discover how to prepare your content for success by tailoring interactions to match your audience's voice, style, and tone using prompt-engineering and fine-tuning. For UX professionals, this book is the key to anchoring your expertise in this evolving field. Writers, researchers, product managers, and linguists will learn to make insightful design decisions. You'll explore use cases like ChatGPT-powered chat and recommendation engines, while uncovering the AI magic behind the scenes. The book introduces a and feeding model, enabling you to leverage feedback and monitoring to iterate and refine any Large Language Model solution. Packed with hundreds of tips and tricks, this guide will help you build a continuous improvement cycle suited for AI solutions. By the end, you'll know how to craft powerful, accurate, responsive, and brand-consistent generative AI experiences, revolutionizing your organization's use of ChatGPT.What you will learnAlign with user needs by applying design thinking to tailor ChatGPT to meet customer expectationsHarness user research to enhance chatbots and recommendation enginesTrack quality metrics and learn methods to evaluate and monitor ChatGPT's quality and usabilityEstablish and maintain a uniform style and tone with prompt engineering and fine-tuningApply proven heuristics by monitoring and assessing the UX for conversational experiences with trusted methodsRefine continuously by implementing an ongoing process for chatbot and feedingWho this book is forThis book is for user experience designers, product managers, and product owners of business and enterprise ChatGPT solutions who are interested in learning how to design and implement ChatGPT-4 solutions for enterprise needs. You should have a basic-to-intermediate level of understanding in UI/UX design concepts and fundamental knowledge of ChatGPT-4 and its capabilities.
ER  -
TY  - CONF
AU  - Mougouei, Davoud
AU  - Azarnik, Ahmad
AU  - Fahmideh, Mahdi
AU  - Mougouei, Elahe
AU  - Dam, Hoa Khanh
AU  - Khan, Arif Ali
AU  - Rafi, Saima
AU  - Khan, Javed Ali
AU  - Ahmad, Aakash
T1  - A First Look at AI Trends in Value-Aligned Software Engineering Publications: Human-LLM Insights
SP  - 82–93
EP  - 82–93
M3  - https://doi.org/10.1109/ICSE-SEIS66351.2025.00014
KW  - Artificial intelligence;Artificial Intelligence (AI);Chatbots;ChatGPT;Human Values;Large language models;Large language models (LLMs);Market research;Pragmatics;Safety;Security;Social networking (online);Software;Software engineering;Value-Aligned Publications
N2  - Recent criticism of social media platforms by the U.S. Senate Judiciary Committee for neglecting child safety exemplifies how software can undermine human values. This is further complicated by the growing integration of Artificial Intelligence (AI) in software, which introduces inherent challenges such as biases and limited transparency. However, AI also presents opportunities to embed human values into software. To explore these opportunities, we have utilized the reasoning abilities of ChatGPT, a large language model (LLM), in combination with human expertise, to study the use of AI in publications that address human values, across some of the leading software engineering (SE) venues from 2022 to 2023. Our findings confirm the use of AI concepts - mainly General Machine Learning - in around 33 % of these valuealigned publications. The value alignments largely concern pragmatic aspects of Achievement and (personal) Security, while the majority of the values receive less attention. The socially focused values of Conformity and Tradition and the personally focused value of Hedonism are rarely addressed in the SE publications.
ER  -
TY  - CONF
AU  - Munjal, Geetika
AU  - Dious, Austin
AU  - Agarwal, Devansh
AU  - Sharma, Rahul
AU  - Jarawat, Gunjan
T1  - Blindviz — Intelligent Navigation and Environmental Awareness for the Visually Impaired
SP  - 492–496
EP  - 492–496
M3  - https://doi.org/10.1109/ICDT63985.2025.10986575
KW  - Accuracy;Blind;BlindViz;Image processing;Image segmentation;Large language models;LLMs;Natural language processing;Navigation;Object detection;OpenAI API;Real-time systems;Speech to text;Text Recognition;Usability;User experience;Visual Assistance;Visualization;Visually Impaired;YOLO;YOLOv8
N2  - The blind and visually impaired people must face significant challenges while navigating their environment independently. This paper presents ``BlindViz'', a mobile application specially designed to aid the visually impaired by providing real-time object detection and scene description using the latest advancements in the AI technology. This app integrates Large Language Models (LLMs) and leverages You Only Look Once (YOLOv8) for object detection, Easy Optical Character Recognition (EasyOCR) for Text Recognition and leveraging Image Segmentation for natural language scene descriptions. As the word ``Activate'' is spoken into the app, it takes an image, processes it, and delivers a description through an audible output. This paper outlines the development, implementation, and evaluation of BlindViz, along with the suggestions for future enhancements to improve its usability and effectiveness.
ER  -
TY  - CONF
AU  - Nabasirye, Angella
AU  - Ssali, Irene Wanyana
T1  - Integrating Natural Language Processing and Large Language Models Into DHIS2 to Improve Health Data Utilization
SP  - 47–52
EP  - 47–52
M3  - https://doi.org/10.1109/SEiGS66664.2025.00012
KW  - Artificial intelligence;Data analysis;Data models;Decision making;DHIS2;Healthcare Informatics;Informatics;Large language models;Natural language processing;NLP;Public healthcare;Software engineering;Technological innovation;Uganda;Usability
N2  - In the evolving landscape of global healthcare, the need for efficient and scalable data analysis has never been more critical. This research explores the integration of Large Language Models (LLMs) and Natural Language Processing (NLP) into the District Health Information System 2 (DHIS2), a platform widely used for health data management in Uganda. The objective of this study is to enhance the usability and data analysis capabilities of DHIS2, making it accessible to both technical and non-technical users. By leveraging AI-driven models like LangChain and Ollama, the research developed an automated data analysis system that allows users to interact with DHIS2 using natural language queries in English. This integration simplifies the extraction of meaningful insights from complex datasets, thereby improving decision-making processes. The system was evaluated for usability, performance, and its potential to enhance public health data analysis. Results demonstrate that the AI-augmented DHIS2 platform significantly reduces analysis time, improves data accessibility, and facilitates timely public health interventions. This innovation presents a scalable solution for improving health information systems globally, particularly in resource-constrained environments.
ER  -
TY  - CONF
AU  - Nie, Runze
AU  - Wu, Hao
AU  - Ma, Lan
AU  - Liu, Zhenyu
AU  - Wang, Zhigang
AU  - Zhang, Ping
T1  - Towards a Conversational Invoice Issuance LLM-Based Agent
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/ACAI63924.2024.10899737
KW  - Aerodynamics;Autonomous agents;Collaboration;E-Invoice System;Finance;Large language models;Memory architecture;Natural language processing;Regulation;Security;Software;Training
N2  - The traditional invoice issuance process within tax administration is labor-intensive and prone to errors, necessitating a shift towards digitalization. Despite the advent of digital invoicing systems that streamline invoice generation and automate rule-based audits, integration with existing financial accounting systems remains a challenge. Particularly in the hospitality and bookkeeping sectors, the adoption of these systems is hindered by the lack of standardized software, high costs, and the absence of technical expertise among small and micro enterprises. The integration of digital invoicing systems with diverse financial software presents significant barriers to uniform adaptation. Furthermore, the complexity of tax regulations and the dynamic nature of tax categories require advanced understanding beyond the capabilities of standard Large Language Models (LLMs). The need for a specialized system that can comprehend finance and tax contexts, securely handle sensitive information, and adapt to user interactions is paramount. This paper introduces an autonomous agent based on a finance and tax-specific Large Language Model (LLM) designed to address the aforementioned challenges. The system includes a Specialized Training Framework to enhance domain comprehension, a Hierarchical Memory Architecture for dynamic user interaction, and a Tax Domain Security Module to ensure compliance with tax regulations. The proposed agent aims to improve the efficiency and accuracy of the invoice issuance process, providing a robust solution for tax administration in the digital era.
ER  -
TY  - CONF
AU  - Nikeghbal, N.
AU  - Kargaran, A. H.
AU  - Heydarnoori, A.
T1  - GIRT-Model: Automated Generation of Issue Report Templates
SP  - 407–418
EP  - 407–418
KW  - Bug Report;Bug Template;Codes;Data mining;GitHub;Issue Report Template;Issue Template;Issue Template Generation;Issue Tracker;Meteors;Software;Software development management
N2  - Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model.In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.CCS CONCEPTS• Software and its engineering → Software notations and tools; Software creation and management
ER  -
TY  - CONF
AU  - de Oliveira, B.
AU  - Castor, F.
T1  - AthenaLLM: Supporting Experiments with Large Language Models in Software Development
SP  - 69–73
EP  - 69–73
KW  - Adaptation models;Codes;Empirical Software Engineering;Experimentation;Large language models;Manuals;MIMICs;Protocols;Scalability;Software development;Visualization
N2  - Existing studies on the use of Large Language Models (LLMs) in software development leverage methodologies that limit their scalability and require intensive manual data collection and analysis, for example, due to the use of video data or think-aloud protocols. We propose the use of a specialized tool capable of automatically collecting fine-grained, relevant data during experiments and case studies. It enables researchers to understand for example how often participants accept or reject suggestions made by LLMs and what kinds of prompts are more likely to trigger accepted suggestions, even in studies targeting a large number of participants. We implement this idea as a Visual Studio Code plugin named AthenaLLM1. It mimics the functionalities of GitHub Copilot and offers seamless integration with OpenAI API models like GPT-4 and GPT-3.5, and compatibility with other models providing an OpenAI-compatible API, e.g., Vicuna [6]. It automatically collects data at a fine level of granularity and covers both the interactions of developers with their IDE, e.g., all changes made in the code, and the products of such interactions, e.g., the generated code, when accepted. Thus, the proposed approach also reduces bias that the experimental process itself may introduce, e.g., due to the need for participants to verbalize their thoughts. In this paper we discuss how AthenaLLM could enable researchers to go both broader (in terms of number of participants) and deeper (in terms of the kinds of research questions that can be tackled).1Extension available at https://github.com/nandooliveira/athena\_llm\_extension
ER  -
TY  - CONF
AU  - Olmez, M. M.
AU  - Gehringer, E.
T1  - Automation of Test Skeletons Within Test-Driven Development Projects
SP  - 1–10
EP  - 1–10
M3  - https://doi.org/10.1109/CSEET62301.2024.10663016
KW  - AI tool;Human computer interaction;Human-machine interaction;Human-machine systems;Maintenance engineering;programming;Software algorithms;Source coding;Test Generation;Test skeletons;Test-driven development;Writing
N2  - In addressing the need for test case generation in software projects and the validation and repair processes, various algorithms and AI models are increasingly being applied with novel approaches. On the other hand, despite the established effectiveness of the Test-Driven Development (TDD) approach in testing and development, there is still a lack of research examining the impact of human-machine interaction on software validation and coding. This paper introduces a tool, the test-skeleton generator, which utilizes an OpenAI model to generate test skeletons. These skele-tons include test names, signatures, and scenario descriptions, omitting the actual test bodies. To explore the implications of this tool, an empirical experiment involving student participation was conducted to assess the conversion of test skeletons into functional tests with human-machine interaction. The study reveals significant insights, indicating that human-machine interaction plays a crucial role in shaping both the testing and programming phases, encouraging students to prioritize writing tests before modifying source code. Teams adopting this approach demonstrate a tendency to produce more tests, leading to higher code coverage. Additionally, our research underscores the growing potential of AI language models to generate tests that closely resemble those written by human developers. Notably, human-machine interaction has proven its significant positive impact on the validation and repair process of AI -generated tests.
ER  -
TY  - CONF
AU  - Mishra, Prabhat Kumar
AU  - Arulappan, Arun Kumar
AU  - Ra, In-Ho
AU  - Thanga Mariappan, L.
AU  - Gina Rose, G.
AU  - Lee, Young-Seok
T1  - AI-Driven Virtual Mock Interview Development
SP  - 1–4
EP  - 1–4
M3  - https://doi.org/10.1109/SCISISIS61014.2024.10760210
KW  - ADA 2 Embeddings;Advanced Artificial Intelligence;CAC;Costs;Educational technology;Interviews;Large language models;Learning (artificial intelligence);Machine Learning;Machine Learning Innovations;Mirrors;Mock Interview;Natural language processing;Python Programming;Sentiment analysis;Standards;Transforms;Virtual Mock Interviews
N2  - The integration of Artificial Intelligence (AI) into educational technologies marks a significant shift in learning methodologies and operational dynamics within educational institutions. At the forefront is an AI-driven virtual mock interview platform designed to address the high Customer Acquisition Costs (CAC) in the edtech sector, especially for interview preparation services. This initiative harnesses a blend of AI technologies, including ADA 2 for creating context-aware embeddings and Machine Learning (ML), to transform the traditional mock interview process into a dynamic, cost-effective system. Central to the platform is its use of advanced Natural Language Processing (NLP) techniques and GPT-4 Large Language Model (LLM), automating the process of mock interviews and providing personalized feedback, ensuring a preparation journey that meets specific candidate needs and mirrors real interview scenarios. A key evaluation among 100 students from a cohort of 1800 demonstrated a 90% cost reduction for three mock interviews, reducing expenses from ₹3000 to just ₹300 per candidate. This cost efficiency significantly enhances access to quality interview preparation, improving student satisfaction and accessibility. Moreover, the platform provides valuable insights into student performance, setting a new standard in educational technology by offering an effective, personalized interview preparation experience. This project reflects a holistic approach to student development and the critical role of technology in addressing the evolving needs of learners
ER  -
TY  - CONF
AU  - Mahmud, Tarek
AU  - Duan, Bin
AU  - Che, Meiru
AU  - Ngu, Anne
AU  - Yang, Guowei
T1  - Testing Android Third Party Libraries with LLMs to Detect Incompatible APIs
SP  - 280–291
EP  - 280–291
M3  - https://doi.org/10.1109/Forge66646.2025.00039
KW  - Android;Ecosystems;Foundation models;Incompatible APIs;Large Language Model;Large language models;Libraries;Object recognition;Software engineering;Test Generation;Test pattern generators;Testing;Third-party libraries
N2  - Third-party libraries (TPLs) are an integral part of Android app development, offering app developers essential tools for enhancing app functionality, design, and integration capabilities. However, the fast-paced evolution of Android APIs introduces compatibility issues not only in Android apps but also in TPLs as they rely heavily on these Android APIs too. These challenges necessitate continuous updates and compatibility checks to maintain apps as well as TPL's compatibility across Android's diverse ecosystem. Prior research primarily focused on detecting compatibility issues induced by native Android APIs in Android apps falling short in detecting the incompatible APIs associated with TPLs due to the additional layer of abstraction they introduce as well as the obfuscation used by the TPL developers.In this paper, we propose LibCT that leverages a pre-trained Large Language Model (LLM), GPT-4, for detecting incompatible APIs in Android TPLs. It leverages GPT-4 to generate tests on TPL API usages and executes them across a wide range of Android devices available in the Amazon Device Farm. In our experimental evaluation, we tested 312 libraries with 12,831 APIs on 86 devices in the Amazon device farm, which revealed 274 incompatible APIs, highlighting the LibCT's capability in identifying both evolution-induced and device-specific compatibility issues.
ER  -
TY  - CONF
AU  - Sharma, Siddharth
AU  - Tiwari, Divyan
AU  - Garg, Avaneesh
AU  - Kaushal, Abhishek
AU  - Mezina, Anzhelika
AU  - Frolka, Jakub
AU  - Kishore Dutta, Malay
T1  - Enhancing Plant Disease Detection with CNNs and LLMs: A Comprehensive Approach to Diagnosis and Mitigation
SP  - 13–18
EP  - 13–18
N2  - This study presents a novel approach to plant disease detection by integrating a Convolutional Neural Network (CNN) with an open-source Language Model (LLM) within a userfriendly web application. From a custom-built dataset assembled using open access sources, consisting of 48 classes representing various plant diseases and healthy specimens, the CNN model achieves an impressive accuracy of 99.73% on the test set and application tests reveal high precision of the model. The framework employs a robust experimental setup, including meticulous data partitioning and hyperparameter tuning, to ensure effective model training and evaluation. While CNN demonstrates exceptional performance in detecting wellrepresented diseases, challenges in accurately classifying underrepresented classes are identified, emphasizing the need for data augmentation strategies to enhance model robustness. The integrated LLM enhances user interaction by providing real-time insights and actionable recommendations based on CNN predictions, making the tool accessible to users with varying agricultural expertise. Further work aims to refine the system through dataset expansion and advanced training techniques, ultimately positioning this tool as an asset for sustainable agricultural practices.
ER  -
TY  - CONF
AU  - Siddiq, M. L.
AU  - Roney, L.
AU  - Zhang, J.
AU  - Santos, J. C. S.
T1  - Quality Assessment of ChatGPT Generated Code and their Use by Developers
SP  - 152–156
EP  - 152–156
KW  - Chatbots;ChatGPT;Codes;Data mining;datasets;open-coding;programming;pull-request;quality;Quality assessment;Security;Software performance
N2  - The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT's generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT's responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python & Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers' discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits.CCS CONCEPTS• Software and its engineering → Software performance; Software usability; Empirical software validation.
ER  -
TY  - JOUR
AU  - Wu, F.
AU  - Zhao, G.
AU  - Li, T.
AU  - Shen, J.
AU  - Qian, X.
T1  - Improving Conversational Recommendation System Through Personalized Preference Modeling and Knowledge Graph
JO  - IEEE Transactions on Knowledge and Data Engineering
Y1  - 2024
VL  - 36
IS  - 12
SP  - 8529–8540
EP  - 8529–8540
M3  - https://doi.org/10.1109/TKDE.2024.3421580
KW  - Accuracy;Conversational recommendation system;dialogue generation;History;Knowledge graphs;Motion pictures;Oral communication;personalized recommendations;Recommender systems;Task analysis
N2  - Conversational recommendation systems (CRS) can actively discover users' preferences and perform recommendations during conversations. The majority of works on CRS tend to focus on a single conversation and dig it using knowledge graphs, language models, etc. However, they often overlook the abundant and rich preference information that exists in the user's historical conversations. Meanwhile, end-to-end generation of recommendation results may lead to a decrease in recommendation quality. In this work, we propose a personalized conversational recommendation system infused with historical interaction information. This framework leverages users' preferences extracted from their historical conversations and integrates them with the users' preferences in current conversations. We find that this contributes to higher accuracy in recommendations and fewer recommendation turns. Moreover, we improve the interactive pattern between the recommendation module and the dialogue generation module by utilizing the slot filling method. This enables the results inferred by the recommendation module to be integrated into the conversation naturally and accurately. Our experiments on the benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art methods in the evaluation of recommendations and dialogue generation.
ER  -
TY  - CONF
AU  - Wu, Liangxuan
AU  - Zhao, Yanjie
AU  - Wang, Chao
AU  - Liu, Tianming
AU  - Wang, Haoyu
T1  - A First Look at LLM-powered Smartphones
SP  - 208–217
EP  - 208–217
KW  - Conferences;Ethics;Industries;Large language models;Privacy;Security;Smart phones;Software engineering;Technological innovation;User experience
N2  - The integration of Large Language Models (LLMs) into edge devices such as smartphones represents a significant leap in mobile technology, promising enhanced user experiences and novel functionalities. This paper presents a first look at LLM-powered smartphones, addressing four key aspects: the current market landscape, core functions enabled by integrated LLMs, potential security risks, and user perceptions. The findings reveal a rapidly evolving market with major manufacturers competing to integrate LLMs, innovative features that improve user interaction, significant security challenges, and mixed user perceptions that balance enthusiasm for new capabilities with privacy concerns. This study contributes to understanding LLM integration in mobile devices and its implications for users, manufacturers, and the broader technological landscape.
ER  -
TY  - CONF
AU  - Xia, Yuchen
AU  - Dittler, Daniel
AU  - Jazdi, Nasser
AU  - Chen, Haonan
AU  - Weyrich, Michael
T1  - LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins
SP  - 1–4
EP  - 1–4
M3  - https://doi.org/10.1109/ETFA61755.2024.10710900
KW  - Cognitive load;Decision making;Digital Twin;Digital twins;Intelligent Automation;Large language models;Load modeling;Manufacturing automation;Multi-Agent System;Multi-agent systems;Simulation;Software development management;Usability;Visualization
N2  - This paper presents a novel design of a multi-agent system framework that applies large language models (LLMs) to automate the parametrization of simulation models in digital twins. This framework features specialized LLM agents tasked with observing, reasoning, decision-making, and summarizing, enabling them to dynamically interact with digital twin simulations to explore parametrization possibilities and determine feasible parameter settings to achieve an obj ective. The proposed approach enhances the usability of simulation model by infusing it with knowledge heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos and codes are available at a GitHub Repository: https://github.comlYuchenXia/LLMDrivenSimulation
ER  -
TY  - CONF
AU  - Xu, Y.
AU  - Feng, J.
AU  - Miao, W.
T1  - Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models
SP  - 204–215
EP  - 204–215
M3  - https://doi.org/10.1109/QRS62785.2024.00029
KW  - Accuracy;Formal Specification;Industries;Large language models;Logic;Natural language processing;Prompt engineering;Requirements engineering;Software quality;Software reliability;Training
N2  - Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4% accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27% to 78% under interactive prompt evolution.
ER  -
TY  - CONF
AU  - Yabaku, Mounika
AU  - Pombo, Nuno
AU  - Ouhbi, Sofia
T1  - Exploring the Potential Use of Generative AI in Software Engineering Education
SP  - 1–7
EP  - 1–7
M3  - https://doi.org/10.1109/AICT61888.2024.10740416
KW  - AIDriven Educational Tools;Debugging;Education;Generative AI;Large language models (LLMs);Pedagogical Innovation;Requirements engineering;Software;Software development management;Software engineering;Software Engineering Education;Software measurement;Testing;Usability
N2  - The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies.
ER  -
TY  - CONF
AU  - Yang, Xixuan
AU  - Jia, Tong
AU  - Li, Ying
AU  - Huang, Gang
T1  - IFKG: An Intelligent Fault Diagnosis Tool with Knowledge Graph and Generative LLM
SP  - 839–843
EP  - 839–843
M3  - https://doi.org/10.1109/SANER64311.2025.00088
KW  - Fault diagnosis;Human computer interaction;knowledge graph;Knowledge graphs;Large language models;LLM;Natural languages;Software development management;software fault diagnosis;Software systems;Terminology;Video on demand;Web sites
N2  - The development of effective diagnostic methodolo-gies for software system failures is of paramount importance. Traditional methods, which rely on specialized terminology and intricate reasoning, require users to have a technical background, resulting in reduced flexibility and decreased user-friendliness. With the rise of generative large language models, optimizing human-computer interaction has become a critical area of focus. Additionally, the inherent intelligence and extensive knowledge of large language models make them both easy and effective to employ for fault diagnosis assistance. We introduce IFKG, an advanced tool for diagnosing software system failures. IFKG integrates generative large language models with knowledge graphs, employing natural language interactions to implement fault detection and deliver solutions. IFKG enables users to upload descriptive problems, retrieve pertinent information from the knowledge graph, and present diagnostic results in natural language. Our accuracy assessments across diverse software system failures indicate that the IFKG provides targeted and actionable recommendations, effectively assisting users in ad-dressing a range of software system issues. The tool is available on GitHub at https://github.com/mako-xxlIFKG, and the demo video can be found on YouTube: https://youtu.belDie2vgZm2hk.
ER  -
TY  - CONF
AU  - Yu, S.
AU  - Fang, C.
AU  - Ling, Y.
AU  - Wu, C.
AU  - Chen, Z.
T1  - LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities
SP  - 206–217
EP  - 206–217
M3  - https://doi.org/10.1109/QRS60937.2023.00029
KW  - Automation;Behavioral sciences;ChatGPT;Large Language Model;Mobile App Testing;Mobile applications;Software quality;Software reliability;Software testing;Test Generation;Test Migration;User interfaces
N2  - This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs' capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.
ER  -
TY  - CONF
AU  - Yuan, Mingyue
AU  - Chen, Jieshan
AU  - Xing, Zhenchang
AU  - Quigley, Aaron
AU  - Luo, Yuyu
AU  - Luo, Tianqi
AU  - Mohammadi, Gelareh
AU  - Lu, Qinghua
AU  - Zhu, Liming
T1  - DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models
SP  - 2483–2494
EP  - 2483–2494
M3  - https://doi.org/10.1109/ICSE55347.2025.00109
KW  - Codes;Design Guideline;Design methodology;Frontend Code Repair;Knowledge based systems;Large language models;Maintenance engineering;Software development management;Software engineering;System analysis and design;UI Design;Usability;User experience
N2  - The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel's v0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google's Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.
ER  -
TY  - CONF
AU  - Zaeifi, Mehdi
AU  - Lin, Beiyu
T1  - Smartphone Usage Data Cleaning Using LLM-Based Processing
SP  - 8871–8873
EP  - 8871–8873
M3  - https://doi.org/10.1109/BigData62323.2024.10825647
KW  - Accuracy;Big Data;Cleaning;Codes;Data processing;Large language models;Performance evaluation;Prompt engineering;Zero shot learning
N2  - The proliferation of smartphone technology has generated unprecedented volumes of data, creating challenges in under-standing digital behavior patterns. We present a new computational system that integrates Large Language Models (LLMs) with conventional data processing techniques using a novel three-levelOur system employs LLMs for zero-shot learning capabilities to classify usage patterns, achieving 95% accuracy in task classification through automated pattern identification. The system implements pattern verification reaching 98% validation accuracy and utilizes automated validation that reduces data loss by 75%. This hierarchical approach demonstrates consistent performance across diverse device types and usage scenarios while maintaining processing efficiency through automated prompt engineering and code generation.
ER  -
TY  - CONF
AU  - Zhang, Dingsheng
AU  - Li, Zepeng
AU  - Liang, Xueying
AU  - Chen, Gang
T1  - Research on the Intelligent Park Energy Control Platform Based on LLM
SP  - 41–45
EP  - 41–45
M3  - https://doi.org/10.1109/ICSCGE64239.2024.11064308
KW  - Accuracy;Business;Carbon;Carbon dioxide;carbon management;Digital twins;Energy consumption;energy control;Energy management;energy optimization;LLM;Market research;Optimization;Real-time systems
N2  - The Intelligent Park Energy Control Platform (IPECP) leverages Large Language Models (LLM) to advance smart park energy management, aligning with China's carbon peaking and neutrality goals. By integrating LLMs, digital twin technology, and hierarchical scheduling optimization, the IPECP processes complex, unstructured data to uncover energy usage patterns and market trends. This enables accurate predictions and optimized energy dispatch strategies, enhancing real-time monitoring and control. The platform's architecture includes a data layer for collection and processing, an intelligent analytic layer for deep insights, and a user interaction layer for personalized advice and support. Application results show a 15% to 20% reduction in monthly energy consumption and over 50% reduction in operational carbon emissions, achieving zero emissions with carbon sinks. The IPECP also improves energy management efficiency through automated operations and maintenance. Future work will expand the platform's capabilities to include more renewable energy sources and develop advanced business models to promote wider adoption and support global climate efforts.
ER  -
TY  - CONF
AU  - Zhang, Shenglin
AU  - Zhu, Pengtian
AU  - Ma, Minghua
AU  - Wang, Jiagang
AU  - Sun, Yongqian
AU  - Li, Dongwen
AU  - Wang, Jingyu
AU  - Guo, Qianying
AU  - Hua, Xiaolei
AU  - Zhu, Lin
AU  - Pei, Dan
T1  - Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models
SP  - 61–66
EP  - 61–66
M3  - https://doi.org/10.1109/ISSREW63542.2024.00048
KW  - Conferences;Data mining;domain alignment;Faces;Filters;Iterative methods;Large Language Model;Large language models;Maintenance;Privacy;Protection;question answering;Software reliability
N2  - Large language models (LLMs) excel at general question-answering (Q&A) but often fall short in specialized domains due to a lack of domain-specific knowledge. Commercial companies face the dual challenges of privacy protection and resource constraints when involving LLMs for fine-tuning. This paper propose a novel framework, Self-Evolution, designed to address these issues by leveraging lightweight open-source LLMs through multiple iterative fine-tuning rounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution employ a strategy that filters and reinforces the knowledge with higher value during the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat using 4,000 documents containing rich domain knowledge from China Mobile, achieving a performance score 174% higher on domain-specific question-answering evaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat. Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days, and it improves the efficiency of locating alarms, fixing problems, and finding related reports, with an average efficiency improvement of over 18.6%. In addition, we release Self-Evolution framework code in https://github.com/Zero-Pointer/Self-Evolution.
ER  -
TY  - JOUR
AU  - Zhang, Y.
AU  - Qiu, Z.
AU  - Stol, K. J.
AU  - Zhu, W.
AU  - Zhu, J.
AU  - Tian, Y.
AU  - Liu, H.
T1  - Automatic Commit Message Generation: A Critical Review and Directions for Future Work
JO  - IEEE Transactions on Software Engineering
Y1  - 2024
VL  - 50
IS  - 4
SP  - 816–835
EP  - 816–835
M3  - https://doi.org/10.1109/TSE.2024.3364675
KW  - benchmark;Chatbots;Codes;commit message generation;Commit-based software development;Information retrieval;Machine translation;Noise measurement;open collaboration;Software maintenance;Task analysis
U1  - 1939-3520
N2  - Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of `noise'; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models `learn' inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.
ER  -
TY  - CONF
AU  - Zheng, Kaisheng
AU  - Shen, Yuanyang
AU  - Tao, Yida
T1  - Automatic Unit Test Generation for Programming Assignments Using Large Language Models
SP  - 242–252
EP  - 242–252
M3  - https://doi.org/10.1109/CSEET66350.2025.00031
KW  - Computer bugs;Large language models;programming assignments;Programming profession;Reproducibility of results;Semantics;Syntactics;Test pattern generators;Testing;Unit test generation;Usability;Writing
N2  - Programming knowledge is a crucial aspect of computer science education, and unit testing is commonly employed to automatically assess programming assignments. Instructors and teaching assistants typically invest considerable efforts in writing unit tests, which may still be vulnerable to human oversight and mistakes. In this work, we explored the feasibility of using Large Language Models (LLMs) to automate the assessment of programming assignments. In particular, we proposed two approaches: the plain approach that uses GPT-4o-mini in a vanilla setting, and the augmented approach that integrates additional strategies such as tailored prompts with syntax and semantic constraints, and a feedback mechanism with information on test-effectiveness metrics. We evaluate the two approaches on six real-world programming assignments from an introductory-level programming course at our university. Compared to the plain approach, the augmented approach improves the usability and effectiveness of the generated unit tests, reducing 85 % compilation errors while enhancing the statement coverage and mutation scores by 1.7 x and 2.1 x, respectively. In addition, the augmented approach also complements human-written tests by covering additional program behaviors. In a case study of 1296 students' submissions that pass human-written tests, the augmented approach successfully detected new bugs in 13 % submissions, with an accuracy of 27 %. These results not only demonstrate the potentials of LLMs in generating useful unit tests for programming assignments, but also highlight the strategies that can effectively enhance LLMs' capabilities to augment human-written tests, offering practical benefits for both educators and students.
ER  -
TY  - CONF
AU  - You, Boqian
T1  - Research on Multimodal Data Fusion Technology based on LLM and Attention Mechanism
SP  - 1–4
EP  - 1–4
M3  - https://doi.org/10.1109/ICHORA65333.2025.11016994
KW  - Analytical models;attention mechanism;Attention mechanisms;data fusion technology;Data integration;Data models;Feature extraction;Human computer interaction;LLM;multimodal data;Neurons;Optimization;Problem-solving;Robots
N2  - This paper studies the practical method of multimodal data fusion technology in traffic. The practical method of multimodal data fusion technology based on LLM and attention mechanism is proposed, and the analysis model of multimodal data fusion technology is constructed, and the quantitative recursive analysis model is used to analyze the constructed data model to realize the feature extraction of constrained feature data. Then, we study the practical method of traffic multi-modal data fusion technology based on LLM and attention mechanism, optimize the number of neurons in the hidden layer neural network, and complete the practical analysis of traffic multi-modal data fusion technology. The simulation experiment results show that the proposed method in this paper can effectively improve the application efficiency and application effect of traffic multi-mode data fusion technology, and provide professional talents for the development of innovative LLM and attention mechanism.
ER  -
TY  - CONF
AU  - Shuvo, Mahfuzur Rahman
AU  - Rahman, Ashifur
AU  - Akuthota, Vishwanath
AU  - Paul, Tanay
AU  - Islam, Mahfujul
AU  - Ashraf, Md Sadi
AU  - Roy, Prosenjit
AU  - Reza, Md Tanzim
T1  - A Multi-Agent Garage Service Search and Recommendation with Hybrid MLs and LLMs
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICCECE61355.2025.10940937
KW  - Garage;Generative AI;Hybrid power systems;LLMs;Measurement;MLs;MultiAgent;Multi-agent systems;Navigation;Radio frequency;Random forests;Reliability;Scalability;User experience
N2  - The automobile service industry's explosive growth highlights the need for creative approaches to boost operational effectiveness and user experience. This study introduces a Hybrid Garage Assistance System, integrating Classical Machine Learning (ML) techniques with Generative AI to optimize garage service discovery and analysis. The system employs sophisticated data processing methods, including Term Frequency-Inverse Document Frequency (TF-IDF) vectorization and regex-based service detection, to extract actionable insights from unstructured garage data.Central to the system are machine learning models Random Forest (RF) and XGBoost (XGB) which achieve high precision and recall in classifying garage services. A hybrid search mechanism, combining cosine similarity with ML-driven predictions, ensures the delivery of highly personalized search results. To further refine decision-making, the system incorporates Generative AI models such as Perplexity for web-based research, Gemini for location-specific analysis, Mistral for email sending and GPT-4 for detailed service recommendations and dall-e for creating user specific parts images. These advanced tools provide users with comprehensive information that enables them to make well-informed decisions about garage services.Performance evaluation of the system is conducted using robust metrics, including precision, recall, F1-score, and system latency. Experimental results reveal a precision of 𝟖 𝟓 %, recall of 𝟕 𝟎 . 𝟖 %, and an F1-score of 𝟕 𝟕 . 𝟐 %, demonstrating the efficacy of integrating classical ML with generative AI. The system's average latency of 5.9 seconds ensures a seamless and responsive user experience.This hybrid framework highlights the potential of blending classical ML and Large Language Models (LLMs) to enhance search and recommendation functionalities, offering a scalable and robust blueprint for future advancements in the automotive service sector. The system's Propose a Multi-Agent System With high accuracy, scalability, and reliability position it as a cutting-edge solution for users navigating the complexities of garage service selection.
ER  -
TY  - CONF
AU  - Wang, Liu
AU  - Wang, Dong
AU  - Pan, Shidong
AU  - Jiang, Zheng
AU  - Wang, Haoyu
AU  - Wang, Yi
T1  - A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements
SP  - 4210–4228
EP  - 4210–4228
M3  - https://doi.org/10.1109/SP61157.2025.00223
KW  - Data privacy;Data transparency;Mobile applications;Pipelines;Privacy;Security;Stakeholders;Systematics;Technological innovation;Usability
N2  - The prevalent engagement with mobile apps underscores the importance of understanding their data practices. Transparency plays a crucial role in this context, ensuring users to be informed and give consent before any data access occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to inform users about detailed insights into apps' data access and sharing. This feature continues Apple's trend of privacy-focused innovations (following Privacy Nutrition Labels), and has been marketed as a big step forward in user privacy. However, its real-world impacts on user privacy and control remain unexamined. We thus proposed an end-to-end study involving systematic assessment of the App Privacy Report's real-world benefits and limitations, LLM-enabled and multi-technique synthesized enhancements, and comprehensive evaluation from both system and user perspectives. Through a structured focus group study with twelve everyday iOS users, we explored their experiences, understanding, and perceptions of the feature, suggesting its limited practical impact resulting from missing important details. We identified two primary user concerns: the clarity of data access purpose and domain description. In response, we proposed enhancements including a purpose inference framework and domain clarification pipeline. We demonstrated the effectiveness and benefits of such enhancements for mobile app users. This work provides practical insights that could help enhance user privacy transparency and discusses areas for future research.
ER  -
TY  - JOUR
AU  - Voria, Gianmario
AU  - Casillo, Francesco
AU  - Gravino, Carmine
AU  - Catolino, Gemma
AU  - Palomba, Fabio
T1  - RECOVER: Toward Requirements Generation From Stakeholders' Conversations
JO  - IEEE Transactions on Software Engineering
Y1  - 2025
VL  - 51
IS  - 6
SP  - 1912–1933
EP  - 1912–1933
M3  - https://doi.org/10.1109/TSE.2025.3572056
KW  - Accuracy;automated software engineering;Conversational requirements engineering;Data mining;Documentation;Focusing;Large language models;Machine Learning;Natural language processing;Oral communication;Requirements engineering;Software engineering;Stakeholders
U1  - 1939-3520
N2  - Stakeholders' conversations requirements elicitation meetings hold valuable insights into system and client needs. However, manually extracting requirements is time-consuming, labor-intensive, and prone to errors and biases. While current state-of-the-art methods assist in summarizing stakeholder conversations and classifying requirements based on their nature, there is a noticeable lack of approaches capable of both identifying requirements within these conversations and generating corresponding system requirements. These approaches would assist requirement identification, reducing engineers' workload, time, and effort. They would also enhance accuracy and consistency in documentation, providing a reliable foundation for further analysis. To address this gap, this paper introduces RECOVER (Requirements EliCitation frOm conVERsations), a novel conversational requirements engineering approach that leverages natural language processing and large language models (LLMs) to support practitioners in automatically extracting system requirements from stakeholder interactions by analyzing individual conversation turns. The approach is evaluated using a mixed-method research design that combines statistical performance analysis with a user study involving requirements engineers, targeting two levels of granularity. First, at the conversation turn level, the evaluation measures RECOVER's accuracy in identifying requirements-relevant dialogue and the quality of generated requirements in terms of correctness, completeness, and actionability. Second, at the entire conversation level, the evaluation assesses the overall usefulness and effectiveness of RECOVER in synthesizing comprehensive system requirements from full stakeholder discussions. Empirical evaluation of RECOVER shows promising performance, with generated requirements demonstrating satisfactory correctness, completeness, and actionability. The results also highlight the potential of automating requirements elicitation from conversations as an aid that enhances efficiency while maintaining human oversight.
ER  -
TY  - CONF
AU  - Sinha, Nishchay
AU  - Trivedi, Raghav
AU  - Mittapalli, Sanjit
AU  - Parihar, Anuj
AU  - Selvanambi, Ramani
T1  - Targeting and Automating Recoveries from Cybersecurity Vulnerabilities Using Large Language Models
SP  - 1369–1373
EP  - 1369–1373
M3  - https://doi.org/10.1109/IC3ECSBHI63591.2025.10990905
KW  - Codes;Computational modeling;Computer security;Cybersecurity;Deep Learning;Generative AI;Large language models;Large Language Models (LLM);Maintenance engineering;Pipelines;Python;Real-time systems;Software;Threat Intelligence;User experience
N2  - There is a growing need to develop more robust, automated methods to check for vulnerabilities in code and software. Large language models, due to their ability to engage with a wide variety of scenarios, provide a personalized framework for users to safeguard their devices. This paper focuses on using a LLaMa3.2 3B model as its base, pretrained on the DiverseVul dataset consisting of vulnerable code snippets in programming languages such as C, C++, and Python. Using QLoRa for PEFT-based fine-tuning, the model achieved an Accuracy of 93.75%, outperforming other models. The CLI application provides a seamless user experience by pinpointing user directories for vulnerability scanning and classification. Despite its promising results, the study identifies limitations, including challenges with obfuscated code, computational resource constraints, and generalization issues. This research contributes to advancing real-time detection capabilities and scalable solutions for cybersecurity.
ER  -
TY  - CONF
AU  - Sladić, M.
AU  - Valeros, V.
AU  - Catania, C.
AU  - Garcia, S.
T1  - LLM in the Shell: Generative Honeypots
SP  - 430–435
EP  - 430–435
M3  - https://doi.org/10.1109/EuroSPW61312.2024.00054
KW  - Computer security;honeypots;Large language models;Linux;shelLM;Software;Source coding
N2  - Honeypots are essential tools in cybersecurity for early detection, threat intelligence gathering, and analysis of attacker's behavior. However, most of them lack the required realism to engage and fool human attackers long-term. Being easy to distinguish honeypots strongly hinders their effectiveness. This can happen because they are too deterministic, lack adaptability, or lack deepness. This work introduces shelLM, a dynamic and realistic software honeypot based on Large Language Models that generates Linux-like shell output. We designed and implemented shelLM using cloud-based LLMs. We evaluated if shelLM can generate output as expected from a real Linux shell. The evaluation was done by asking cybersecurity researchers to use the honeypot and give feedback if each answer from the honeypot was the expected one from a Linux shell. Results indicate that shelLM can create credible and dynamic answers capable of addressing the limitations of current honeypots. ShelLM reached a TNR of 0.90, convincing humans it was consistent with a real Linux shell. The source code and prompts for replicating the experiments have been publicly available.
ER  -
TY  - CONF
AU  - Strand, Aleksander Theo
AU  - Gautam, Sushant
AU  - Midoglu, Cise
AU  - Halvorsen, Pål
T1  - Demo: Soccer Information Retrieval Via Natural Queries using SoccerRAG
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/CBMI62980.2024.10859233
KW  - association football;Data mining;Databases;Feature extraction;Information retrieval;Large language models;Natural language processing;Natural languages;Pipelines;Retrieval augmented generation;Sports;Structured Query Language;UI;Visualization
N2  - The rapid evolution of digital sports media necessitates sophisticated information retrieval systems that can efficiently parse extensive multimodal datasets. This paper demonstrates SoccerRAG, an innovative framework designed to harness the power of Retrieval Augmented Generation (RAG) and Large Language Models (LLMs) to extract soccer-related information through natural language queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic querying and automatic data validation, enhancing user interaction and accessibility to sports archives. We present a novel interactive user interface (UI) based on the Chainlit framework which wraps around the core functionality, and enable users to interact with the SoccerRAG framework in a chatbot-like visual manner.
ER  -
TY  - CONF
AU  - Sunico, R. J.
AU  - Pachchigar, S.
AU  - Kumar, V.
AU  - Shah, I.
AU  - Wang, J.
AU  - Song, I.
T1  - Resume Building Application based on LLM (Large Language Model)
SP  - 486–492
EP  - 486–492
M3  - https://doi.org/10.1109/ICCCIS60361.2023.10425602
KW  - Buildings;Engineering profession;LLM (Large Language Model);Prompt engineering;resume building;Resumes;Social networking (online);Standards;Usability;User interfaces
N2  - Amid the highly competitive job market, creating an effective resume is vital but often difficult, particularly for students from underprivileged backgrounds with limited career development support. To mitigate this, we introduce a novel resume building application that employs the Large Language Model (LLM) to aid students in composing their first resumes. The application comprises three modules: Resume Generation, Resume Assessment, and User I/O. The Resume Generation module utilizes prompt engineering to produce resume bullet points, while the Resume Assessment module evaluates these bullet points for potential enhancements. The User I/O module simplifies user interaction by accepting free-style plain English as input and displaying the generated bullet points as suggestions. We have developed a prototype application that demonstrates the effectiveness of these functionalities while emphasizing ease-of-use. We have also confirmed that the content generated for resumes adheres to the benchmarks of high-quality standards. As future work, we aim to carry out usability testing with real students to further evaluate the application's utility in educational environments.
ER  -
TY  - CONF
AU  - Swamy, S. Sriramana
AU  - Alanssari, Ali Ihsan
AU  - Adel, Abual-Hass
AU  - Al-Hussein, Rouaida Kadhim A.
AU  - Manipreeth, Macha
AU  - Nithin, Vislavath
T1  - AI PDF's using LLMA Model: Time Saving Document Analysis
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/ARIIA63345.2024.11051535
KW  - Adaptation models;Artificial intelligence;Databases;Google Auth;Lang Chain;Large language models;LLMA Model;Portable document format;PostgreSQL Database;Software;Stripe;Text analysis;Training;User interfaces;Vector Database;Vectors
N2  - AI PDF is a smart and simple to use program for conversing with documents. Many people use it for interaction with documents for work, study and life management purposes. AI PDF is a tool of analysis chatbot which enables precise extraction of insight from uploaded PDF and other documents into it. AI PDF as another AI tool for interacting with files. Yes, you can pose the document questions to help you summarize the document, learn more information, and overall get a better understanding of the document without a necessity to read it. This software is a perfect fit for students studying new skills, theories, projects, and other stuff. You can save lots of time of reading entire PDFs by just AI summarizing main points for you. Users can operate the interface and upload the documents easily due to its user-friendly nature. This leaves you able to add PDFs respectively and start chatting to which is weird at first but gets used to quickly so one can easily see where the sourcing is utilized in a document which is good for proofing. The present AI PDF works with LLMA (Large Language Model Algorithm) AI would quickly search for information and give you a succinct summary. Thus, without spending time on reaching the very end of the document, you are able to grasp its main idea in general, this issue can be solved even though you scanning pdfs.
ER  -
TY  - CONF
AU  - Tanaka, H.
AU  - Ide, M.
AU  - Yajima, J.
AU  - Onodera, S.
AU  - Munakata, K.
AU  - Yoshioka, N.
T1  - Taxonomy of Generative AI Applications for Risk Assessment
SP  - 288–289
EP  - 288–289
KW  - Atmospheric modeling;Computational modeling;Ethics;Generative AI;language models;responsible AI;responsible innovation;risk assessment;Risk management;Surveys;Taxonomy;technology risks
N2  - The superior functionality and versatility of generative AI have raised expectations for the improvement of human society and concerns about the ethical and social risks associated with the use of generative AI. Many previous studies have presented risk issues as concerns associated with the use of generative AI, but since most of these concerns are from the user's perspective, they are difficult to lead to specific countermeasures. In this study, the risk issues presented by the previous studies were broken down into more detailed elements, and risk factors and impacts were identified. In this way, we presented information that leads to countermeasure proposals for generative AI risks.CCS CONCEPTS• General and reference→Evaluation; Surveys and overviews, • Human-centered computing→HCI theory, concepts and models; • Social and professional topics→Computing / technology policy.
ER  -
TY  - CONF
AU  - Tao, Chunliang
AU  - Fan, Xiaojing
AU  - Yang, Yahe
T1  - Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation
SP  - 628–634
EP  - 628–634
M3  - https://doi.org/10.1109/CAIT64506.2024.10962957
KW  - Accuracy;API Classification;API Management;Business;Dataset Generation;Large language models;Large language models (LLMs);Natural language processing;Natural Language Processing (NLP);Software;Synthetic data;Systematics;Testing;Transforms;Translation
N2  - As Large Language Models (LLMs) advance in natural language processing, there is growing interest in leveraging their capabilities to simplify software interactions. In this paper, we propose a novel framework that integrates LLMs for both classifying natural language inputs into corresponding API calls and automating the creation of sample datasets tailored to specific API functions. By classifying natural language commands, our work allows users to invoke complex software functionalities through simple inputs, improving interaction efficiency and lowering the barrier to software utilization. Our dataset generation approach also enables the efficient and systematic evaluation of different LLMs in classifying API calls, offering a practical tool for developers or business owners to assess the suitability of LLMs for customized API management. We conduct experiments on several prominent LLMs using generated sample datasets for various API functions. The results show that GPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B performs much worse at 0.759. These findings highlight the potential of LLMs to transform API management and validate the effectiveness of our framework in guiding model testing and selection across diverse applications.
ER  -
TY  - CONF
AU  - Wahba, Karim Ahmed
AU  - Amr Ahmed, Khaled
AU  - Kamel, Martina Raafat
AU  - Fathy, Marwan
AU  - Abdelfatah, Pr. Khaled Hussien
AU  - Hatem, Sarah
T1  - Creating a Digital Human Twin: Cloning Voice, Face, and Attitude
SP  - 199–205
EP  - 199–205
M3  - https://doi.org/10.1109/MIUCC62295.2024.10783609
KW  - Deep Learning;Large language models;Mobile applications;Reinforcement learning;Solid modeling;Speech recognition;Three-dimensional displays;Training;User experience;Virtual assistants
N2  - This paper presents and explores the development of digital human twin through mobile app integrated with chat-bot and aimed to deliver dynamic and realistic user interactions. By utilizing advanced technologies such as reinforcement learning, lip-sync animation, and facial expression modeling. This project aims to develop a personal virtual assistant using various technologies. The system integrate various deep learning models such as 3D avatar generation, speech recognition, clone voice from the user, Large Language Model (LLM), 3D avatar after uploading images from the user, and engages in lifelike human conversation. In the evaluation of the system there's a potential to improve user experience. Moreover, the paper also includes a review of related datasets, highlighting the capabilities and challenges of creating a realistic virtual avatar.
ER  -
TY  - CONF
AU  - Tao, XueHong
AU  - Miao, Yuan
AU  - Wang, Guanhua
T1  - Rational Intelligence Model: Overcoming Data Handling Limitations in LLMs
SP  - 519–523
EP  - 519–523
M3  - https://doi.org/10.1109/ICARCV63323.2024.10821608
KW  - Accuracy;ChatGPT;Cognition;Costs;Data models;Data processing;Error analysis;expert system;Large Language Model;Large language models;Process Automation;Rational Intelligence;Software;Software engineering;Solid modeling;SQL
N2  - Many human-computer interactions and automated processes are dependent on data. We need to correctly store data and retrieve data to support these processes. Large Language Models (LLMs) have made significant progress in comprehension and reasoning. However, we have found that their ability to handle professional data is weak, which significantly limits their applications in automated processes and professional applications. These scenarios often involve a large number of similar data entries, which are challenging for LLMs such as ChatGPT-4 Omni. In this paper, we propose a Rational Intelligence Model that comprehends human experts' knowledge of data structure and process requirements of data, automatically extracts data from conversations with end users, and effectively stores the data and retrieves it for supporting the interaction and process. Experiments show that ChatGPT-4o can achieve 0% error in simple queries, 2.6% errors when data entries are out of order, and 38% errors when the queries are reasonably complex. However, with our proposed Rational Intelligence Model (RIM), we can achieve 0% error rate in all tests. RIM fundamentally changes software engineering and expert system development approaches. Instead of having a software engineer understand expert knowledge of data processing, this is now achieved by RIM, which means it is much more flexible, lower in cost, and requires much less development time.
ER  -
TY  - CONF
AU  - Valenzuela-Toledo, Pablo
AU  - Wu, Chuyue
AU  - Hernández, Sandro
AU  - Boll, Alexander
AU  - Machacek, Roman
AU  - Panichella, Sebastiano
AU  - Kehrer, Timo
T1  - Explaining GitHub Actions Failures with Large Language Models: Challenges, Insights, and Limitations
SP  - 286–297
EP  - 286–297
M3  - https://doi.org/10.1109/ICPC66645.2025.00037
KW  - CI/CD;Cognition;Focusing;GitHub Action Run Failure Explanation;GitHub Actions;Large language models;Manuals;Productivity;Software;Software development management;Stability analysis;Testing;Usability
N2  - GitHub Actions (GA) has become the de facto tool that developers use to automate software workflows, seamlessly building, testing, and deploying code. Yet when GA fails, it disrupts development, causing delays and driving up costs. Diagnosing failures becomes especially challenging because error logs are often long, complex and unstructured. Given these difficulties, this study explores the potential of large language models (LLMs) to generate correct, clear, concise, and actionable contextual descriptions (or summaries) for GA failures, focusing on developers' perceptions of their feasibility and usefulness. Our results show that over 80 % of developers rated LLM explanations positively in terms of correctness for simpler/small logs. Overall, our findings suggest that LLMs can feasibly assist developers in understanding common GA errors, thus, potentially reducing manual analysis. However, we also found that improved reasoning abilities are needed to support more complex CI/CD scenarios. For instance, less experienced developers tend to be more positive on the described context, while seasoned developers prefer concise summaries. Overall, our work offers key insights for researchers enhancing LLM reasoning, particularly in adapting explanations to user expertise.
ER  -
TY  - CONF
AU  - Vijayvargiya, Sanidhya
AU  - Saad, Mootez
AU  - Sharma, Tushar
T1  - Enhancing Identifier Naming Through Multi-Mask Fine-Tuning of Language Models of Code
SP  - 71–82
EP  - 71–82
M3  - https://doi.org/10.1109/SCAM63643.2024.00017
KW  - Codes;Large language models;Predictive models;Software development management;Software maintenance;Source coding;Surveys;Training;Transformers;Usability
N2  - Code readability strongly influences code compre-hension and, to some degree, code quality. Unreadable code makes software maintenance more challenging and is prone to more bugs. To improve the readability, using good identifier names is crucial. Existing studies on automatic identifier re-naming have not considered aspects such as the code context. Additionally, prior research has done little to address the typical challenges inherent in the identifier renaming task. In this paper, we propose a new approach for renaming identifiers in source code by fine-tuning a transformer model. Through the use of perplexity as an evaluation metric, our results demonstrate a significant decrease in the perplexity values for the fine-tuned approach compared to the baseline, reducing them from 363 to 36. To further validate our method, we conduct a developers' survey to gauge the suitability of the generated identifiers, comparing original identifiers with identifiers generated with our approach as well as two state-of-the-art large language models, GPT-4 Turbo and Gemini Pro. Our approach generates better identifier names than the original names and exhibits competitive performance with state-of-the-art commercial large language models. The proposed method carries significant implications for software developers, tool vendors, and researchers. Software developers may use our proposed approach to generate better variable names, increasing the clarity and readability of the software. Researchers in the field may use and build upon the proposed approach for variable renaming.
ER  -
TY  - JOUR
AU  - Villagrán, I.
AU  - Hernández, R.
AU  - Schuit, G.
AU  - Neyem, A.
AU  - Fuentes-Cimma, J.
AU  - Miranda, C.
AU  - Hilliger, I.
AU  - Durán, V.
AU  - Escalona, G.
AU  - Varas, J.
T1  - Implementing Artificial Intelligence in Physiotherapy Education: A Case Study on the Use of Large Language Models (LLM) to Enhance Feedback
JO  - IEEE Transactions on Learning Technologies
Y1  - 2024
VL  - 17
SP  - 2025–2036
EP  - 2025–2036
M3  - https://doi.org/10.1109/TLT.2024.3450210
KW  - Artificial intelligence;Feedback;generative artificial intelligence (AI);health science education;Large language models;Large language models (LLMs);Logic gates;procedural skills;Reviews;Task analysis;technology-enhanced learning;Training;Tutorials
N2  - This article presents a controlled case study focused on implementing and using generative artificial intelligence, specifically large language models (LLMs), in physiotherapy education to assist instructors with formulating effective technology-mediated feedback for students. It outlines how these advanced technologies have been integrated into an existing feedback-oriented platform to guide instructors in providing feedback inputs and establish a reference framework for future innovations in practical skills training for health professions education. Specifically, the proposed solution uses LLMs to automatically evaluate feedback inputs made by instructors based on predefined and literature-based quality criteria and generates actionable textual explanations for reformulation. In addition, if the instructor requires, the tool supports summary generation for large sets of text inputs to achieve better student reception and understanding. The case study describes how these features were integrated into the feedback-oriented platform, how their effectiveness was evaluated in a controlled setting with documented feedback inputs, and the results of its implementation with real users through cognitive walkthroughs. Initial results indicate that this innovative implementation holds great potential to enhance learning and performance in physiotherapy education and has the potential to expand to other health disciplines where the development of procedural skills is critical, offering a valuable tool to assess and improve feedback based on quality standards for effective feedback processes. The cognitive walkthroughs allowed us to determine participants' usability decisions in the face of these new features and to evaluate the perceived usefulness, how this would integrate into their workload, and their opinion regarding the potential for the future within this teaching strategy. This article concludes with a discussion of the implications of these findings for practice and future research directions in this developing field.
ER  -
TY  - CONF
AU  - Virvou, M.
AU  - Tsihrintzis, G. A.
T1  - Is ChatGPT Beneficial to Education? A Holistic Evaluation Framework Based on Intelligent Tutoring Systems
SP  - 1–8
EP  - 1–8
M3  - https://doi.org/10.1109/IISA59645.2023.10345949
KW  - AI in Education;Chatbots;ChatGPT;Cognition;Education;Educational Evaluation Frameworks;educational software;e-learning;Ethics;Generative AI;Intelligent Tutoring Systems;Knowledge based systems;Large language models;Stakeholders
N2  - The recent launch of ChatGPT by OpenAI has created a profound global impact, initiating deep questions among educators about how it might affect education, syllabi and teaching methods. Currently, the full scope of potential benefits and risks associated with ChatGPT in education remains unclear, given that its impact surpasses the level of preparation educators and institutions may have had for such a pre-trained generative AI tool. While Artificial Intelligence in Education has long been a subject of research, with a particular focus on developing Intelligent Tutoring Systems, the emergence of ChatGPT marks a distinctive advancement in this field. Unlike dedicated Intelligent Tutoring Systems, ChatGPT is readily available to a diverse spectrum of educational stakeholders, including teachers, students, schools, universities, and educational institutions. Scholars have initiated assessments of ChatGPT's effectiveness across various educational disciplines, even though ChatGPT was not explicitly designed for educational purposes. However, the widespread accessibility of ChatGPT, coupled with its extensive knowledge base, necessitates the development of comprehensive evaluation frameworks. In this paper, we introduce a holistic evaluation framework tailored for ChatGPT. This framework takes into account both soft and hard skills, and it is designed to seamlessly incorporate ChatGPT into Intelligent Tutoring Systems, making it suitable for a wide range of educational fields. By establishing a connection between ITS and ChatGPT, as they are both AI tools, we can benefit from the substantial background work achieved by previous research in ITSs to evaluate the educational influence of ChatGPT.
ER  -
TY  - CONF
AU  - Virvou, M.
AU  - Tsihrintzis, G. A.
T1  - Pre-made Empowering Artificial Intelligence and ChatGPT: The Growing Importance of Human AI-Experts
SP  - 1–8
EP  - 1–8
M3  - https://doi.org/10.1109/IISA59645.2023.10345880
KW  - AI-Empowered Software Engineering;Artificial intelligence;Chatbots;ChatGPT;e-learning Requirements Engineering in AI;Ethics;Generative AI;Human-centered AI;Industries;Intelligent Information Systems;Large language models (LLMs);Medical services;Requirements engineering;Responsible Artificial Intelligence;Software;User interfaces
N2  - This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts
ER  -
TY  - CONF
AU  - de Vito, Gabriele
AU  - D'Amici, Damiano
AU  - Izzo, Fabiano
AU  - Ferrucci, Filomena
AU  - Di Nucci, Dario
T1  - LLM-Based Generation of Solidity Smart Contracts from System Requirements in Natural Language: The AstraKode Case
SP  - 170–180
EP  - 170–180
M3  - https://doi.org/10.1109/SANER64311.2025.00024
KW  - Blockchains;Code Generation;Codes;Industries;Knowledge transfer;Large Language Model;Large language models;Natural languages;No-Code Platform;Rapid prototyping;Smart contracts;Software;Usability
N2  - As blockchain technology continues to evolve, the need for accessible solutions for developing smart contracts has grown, especially for non-technical users. This paper addresses practitioners' challenges in generating Solidity smart contracts from natural language requirements within the AstraKode Blockchain no-code platform (AKB). Our goal is to lower the barrier of entry into smart contract development, making it more accessible to users with limited technical expertise. We propose three methods, i.e., Naive Generation, Augmented Generation, and Enhanced Generation, each utilizing large language models to streamline the code generation process. These methods cater to different user needs, from rapid prototyping to handling complex business scenarios, improving accessibility and usability within AKB. We demonstrate their practical relevance, potential, and limitations in addressing real-world challenges in smart contract development through empirical evaluations and practitioner feedback. Thanks to collaboration with academia and effective knowledge transfer, these methods provide innovative solutions to the challenges of smart contract generation. Furthermore, they have been integrated into AKB to enhance user services, ultimately promoting the development and deployment of secure and efficient smart contracts in the industry.
ER  -
TY  - CONF
AU  - de Vito, Gabriele
AU  - Vassallo, Gabriele
AU  - Palomba, Fabio
AU  - Ferrucci, Filomena
T1  - AGORA: An Approach for Generating Acceptance Test Cases from Use Cases
SP  - 126–133
EP  - 126–133
M3  - https://doi.org/10.1109/SEAA64295.2024.00027
KW  - automated software engineering;Collaboration;Guidelines;Industries;Large language models;Natural languages;Prompt engineering;Refining;Software development management;Software engineering;Software testing;User Acceptance Testing;Writing
N2  - This paper introduces AGORA, an innovative approach that leverages Large Language Models to automate the definition of acceptance test cases from use cases. AGORA consists of two phases that exploit prompt engineering to 1) identify test cases for specific use cases and 2) generate detailed acceptance tests cases. AGORA was evaluated through a controlled experiment involving industry professionals, comparing the effectiveness and efficiency of the proposed approach with the manual method. The results showed that AGORA can generate acceptance test cases with a quality comparable to that obtained manually but improving the process efficiency by over 90% in a fraction of the time. Furthermore, user feedback indicated high satisfaction with using the proposed approach. These findings underscore the potential of AGORA as a tool to enhance the efficiency and quality of the software testing process.
ER  -
TY  - CONF
AU  - Usha, V.
AU  - Abhinash, Nalagarla Chiru
AU  - Chowdary, Sakhamuri Nitin
AU  - Sathya, V.
AU  - Reddy, Eeda Ramakrishna
AU  - Sathiya Priya, S.
T1  - Enhanced Database Interaction Using Large Language Models for Improved Data Retrieval and Analysis
SP  - 1302–1306
EP  - 1302–1306
M3  - https://doi.org/10.1109/ICoICI62503.2024.10696623
KW  - Data Retrieval;Databases;Deep Learning;Distributed databases;Gemini API;Generative AI;Google LLM;Internet of Things;Large Language Model;Large language models;Machine Learning;Natural language processing;Prompt engineering;Python;SQLQuery Generation;Structured Query Language;Syntactics;Text to SQL Generation;User experience
N2  - One of the difficult task for many users on SQL is to write the SQL Query due to its syntax and structure. If a person needs to query a database, they should know everything about how data is distributed and what the internal dependencies are. For this reason, it is not easy for everyone to access data in database without proper knowledge. This paper presents a novel application that leverages generative AI and natural language processing (NLP) to enable users to interact with databases using natural language queries. Built on the Gemini API, the application translates user queries into SQL queries, simplifying database interactions for non-technical users. The Python-based backend, SQLite database management, and Streamlit frontend provide a comprehensive solution for database querying and analysis. This approach democratizes data retrieval and analysis, offering automated insights and visualizations to users of all skill levels. The app also features automated data analysis, which boosts insight generation for users of all skill levels. Further, the traditional ways of querying SQL generally require specialized knowledge and are only accessible to those with technical backgrounds. When the application takes charge of the query construction process and offers data as UI elements, it can invigorate users to have a higher degree of insight into what their data actually is and explore it even more efficiently. The Python software stack combines Python for backend processing, SQLite for database management and a web-based frontend for user interaction to provide an all-encompassing database querying and analysis solution.
ER  -
TY  - CONF
AU  - Mahmud, J.
AU  - de Silva, N.
AU  - Khan, S. A.
AU  - Mostafavi, S. H.
AU  - Mansur, S. H.
AU  - Chaparro, O.
AU  - Marcus, A.
AU  - Moran, K.
T1  - On Using GUI Interaction Data to Improve Text Retrieval-Based Bug Localization
SP  - 466–478
EP  - 466–478
M3  - https://doi.org/10.1145/3597503.3608139
KW  - Bug Localization;Computer bugs;GUI;Location awareness;Measurement;Mobile Apps;Natural language processing;Natural languages;Semantics;Software;Source coding
N2  - One of the most important tasks related to managing bug reports is localizing the fault so that a fix can be applied. As such, prior work has aimed to automate this task of bug localization by formulating it as an information retrieval problem, where potentially buggy files are retrieved and ranked according to their textual similarity with a given bug report. However, there is often a notable semantic gap between the information contained in bug reports and identifiers or natural language contained within source code files. For user-facing software, there is currently a key source of information that could aid in bug localization, but has not been thoroughly investigated - information from the graphical user interface (GUI). In this paper, we investigate the hypothesis that, for end user-facing applications, connecting information in a bug report with information from the GUI, and using this to aid in retrieving potentially buggy files, can improve upon existing techniques for text retrieval-based bug localization. To examine this phenomenon, we conduct a comprehensive empirical study that augments four baseline text-retrieval techniques for bug localization with GUI interaction information from a reproduction scenario to (i) filter out potentially irrelevant files, (ii) boost potentially relevant files, and (iii) reformulate text-retrieval queries. To carry out our study, we source the current largest dataset of fully-localized and reproducible real bugs for Android apps, with corresponding bug reports, consisting of 80 bug reports from 39 popular open-source apps. Our results illustrate that augmenting traditional techniques with GUI information leads to a marked increase in effectiveness across multiple metrics, including a relative increase in Hits@10 of 13-18%. Additionally, through further analysis, we find that our studied augmentations largely complement existing techniques, pushing additional buggy files into the top-10 results while generally preserving top ranked files from the baseline techniques.
ER  -
TY  - CONF
AU  - Maeda, Ryuichi
AU  - Baselizadeh, Adel
AU  - Watanabe, Shin
AU  - Kurazume, Ryo
AU  - Torresen, Jim
T1  - Adaptive Tidying Robots: Learning from Interaction and Observation
SP  - 185–192
EP  - 185–192
M3  - https://doi.org/10.1109/SII59315.2025.10871003
KW  - Accuracy;Adaptation models;Benchmark testing;Large language models;Robot learning;Search problems;Service robots;Software;System integration;User experience
N2  - Designing service robots capable of tidying up in unfamiliar and dynamic human environments presents a significant challenge. Such robots must not only recognize and manipulate a wide range of objects but also align their actions with tidying up rules, which may vary greatly from one individual to another. To address these challenges, we propose a comprehensive software framework that integrates Large Language Model (LLM) and Vision-Language Models (VLMs) for service robots. Our framework enables robots to learn human-specific tidying up rules through interaction and observation, and to identify and handle previously unseen objects and receptacles. This adaptive framework offers a unified solution for recognizing, learning, and acting upon diverse and dynamic human environments. We evaluate our framework using both a text-based benchmark dataset to assess tidying up rule learning and a simulated environment to demonstrate practical tidying up performance. In the evaluation using the text-based benchmark dataset, our framework selects appropriate receptacles for unseen objects with high accuracy (87.4%), including unseen receptacle categories. The simulation evaluation confirms the effectiveness of our framework in realistic environments and scenarios. This research advances the field of service robotics by presenting an integrated software solution that leverages LLM and VLMs for more personalized and adaptable robot behavior in real-world tasks.
ER  -
TY  - CONF
AU  - Lohar, Prerna
AU  - Baraskar, Trupti
T1  - Automated AI Tool for Log File Analysis
SP  - 1762–1766
EP  - 1762–1766
M3  - https://doi.org/10.1109/ICMCSI64620.2025.10883511
KW  - Artificial intelligence;Error Detection;LLM;Log Analysis;Log Parsing;Manuals;Mobile computing;Monitoring;Optimization;Reviews;Software systems;Streams;Surveys;Trajectory
N2  - Log file analysis has a critical role in monitoring and maintaining software systems, yet the manual inspection of logs becomes increasingly impractical with the growing volume of data. This survey paper explores recent advancements in automated log file analysis, with a particular focus on the integration of AI techniques., which includes ML models and NLP. The study identifies key challenges in traditional methods, such as the need for human interpretation., difficulty in detecting new errors, and issues in backtracking within continuous data streams. Moreover, we examine state-of-the- art AI approaches., like LLaMA 2, to streamline log analysis by automating error detection., summarization, and anomaly identification. Research deficiencies are identified, notably the necessity for advanced methodologies to manage variety of log formats, automated model optimization, and ongoing learning processes. This comprehensive review endeavors to provide a thorough examination of the current landscape, encompassing perspectives on potential outcomes and prospective trajectories in the domain of AI -enhanced log file analysis. In addition to providing insights into prospective solutions and future directions in AI-driven log file analysis, this study attempts to give an in-depth analysis of the current situation.
ER  -
TY  - CONF
AU  - Charoenthanakitkul, Aticha
AU  - Viboonsang, Pranodnard
AU  - Kosolsombat, Somkiat
T1  - Optimizing Malware Detection with Random Forest, XGBoost, LightGBM, and LLM-Reporting
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/ICCI64209.2025.10987272
KW  - Accuracy;Analytical models;Computational modeling;Computer security;Cybersecurity;Feature extraction;Large language models;LightGBM;LLM;Machine learning algorithms;Malware;Malware Detection;Random Forest;Random forests;Training;XGBoost
N2  - This study presents a comprehensive framework for malware detection that integrates traditional machine learning algorithms with advanced large language models (LLMs) to enhance both classification accuracy and automated report generation. The proposed approach leverages three widely used machine learning models-Random Forest, XGBoost, and LightGBM-to classify different types of malwares based on extracted features from the EMBER dataset, a well-known benchmark dataset for malware analysis. Each of these models was trained and evaluated to determine their effectiveness in identifying malicious software with high accuracy. Among the tested algorithms, LightGBM exhibited the best performance, achieving an impressive classification accuracy of 83%. XGBoost followed closely with 81%, while Random Forest achieved an accuracy of 79%. To further enhance the usability of the system for cybersecurity analysts, the study incorporates a large language model, specifically LLaMA, which is accessed via an API. This model is employed to generate detailed, human-readable reports that provide insights into detected malware threats. By automating the reporting process, the framework aims to reduce the manual effort required for malware analysis, allowing security professionals to focus on high-priority threats. Future research will focus on improving classification performance through advanced feature engineering techniques and exploring deep learning-based neural network architectures to further enhance the accuracy and efficiency of malware detection and analysis.
ER  -
TY  - CONF
AU  - Chaudhary, Daksh
AU  - Vadlamani, Sri Lakshmi
AU  - Thomas, Dimple
AU  - Nejati, Shiva
AU  - Sabetzadeh, Mehrdad
T1  - Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson
SP  - 707–718
EP  - 707–718
M3  - https://doi.org/10.1109/ICSME58944.2024.00075
KW  - Accuracy;Chatbot-Enabled Software Engineering;Chatbots;Continuous integration;Continuous Integration and Continuous Delivery (CI/CD);Feedback loop;Large language models (LLMs);Production;Prototypes;Question answering (information retrieval);Retrieval-Augmented Generation (RAG);Software maintenance;Telecommunications;Usability
N2  - This paper presents our experience developing a Llama-based chatbot for question answering about continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Our chatbot is designed to handle the specificities of CI/CD documents at Ericsson, employing a retrieval-augmented generation (RAG) model to enhance accuracy and relevance. Our empirical evaluation of the chatbot on industrial CI/CD-related questions indicates that an ensemble retriever, combining BM25 and embedding retrievers, yields the best performance. When evaluated against a ground truth of 72 CI/CD questions and answers at Ericsson, our most accurate chatbot configuration provides fully correct answers for 61.11% of the questions, partially correct answers for 26.39%, and incorrect answers for 12.50%. Through an error analysis of the partially correct and incorrect answers, we discuss the underlying causes of inaccuracies and provide insights for further refinement. We also reflect on lessons learned and suggest future directions for further improving our chatbot's accuracy.
ER  -
TY  - CONF
AU  - Chen, Sinan
AU  - Kihara, Taiju
AU  - Abe, Mako
AU  - Abe, Shiryuu
T1  - Design and Development of a Web Application for Childcare Assistance Using AI-Code Generation
SP  - 361–365
EP  - 361–365
M3  - https://doi.org/10.1109/ICCC62609.2024.10942261
KW  - AI-code generation;assistive technology;childcare;Image recognition;Large language models;Mobile applications;Python;Security;Software;Stress;Training;Transfer learning;Usability;Web application
N2  - This study addresses the growing demand for effective childcare support amid increasing challenges dual-income households face. We propose a mobile application designed to alleviate parental burdens by transforming routine tasks, such as dressing, tidying, and meal times, into engaging activities for children. Our goal is to foster children's independence and self-reliance by making these tasks enjoyable through game-like interfaces, thereby reducing parental stress and workload. The application will leverage cutting-edge artificial intelligence to balance automated processes with human input, ensuring a user-friendly experience for parents and children. Through this dual approach, the application aims to enhance family quality of life by facilitating efficient, enjoyable childcare experiences. Our research explores the innovative integration of AI within childcare solutions, highlighting its strengths and compensating for its limitations to achieve a balanced, effective platform.
ER  -
TY  - CONF
AU  - Chen, Y.
AU  - Pan, S.
AU  - Xia, Y.
AU  - Ren, K.
AU  - Zhang, L.
AU  - Mo, Z.
AU  - Chen, J.
AU  - Zhang, M.
AU  - Li, H.
AU  - Shuai, J.
AU  - Xia, Q.
AU  - Yu, R.
T1  - A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy
SP  - 360–367
EP  - 360–367
M3  - https://doi.org/10.1109/CIS-RAM61939.2024.10673360
KW  - Adaptation models;application (APP);Distance measurement;Large Language Model (LLM);Large language models;Low-Rank Adaptation (LoRA);Mechatronics;Medical treatment;Random access memory;sleep disorders;Training
N2  - The aim of this study was to develop a mobile application for psychotherapy with insomnia patients using the ChatGLM-LoRA model, fine-tuned by Low-Rank Adaptation, and validated in a clinical trial.The dataset used to train the model was a collection of 764 dialogues related to sleep disorders. The corpus was randomly divided into three subsets: training, validation, and test sets. The hyperparameters used in this study to train the model were 450 epochs, betas ranging from 0.9 to 0.95, weight decay rate 5e-4, maximum learning rate 1e-5, and AdamW optimizer. Based on the test results of the above hyperparameters, the four metrics of BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L of the model reached 0.0340, 0.0451, and 0.0163; 0.2773, 0.3075, and 0.1986; 0.0592, 0.0735, and 0.0261; 0.2112, 0.2336, and 0.1500 for the training, validation, and test sets.These results indicate the technical feasibility and potential clinical utility of using an advanced language model-based application for psychotherapeutic intervention in insomnia.
ER  -
TY  - CONF
AU  - Chopade, A.
AU  - Shingde, V.
AU  - Chavare, A.
AU  - Bhagwat, T.
T1  - Code Insight - Flowchart Generator
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/IC457434.2024.10486526
KW  - algorithm design;algorithm efficiency;Codes;collaborative development;flowchart;Flowcharts;Generators;GPT-3.5 turbo;Natural language processing;pseudocode interpretation;serialization;Software algorithms;Transforms;tree structures;Visualization
N2  - The goal of the project is to automate the conversion of pseudocode into visually appealing flowcharts, facilitating the transition from human-readable algorithm descriptions to machine-executable code in software. This study presents a new approach that uses the OpenAI GPT-3.5 Turbo model for automatic flowchart generation. Addressing the lack of tools that understand pseudocode, the method interprets pseudocode, generates executable code fragments, and creates aesthetically pleasing flowcharts. This article reviews the literature and highlights a gap in automated flow charts. The selected methodology uses state-of-theart language models, with a special focus on natural language processing (NLP). The study aims to improve the understanding of algorithms by visually representing complex algorithms through automatic flowchart generation.Examples 1. Sorting rhythm: Consider a scenario where a complex sorting algorithm is described in pseudocode. The flowchart generator interprets the pseudocode, generates the corresponding executable code, and converts it into a complete flowchart. This visual representation helps developers understand algorithm logic, encourages discussion, and improves collaboration.2. Recursive algorithms and tree structures: The project extends the application to handle recursive algorithms and tree structures. For example, when a flowchart generator is provided with recursive pseudocode to traverse a binary tree, it smoothly transforms the abstract description into a detailed flowchart. This feature increases the versatility of the toolbox and offers a wide range of algorithmic models.3. Optimization Algorithm Performance: The flowchart generator not only helps to understand but also to improve algorithms by identifying potential bottlenecks and optimizing the generated code. Critical decision points and loops in the flowchart visually highlight key areas and provide developers with valuable information to make informed optimization decisions.
ER  -
TY  - CONF
AU  - Choudhuri, Rudrajit
AU  - Trinkenreich, Bianca
AU  - Pandita, Rahul
AU  - Kalliamvakou, Eirini
AU  - Steinmacher, Igor
AU  - Gerosa, Marco
AU  - Sanchez, Christopher
AU  - Sarma, Anita
T1  - What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards Genai
SP  - 1691–1703
EP  - 1691–1703
M3  - https://doi.org/10.1109/ICSE55347.2025.00087
KW  - Behavioral Intentions;Cognitive Styles;Generative AI;LLM;Maintenance;Mathematical models;PLS-SEM;Productivity;Refining;Software;Software development management;Software engineering;trust;Usability;User experience
N2  - Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity-variations in users' cognitive styles-that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice? We developed a theoretically grounded statistical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools in their work. We surveyed software developers (\mathrm{N}{=}{2}38) at two major global tech organizations: GitHub Inc. and Microsoft; and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools in their work. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience.
ER  -
TY  - CONF
AU  - Chu, Weiyan
AU  - Yin, Sitan
AU  - Huang, Lei
AU  - Lin, Ling
AU  - Wang, Xiaodong
AU  - Zhang, Zhi
AU  - Li, Hongwu
T1  - Verify-Agent: Large Language Model Multi-Agent for Intelligent Verification
SP  - 374–379
EP  - 374–379
M3  - https://doi.org/10.1109/IUCC65928.2024.00072
KW  - Computer architecture;framework;Intelligent agents;intelligent verification;Large language models;multi-agent;Multi-agent systems;Optimization;paradigm;performance;Planning;Reviews;Systems architecture;Ubiquitous computing;Usability;verify-agent
N2  - An AI agent is an intelligent framework that can perceive, plan and decompose, perform actions, and reflect. The multi-agent mode is a distributed computing mode that collaborates to complete different tasks in a group of AI intelligent agent environments. After discussion, inspection, reasoning, and evaluation, accurate conclusions are drawn. Multi AI intelligent agents play a crucial role in enhancing the performance of large models. Research and optimization of multi-agent architecture provide more perspectives and insights for system architecture design. In network planning, there are various application scenarios for reviewing planning template documents. This paper studies a multi-agent architecture based on open-source large language models, which is applied to planning document templates and data review to enhance the efficiency of processing and analyzing.
ER  -
TY  - CONF
AU  - Chaplia, Oleh
AU  - Klym, Halyna
T1  - Cloud-Based System for Source Code Analysis of Microservices with LLM Agents
SP  - 1–4
EP  - 1–4
M3  - https://doi.org/10.1109/CSIT65290.2024.10982613
KW  - Artificial intelligence;Cloud computing;Codes;llm agents;Microservice architectures;microservices;Software architecture;Software reliability;Source coding;Standards;Technological innovation;Testing;Text summarization;Usability
N2  - This paper presents a cloud-based system for microservice code analysis that utilizes LLM agents as a foundation. The system is designed to receive the code updates, analyze the code, and provide results to the user. Users can obtain information about the microservice, code summaries, code reviews, reliability checks, and improvement recommendations. The history of changes allows for tracking the evolution of microservices and identifying the issues when they appear. The prototype of the proposed system was implemented and tested. The results show the systems' usability and provide valuable insights. This research underscores the potential of combining LLM agents with cloud technologies, offering a scalable microservice solution that paves the way for future innovations in automated code analysis and software engineering.
ER  -
TY  - CONF
AU  - Corazza, Jan
AU  - Gavran, Ivan
AU  - Moreira, Gabriela
AU  - Neider, Daniel
T1  - Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs
SP  - 542–552
EP  - 542–552
M3  - https://doi.org/10.1109/ICST62969.2025.10989026
KW  - Blockchains;Code Generation;Codes;Formal Methods;Large language models;Maintenance engineering;Mathematical models;Model Synthesis;Model-Based Techniques;Smart contracts;Software;Software Auditing;Syntactics;Trustless services;User experience
N2  - When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct–vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model–a mathematical abstraction of the software system–which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we ``fill in the blanks'' using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.
ER  -
TY  - CONF
AU  - Ding, Y.
T1  - Development of Intelligent Tutors Based on Dialogue Systems
SP  - 927–930
EP  - 927–930
M3  - https://doi.org/10.1109/SSAIC61213.2024.00187
KW  - Customer services;dialogue systems;Education;Foreign language;Machine Learning;model;Natural language processing;Oral communication;Speech recognition;technology;User experience
N2  - Over the course of the past several years, Natural Language Processing (NLP) and machine learning researchers have grown more interested in the subject of Intelligent Tutoring Systems (ITS).The primary purpose of ITS is to facilitate language learning by facilitating voice interaction via speech recognition and speech synthesis technologies and evaluating learners' oral expression using language models. In this study, we examine the history of ITS's evolution as well as its present research state and potential future possibilities. First, a brief overview of ITS's past development is provided, followed by an examination of its primary application domains. Second, a discussion of some of the most important technologies utilized by ITS is presented. These technologies include voice recognition, speech synthesis, language modeling, conversation management, and learning management. In the final part of this article, we will go over some of the next developments in ITS, such as cross-language learning, multimodal interaction, and tailored learning.
ER  -
TY  - CONF
AU  - Djajadi, Natanael
AU  - Deljouyi, Amirhossein
AU  - Zaidman, Andy
T1  - Using Large Language Models to Generate Concise and Understandable Test Case Summaries
SP  - 322–326
EP  - 322–326
M3  - https://doi.org/10.1109/ICPC66645.2025.00040
KW  - Automated Test Generation;Documentation;Large language models;Readability;Software;Software testing;Test pattern generators;Unit Testing
N2  - Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.
ER  -
TY  - CONF
AU  - Dong, Fangming
AU  - Jiang, Zhengwei
AU  - Ma, Chunyan
AU  - He, Qiying
AU  - Yang, Peian
AU  - Yao, Yepeng
AU  - Wang, Jian
T1  - From Threat Report to ATT&CK: Automated Extraction and Reasoning of TTPs Using Large Language Models
SP  - 860–865
EP  - 860–865
M3  - https://doi.org/10.1109/CSCWD64889.2025.11033281
KW  - Accuracy;Cyber threat intelligence;Data mining;Federated learning;Large Language Model;Large language models;MITRE ATT&CK;Natural languages;Pipelines;Standardization;Synthetic data;TTP Extraction;Usability
N2  - The escalating frequency and increasing complexity of cyber attacks underscore the importance of Cyber Threat Intelligence (CTI). Tactics, Techniques, and Procedures (TTPs), as advanced CTI capable of characterizing adversarial behaviors and intentions, have garnered increased attention. However, TTPs are predominantly found embedded within unstructured natural language texts of threat reports. The accurate extraction and standardization of TTPs pose significant challenges. Existing methods exhibit limitations in terms of accuracy, generalizability, and interpretability. This paper presents a pipeline for automatically extracting TTPs from threat reports and providing rationales using large language models. To support this approach, we have developed three datasets using advanced commercial LLMs for data synthesis. These datasets are made publicly available to facilitate further research. Experimental results demonstrate the superior performance of our proposed approach, achieving an F1-score of 97.15% and accuracies of 79.22% and 92.97% in the respective tasks. These results surpass state-of-the-art methods by 15.39%, 12.87%, and 27.57%, respectively. To the best of our knowledge, this paper is the first to simultaneously extract TTPs while providing the underlying rationales for the extraction. This novel approach significantly improves the usability of the results by providing a richer context for threats.
ER  -
TY  - CONF
AU  - Du, Hongkai
AU  - Li, Haozhou
AU  - Peng, Qinke
AU  - Fu, Laiyi
T1  - User Linguistic Style Awareness and Interest-Driven Conversational Recommender Systems
SP  - 215–220
EP  - 215–220
M3  - https://doi.org/10.1109/DTPI61353.2024.10778861
KW  - Chatbots;Codes;conversational recommender systems;Digital twins;interest-driven;Large Language Model;Large language models;Linguistics;prompt learning;Recommender systems;Testing;Trajectory;user personality
N2  - Generation-based conversational recommender systems (CRSs) are tailored to individual user needs, employing cutting-edge text generation techniques for generating contextually relevant and fluent responses. Nevertheless, current CRSs, including advanced general language models like ChatGPT, often overlook the subtle differences in users' linguistic styles and the evolving nature of their interests, which leads to a tendency for uniform responses at the onset of dialogues and unnecessarily prolonged interaction cycles. To overcome these challenges, our approach leverages user historical data and information from similar user profiles to more accurately capture the user linguistic feature within dialogue subtasks. We introduce an innovative multi-factor cross-attention mechanism, specifically designed for the efficient integration of diverse quantitative relationships. In the recommendation subtask, our strategy involves the intuitive representation of user interest migration trajectories and the stereotype. This not only allows for a more granular understanding of the trajectory of user interests but also alleviates data sparsity for new users. Our proposed framework, User Linguistic Style Awareness and Interest-Driven CRSs (LSAID), has undergone extensive testing on well-established CRS datasets such as ReDial and INSPIRED. The empirical results underscore LSAID's effectiveness, showcasing its state-of-the-art performance. Our code will be released at http://github.com/zhizhaixingchen/LSAID.
ER  -
TY  - CONF
AU  - Durgaprasad, K. V. V. B.
AU  - Abozibid, Hassan Khalid
AU  - Nasri Hawas, Jawdat
AU  - Sai Krishna, Pusala Bhuvan
AU  - Bhavya Sri, Kanumuri
AU  - Reddy, Gali Pranay
T1  - AI Agents and Conversation System
SP  - 1–7
EP  - 1–7
M3  - https://doi.org/10.1109/ARIIA63345.2024.11051848
KW  - AI Agents;Artificial intelligence;Automation;Business;Decision making;Generative AI;Industries;Internet;Large language models (LLMs);Natural languages;Oral communication;Pattern Recognition;Real-time systems;Transfer learning;Transformer Architecture;Transportation
N2  - The development of artificial intelligence (AI) agents represents a significant technological advancement characterized by autonomous software programs capable of executing diverse tasks with human-like intelligence. This study investigates the creation of AI agents that integrate state-of-the-art capabilities for decision-making and natural language interaction. Drawing on the fundamental concepts of AI agents, this research endeavors to enhance their functionality by seamlessly incorporating real-time Internet data into decision-making processes and implementing contextually aware conversation systems. The proposed AI agents were designed as independent entities capable of learning from data, making decisions, and executing actions without the need for direct human intervention. By leveraging language models (LLMs), these agents process vast amounts of data and continuously adapt and improve their performance over time. The integration of real-time Internet data enables agents to adjust their decision-making processes dynamically, ensuring prompt and well-informed responses to changing information. Contextually aware conversation systems empower these agents to engage in natural and personalized interactions, thereby fostering heightened user satisfaction and engagement. An intriguing aspect of AI agents is their capacity to learn and adapt to new situations without explicit programming in each unique scenario. Through continuous learning and interaction with the environment, AI agents have evolved to handle complex tasks efficiently and accurately. They can analyze patterns in large datasets, providing valuable insights and predictions across various industries, including healthcare, finance, and transportation. This research underscores a commitment to innovation, addressing existing limitations in AI technology and exploring its potential to advance artificial intelligence in practical business applications. By bridging the gap between traditional AI capabilities and emerging business needs, this research aims to contribute significantly to the development and implementation of intelligent AI agents in real-world scenarios.
ER  -
TY  - CONF
AU  - Duvvuru, Venkata Sai Aswath
AU  - Zhang, Bohan
AU  - Vierhauser, Michael
AU  - Agrawal, Ankit
T1  - LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems
SP  - 385–397
EP  - 385–397
M3  - https://doi.org/10.1109/ICSE55347.2025.00223
KW  - ai for se;Autonomous aerial vehicles;Manuals;Planning;Simulation;Simulation Testing;Software engineering;sUAS;Surveillance;Testing;Urban areas;Usability;Wind
N2  - Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose Autosimtest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that Autosimtest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.
ER  -
TY  - CONF
AU  - Ellahi, Ehsan
AU  - Krishna Priya, R.
AU  - Talha, Muhammad
AU  - Hariprabhu, M.
AU  - Aragani, Venu Madhav
AU  - Tiwari, Mohit
T1  - Optimizing the Performance of Generative Artificial Intelligence ,Recent Approaches to Engineering Large Language Models
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICACCM61117.2024.11059049
KW  - Adaptation models;AI;AI chatbots;Chatbots;Generative AI;Internet;Large language models;LLMs;Measurement;performance assessment of LLMs;Prompt engineering;QTM;Real-time systems;Tamil AI models;Training;Training data;Usability
N2  - In the ever-growing field of Generative Artificial Intelligence, quick engineering has become a revolutionary method in the field of NLP for big language versions. This technique entails the manipulation of input queries to improve the quality and relation of the textual outputs of these models. New findings point to the fact that through the enhancement of the aspect of prompt engineering it could indeed be a way of significantly boosting LLM performance through a change of the input query format. This paper highlights the practical use of the prompt engineering concept to Tamil-based LLMs with an explicit purpose of developing a well-structured process capable of generating accurate and contextually relevant conversational responses. Thus, with subtle changes to how prompts are offered, this research aims to enhance the efficacy of Tamil LLMs that demand trivial data entry. The research uses the Query Transformation Module (QTM), which is an elaborate method developed to systematically change the input prompts into three forms of inquiries. Therefore, each format is built with objectives and key points in mind to enhance the understanding of the models as well as the results. To measure the effectiveness of this strategy, the QTM was employed with leading Tamil LLMs SKT GPT-2 and Tamil ChatGPT. We employed four different query methods in our experiments: all the initially employed unaltered request and three modified requests according to the QTM. The evaluation was made using Google SSA to fit the evaluation criteria of naturalness and specificity for the sentences generated by the models. Our experimental findings show that overall, there is a reasonable improvement in the performance by an average of 11 percent. Identified that there is an increase of 46% in the quality of generated sentences when using the transformed queries as compared to the unmodified prompts. This improvement also depicts how the QTM has an effectiveness of enhancing the performance of Tamil LLMs, resulting into its usability as a tool in the corresponding field of prompt engineering.
ER  -
TY  - CONF
AU  - Dhyani, P.
AU  - Nautiyal, S.
AU  - Negi, A.
AU  - Dhyani, S.
AU  - Chaudhary, P.
T1  - Automated API Docs Generator using Generative AI
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/SCEECS61402.2024.10482119
KW  - API Documentation;Documentation;fine-tuning;Generative AI;Generators;Industries;Large language models;Natural language processing;Training data;User experience;Web pages;Web Scraping
N2  - Our study provides an improvement on the creation of Application Programming Interfaces (APIs) usage documentation using the efficiency and power of Generative AI. APIs play an important role in software integration and software maintenance but the process of API documentation creation has been traditional and did not evolve with time, this paper employs Generative AI to enhance the accuracy, speed, and scale of API documentation generation. The automated API documentation generator is created using natural language processing applied through a large language model (TinyPixel/Llama-2-7B-bf16-sharded model). Training data was created by applying web scraping on various large tech companies' documentation web pages to get a good quality and industry-standard documentation dataset. It was further diversified and increased using the GPT model to handle a wide range of API scenarios. The fine-tuning greatly enhanced the TinyPixel/Llama-2-7B-bf16-sharded model's efficiency and quality of output which is proven by the reduced response time and the accuracy of documentation generated. Our study's comparative study confirms the effectiveness of the approach used. Our study's conclusion offers a comprehensive approach that should improve software development processes and pave the way for additional developments in API documentation.
ER  -
TY  - JOUR
AU  - Fakhoury, S.
AU  - Naik, A.
AU  - Sakkas, G.
AU  - Chakraborty, S.
AU  - Lahiri, S. K.
T1  - LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation
JO  - IEEE Transactions on Software Engineering
Y1  - 2024
VL  - 50
IS  - 9
SP  - 2254–2268
EP  - 2254–2268
M3  - https://doi.org/10.1109/TSE.2024.3428972
KW  - Accuracy;Artificial intelligence;Benchmark testing;Code Generation;Codes;Cognitive load;Human factors;Intent disambiguation;LLMs;Natural languages;Python;Task analysis;Test Generation
U1  - 1939-3520
N2  - Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.
ER  -
TY  - CONF
AU  - Chaaben, M. B.
AU  - Burgueño, L.
AU  - Sahraoui, H.
T1  - Towards using Few-Shot Prompt Learning for Automating Model Completion
SP  - 7–12
EP  - 7–12
M3  - https://doi.org/10.1109/ICSE-NIER58687.2023.00008
KW  - domain modeling;few-shot learning;language models;model completion;Natural languages;prompt learning;Semantics;Software engineering;Task analysis;Usability
N2  - We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.
ER  -
TY  - CONF
AU  - Bouzenia, Islem
AU  - Devanbu, Premkumar
AU  - Pradel, Michael
T1  - RepairAgent: An Autonomous, LLM-Based Agent for Program Repair
SP  - 2188–2200
EP  - 2188–2200
M3  - https://doi.org/10.1109/ICSE55347.2025.00157
KW  - ai for se;Autonomous agents;Computer bugs;Large language models;llm agents;Maintenance engineering;Pricing;program repair;Reliability;Software;Software engineering;Translation;User experience
N2  - Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.
ER  -
TY  - CONF
AU  - Aberkane, Abdel-Jaouad
AU  - Broucke, Seppe vanden
AU  - Poels, Geert
AU  - Georgiadis, Georgios
T1  - Leveraging ChatGPT for GDPR Compliance in Requirements Engineering: A Pilot Study
SP  - 34–41
EP  - 34–41
M3  - https://doi.org/10.1109/SpaCCS63173.2024.00012
KW  - Chatbots;data protection by design and by default;Decision making;General Data Protection Regulation;Industries;Large language models;Requirements engineering;Security;Software development management;Surveys
N2  - Large Language Models (LLMs) have significantly impacted various industries, offering enhanced processes and decision-making capabilities. Among these models, ChatGPT, an intelligent conversational agent, has found applications in multiple fields, including software development. This study explores how ChatGPT can benefit requirements engineering (RE), specifically in facilitating compliance with the General Data Protection Regulation (GDPR) principle of data protection by design and default. In particular, our research objective is to evaluate ChatGPT's ability to support professionals in assessing GDPR compliance within user stories, thus enhancing requirement quality. We obtain insights into ChatGPT's strengths and limitations in this context through an experiment and survey pilot study with experts. The experiment's outcome suggests that respondents generally agreed with ChatGPT's evaluation of user stories on GDPR compliance, agreeing with almost 73% of ChatGPT's evaluations. Furthermore, the survey results showed reservations regarding recommending ChatGPT to peers or colleagues, suggesting a cautious stance within the professional community. This study provides insights into ChatGPT's utility as an assistive tool for GDPR compliance in RE, presenting a starting point to address GDPR challenges in RE using LLMs.
ER  -
TY  - CONF
AU  - Abu-Arqoub, Mohammad
AU  - Alkarim Banna, Abed
AU  - El-Khalili, Nuha
AU  - Al-Shaikh Hasan, Mohammad
T1  - Design and Implementation of a Comprehensive RAG-Driven Dashboard Within the ILO System for Data Visualization and Query Support: A Case Study of the University of Petra
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/ICCIAA65327.2025.11013578
KW  - Business intelligence;Data systems;Data visualization;Educational technology;ILO System;Information retrieval;Large language models;Learning Analytics;Natural languages;RAG;Retrieval augmented generation;Time factors;Transforms
N2  - This article presents a comprehensive study on designing and implementing a dashboard for enhanced data visualization and query support at the University of Petra. The system leverages retrieval-augmented generation (RAG) and large language models (LLMs) to support diverse document types, including curricula, course descriptions, and program outcomes, alongside intended learning outcome (ILO)-related files. Our implementation demonstrates significant improvements in data accessibility and query response times while maintaining high accuracy in information retrieval and visualization. Through extensive evaluation, we show that this innovative approach transforms data management processes in higher education by enabling natural language interactions with educational data systems, building upon established business intelligence frameworks while introducing advanced AI capabilities.
ER  -
TY  - CONF
AU  - Abukadah, H.
AU  - Fereidouni, M.
AU  - Siddique, A. B.
T1  - Mapping Natural Language Intents to User Interfaces through Vision-Language Models
SP  - 237–244
EP  - 237–244
M3  - https://doi.org/10.1109/ICSC59802.2024.00045
KW  - Adaptation models;Computational modeling;Measurement;Natural language processing;Navigation;pre-trained vision-language models;Systematics;user interface navigation;User interfaces;Visualization
N2  - Efficiently navigating through mobile applications to accomplish specific tasks can be time-consuming and challenging, particularly for users who are unfamiliar with the app or faced with intricate menu structures. Simplifying access to a particular screen is a shared user priority, especially for individuals with diverse needs, including those with specific accessibility requirements. This underscores the exploration of innovative solutions to streamline the navigation process. This work addresses the challenge of mapping natural language intents to user interfaces, with a specific focus on the context of mobile applications. The primary objective of this work is to provide users with a more intuitive and efficient method for accessing desired screens in mobile applications by expressing their intentions in natural language. Existing approaches to this task have relied heavily on qualitative human studies for evaluating the performance. Moreover, widely used pre-trained vision-language models, such as Contrastive Language-Image Pretraining (CLIP), struggle to generalize effectively to the unique visual characteristics of user interfaces. Acknowledging the limitations, we introduce a novel approach that harnesses the power of the pre-trained vision-language models. Specifically, we investigate whether fine-tuning pre-trained vision-language models on mobile screens can address the challenges posed by the intricate nature of mobile application interfaces. Our approach involves the utilization of state-of-the-art pre-trained text and image encoders and employing a supervised fine-tuning process, where pre-trained models are adapted to the specific needs of mobile screen interactions. Moreover, a shared embedding space facilitates the alignment of embeddings of both text and image modalities, fostering a cohesive understanding between the natural language intents and visual features of user interface elements. We conduct extensive experimental evaluations using the Screen2Word dataset. Through systematic analysis and established metrics, we examine the models' ability to accurately map diverse linguistic intents to specific user interfaces. Our analysis demonstrates that fine-tuning yields substantial improvements over the zero-shot performance of the pre-trained vision-language models.
ER  -
TY  - CONF
AU  - Alario-Hoyos, Carlos
AU  - Kemcha, Rebiha
AU  - Kloos, Carlos Delgado
AU  - Callejo, Patricia
AU  - Estévez-Ayres, Iria
AU  - Santín-Cristóbal, David
AU  - Cruz-Argudo, Francisco
AU  - López-Sánchez, José Luis
T1  - Tailoring Your Code Companion: Leveraging LLMs and RAG to Develop a Chatbot to Support Students in a Programming Course
SP  - 1–8
EP  - 1–8
M3  - https://doi.org/10.1109/TALE62452.2024.10834365
KW  - and Programming Course;Chatbots;Codes;Generative Artificial Intelligence (GenAI);Information analysis;Java;Large language models;Large language models (LLMs);Learning (artificial intelligence);programming;Retrieval augmented generation;Retrieval-Augmented Generation (RAG);Surveys;Videos
N2  - Students frequently rely on chatbots powered by generative Artificial Intelligence (GenAI), such as ChatGPT, Copilot, Gemini, and Claude, to assist with a wide range of academic tasks. However, these chatbots are not specifically designed for the context of particular courses, which can lead to responses that are sometimes inaccurate or insufficiently relevant. This paper introduces a chatbot specifically designed to support first-year engineering students in a Java programming course. Developed using the Retrieval-Augmented Generation (RAG) technique, the chatbot draws on course-specific resources such as videos, quizzes, programming exercises, and other materials, while using OpenAI's Large Language Models (LLMs) GPT-4 and GPT-3.5 for information analysis and response generation. The data collected, consisting of logs from 1,059 messages sent by students to the chatbot and 30 responses to a survey, indicate that students primarily used the chatbot to clarify concepts and explain code snippets. Moreover, most of the students reported that the responses provided by the chatbot were well suited to the Java programming course.
ER  -
TY  - CONF
AU  - Alian, Parsa
AU  - Nashid, Noor
AU  - Shahbandeh, Mobina
AU  - Shabani, Taha
AU  - Mesbah, Ali
T1  - Feature-Driven End-to-End Test Generation
SP  - 450–462
EP  - 450–462
M3  - https://doi.org/10.1109/ICSE55347.2025.00141
KW  - Benchmark testing;End-to-End Testing;Feature Inference;Large language models;Manuals;Software engineering;Test pattern generators;Translation
N2  - End-to-end (E2E) testing is essential for ensuring web application quality. However, manual test creation is time-consuming, and current test generation techniques produce incoherent tests. In this paper, we present Autoe2e,a novel approach that leverages Large Language Models (LLMs) to automate the generation of semantically meaningful feature-driven E2E test cases for web applications. Autoe2eintelligently infers potential features within a web application and translates them into executable test scenarios. Furthermore, we address a critical gap in the research community by introducing E2EBENCH, a new benchmark for automatically assessing the feature coverage of E2E test suites. Our evaluation on E2EBENCH demonstrates that Autoe2eachieves an average feature coverage of 79%, outperforming the best baseline by 558 %, highlighting its effectiveness in generating high-quality, comprehensive test cases.
ER  -
TY  - JOUR
AU  - Almatrafi, Afnan A.
AU  - Eassa, Fathy A.
AU  - Sharaf, Sanaa A.
T1  - Code Clone Detection Techniques Based on Large Language Models
JO  - IEEE Access
Y1  - 2025
VL  - 13
SP  - 46136–46146
EP  - 46136–46146
M3  - https://doi.org/10.1109/ACCESS.2025.3549780
KW  - Adaptation models;Clone detection;Cloning;code duplication;Codes;fine-tuning;instruction tuning;Large language models;Natural language processing;Real-time systems;Software engineering;Syntactics;Training;Transformers;Tuning
U1  - 2169-3536
N2  - Code duplication, commonly known as code cloning, is a persistent challenge in software development. While reusing code fragments boosts productivity, excessive cloning poses challenges to maintenance and elevates the risk of bugs. Therefore, integrating code clone detection into the development process is crucial. The extensive code-related knowledge inherent in Large Language Models (LLMs) renders them high-potential candidates for addressing diverse software engineering challenges. However, the effectiveness of LLMs in the specific task of code clone detection requires precise evaluation. This paper proposes an innovative methodology leveraging few-shot instruction-tuned GPT-3.5 Turbo and GPT-4 to detect code clones across all types, focusing on complex clones (Type-3 and Type-4). Unlike conventional approaches confined to specific language pairs or tasks, our method employs versatile language models, showcases generalization strengths for semantic understanding, and leverages instruction tuning with few-shot inference for task-specific adaptability in code clone detection. A conversational dataset was crafted from BigCloneBench for instruction tuning, enhancing task alignment and performance. This study evaluates the proficiency of LLMs in identifying code clones, analyzing the impact of instruction tuning, and assessing the efficiency across various clone types. Experimental results demonstrate these models achieving competitive performance against existing tools for overall and complex clone detection. Integration into an Integrated Development Environment (IDE) enables real-time detection and automated refactoring, bridging the gap between theoretical advancements and practical usability. This work highlights the potential of generalized LLMs setting a new standard in a field traditionally dominated by specialized tools and demonstrates their adaptability for complex challenges in code analysis and maintainability.
ER  -
TY  - CONF
AU  - Ashen Shanuka, K. A.
AU  - Wijayanayake, Janaka
AU  - Vidanage, Kaneeka
T1  - Analyzing the impact of prompt engineering on efficiency, code quality, and security in CRUD application development
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICARC64760.2025.10963005
KW  - Code quality;Codes;Collaboration;crud operations;Focusing;Large language models;Measurement;programming;Prompt engineering;Security;Software development management;Testing;User experience
N2  - This research evaluates the capabilities of Large Language Models (LLMs) in generating CRUD applications using Python Flask framework, focusing on code quality, security, and UI design. The study analyzes five prominent LLMs: Claude 3.5 Sonnet, Gemini, GitHub Copilot, GPT-4, and Perplexity, through automated static code review tools including Code Factor, Codacy, and Code Scene. The evaluation reveals consistently high performance across models, with GitHub Copilot and Gemini achieving superior code health metrics (9.5-10.0) and 100% green code ratings. While all models maintained Grade A ratings in Code Factor, common security vulnerabilities, particularly in Flask debug mode configuration, persisted across implementations. UI evaluation through professional interviews indicated that while generated interfaces were functionally complete, they lagged behind human-designed UIs in aesthetic appeal. The research demonstrates that LLMs can automate 60-90% of CRUD development tasks, though human intervention remains essential for environment setup and security configuration. The findings suggest a shift toward human-AI hybrid development approaches, where entry-level developers can effectively manage LLM-generated code while focusing on strategic implementation decisions.
ER  -
TY  - CONF
AU  - Bucaioni, Alessio
AU  - Weyssow, Martin
AU  - He, Junda
AU  - Lyu, Yunbo
AU  - Lo, David
T1  - A Functional Software Reference Architecture for LLM-Integrated Systems
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/ICSA-C65153.2025.00006
KW  - Computational modeling;Computer architecture;functional reference architecture;Large language models;LLMs;Natural language processing;Privacy;Security;Software architecture;Software reference architecture;Software systems;Systematics;Text processing
N2  - The integration of large language models into software systems is transforming capabilities such as natural language understanding, decision-making, and autonomous task execution. However, the absence of a commonly accepted software reference architecture hinders systematic reasoning about their design and quality attributes. This gap makes it challenging to address critical concerns like privacy, security, modularity, and interoperability, which are increasingly important as these systems grow in complexity and societal impact. In this paper, we describe our emerging results for a preliminary functional reference architecture as a conceptual framework to address these challenges and guide the design, evaluation, and evolution of large language model-integrated systems. We identify key architectural concerns for these systems, informed by current research and practice. We then evaluate how the architecture addresses these concerns and validate its applicability using three open-source large language model-integrated systems in computer vision, text processing, and coding.
ER  -
TY  - CONF
AU  - Babushkin, Konstantin
AU  - Timofeev, Nikolai
AU  - Filianin, Ivan
T1  - Research on AI-Powered Employee Feedback Tools
SP  - 127–132
EP  - 127–132
M3  - https://doi.org/10.1109/SmartIndustryCon65166.2025.10986092
KW  - artificial intelligence integration;Companies;Costs;employee feedback;employee performance evaluation;feedback collection;HR tool;Industries;Large Language Model;Large language models;Performance evaluation;Real-time systems;Surveys;Testing;Usability;Writing
N2  - This paper explores methods for employee feedback (EF) collection and introduces a new framework to improve the process. Current feedback collection process often suffers from slow workflow, bias, and poor Artificial Intelligence (AI) integration. Large Language Models (LLM) combined with human oversight may provide a more agile, objective, and user-friendly way for collecting and analyzing feedback. Existing EF tools were evaluated based on criteria such as AI capabilities, usability, and costs. Building on these findings, an open-source and self-hosted EF tool was developed. The tool integrates AI at every stage of the process and offers real-time AI assistance in writing, summarizing, and interpreting feedback. Pilot testing in two tech companies demonstrated user satisfaction.
ER  -
TY  - CONF
AU  - Baravkar, S.
AU  - Zhang, C.
AU  - Hassan, F.
AU  - Cheng, L.
AU  - Song, Z.
T1  - Decoding and Answering Developers' Questions About Web Services Managed by Marketplaces
SP  - 194–203
EP  - 194–203
M3  - https://doi.org/10.1109/SSE62657.2024.00037
KW  - Data mining;Large Language Model;Large language models;Navigation;Q&A forums;Quality of service;Service Marketplace;Service-oriented architecture;Software;Taxonomy
N2  - Service registry, a key component of the service-oriented architecture (SOA), aids software developers in discovering services that meet specific functionality requirements. Recent years have witnessed the transition from the traditional service registries to its successor, the Service Marketplaces, which involves deeper engagement in the SOA software lifecycle and offers additional features, such as service request delegation and monitoring of services' Quality of Service (QoS). However, by analyzing developers' questions posted on online Q&A forums, we found that many developers struggle with such transition, leading to development inefficiencies and even security vulnera-bilities. This paper presents the first empirical study aimed at uncovering the issues developers face with marketplaces, particularly those arising from the transition. Through a meticulous process of manually labeling and analyzing developers' questions, we develop a taxonomy of these issues, summarize the impacts caused by the transition, and provide actionable suggestions to App developers, service providers, and marketplaces. Utilizing the labeled questions and our insights, we fine-tune a Large Language Model (LLM) for providing answers to similar questions raised by developers and helping service providers and marketplaces extract useful information from these questions, such as service outages and key leakages. Our evaluation of the model's performance in answering and extracting pertinent information from a set of real-world questions demonstrates its effectiveness: it accurately classified 85 % of the queries and successfully identified 88 % of service names and 77 % of key leakages. As the first empirical study in this domain, this work not only aids developers in navigating the transition more effectively but also sheds light on the under explored issue of service registry evolution, offering valuable insights for researchers.
ER  -
TY  - CONF
AU  - Baresi, Luciano
AU  - Camilli, Matteo
AU  - Dolci, Tommaso
AU  - Quattrocchi, Giovanni
T1  - A Conceptual Framework for Quality Assurance of LLM-based Socio-critical Systems
SP  - 2314–2318
EP  - 2314–2318
KW  - AI-enabled agents;Computational modeling;Focusing;Human intelligence;Large language models;Market research;Quality assurance;Reliability engineering;Runtime;Software agents;Software reliability
N2  - Recent breakthroughs in Artificial Intelligence (AI) obfuscate the boundaries between digital, physical, and social spaces, a trend expected to continue in the foreseeable future. Traditionally, software engineering has prioritized technical aspects, focusing on functional correctness and reliability while often neglecting broader societal implications. With the rise of software agents enabled by Large Language Models (LLMs) and capable of emulating human intelligence and perception, there is a growing recognition of the need for addressing socio-critical issues. Unlike technical challenges, these issues cannot be resolved through traditional, deterministic approaches due to their subjective nature and dependence on evolving factors such as culture and demographics. This paper dives into this problem and advocates the need for revising existing engineering principles and methodologies. We propose a conceptual framework for quality assurance where AI is not only the driver of socio-critical systems but also a fundamental tool in their engineering process. Such framework encapsulates pre-production and runtime workflows where LLM-based agents, so-called artificial doppelgängers, continuously assess and refine socio-critical systems ensuring their alignment with established societal standards.CCS CONCEPTS• Software and its engineering → Extra-functional properties; Software verification and validation; • Computing methodologies → Artificial intelligence.
ER  -
TY  - CONF
AU  - Bento, A. C.
AU  - Santos-Reséndiz, C. De Los
AU  - Segura-Prado, L. E.
AU  - La Rosa, R. Garza-de
AU  - de Jesus Hilario-Cruz, R.
AU  - González-Vázquez, S.
T1  - Experimental Results with the Use of OpenAI with IoT Classes for Water Analysis in Rainfall
SP  - 60–65
EP  - 60–65
M3  - https://doi.org/10.1109/icSmartGrid61824.2024.10578189
KW  - Artificial intelligence;Decision making;higher education;Internet of Things;IoT;OpenAI;professional education;Rain;rainfall collection;Sensor systems;Sensors;Sociology;Technological innovation
N2  - Water, being an essential resource for life and sustainable development, faces significant challenges such as scarcity due to factors such as climate change and population growth. Given this panorama, the need arises for innovative solutions to guarantee universal access to water. In this context, an Internet of Things (IoT) system has been developed in collaboration with OpenAI technology during classrooms, aimed at automatically capturing rainwater using sensors and actuators. The system proposes to optimize the management of water resources, eliminating dependence on operators and efficiently taking advantage of rainfall. The IoT system integrates various sensors to collect relevant environmental data, which is sent and stored in the cloud through Internet connections. These environmental parameters are analyzed to determine the optimal time for rainwater harvesting and to identify possible uses of the collected water. A comparison between the language models and the microcontroller data is carried out to evaluate the knowledge-based decision-making capability of artificial intelligence. As a result, a rigorous evaluation was obtained for decision making and the identification of the most appropriate OpenAI language model for this project. The rainwater harvesting system based on IoT and OpenAI technology represents a significant step and contributes towards sustainable water management, taking advantage of technological innovation to address crucial challenges in the availability of water resources.
ER  -
TY  - CONF
AU  - Bhatia, Gaurav
AU  - Alhajri, Raya
T1  - Accelerate Learning with AI Powered Quiz Master Using Llama LLM
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/AI2E64943.2025.10983252
KW  - Adaptation models;Adaptive systems;AI;Data models;Education;Ethics;Human computer interaction;LangChain;Large language models;Llama3.2;LLM;Manuals;Natural language processing;Ollama;Streamlit;Technological innovation
N2  - Multiple-choice questions quizzes provide an interactive and engaging way to assess students' understanding and reinforce learning, but their manual creation can be time-consuming and labor-intensive for educators. The emergence of Large Language Models (LLMs) offers a transformative solution by enabling automated MCQ generation, correction, and personalized feedback. LLMs, with their advanced natural language understanding and generation capabilities, facilitate the creation of diverse and adaptive MCQ quizzes that cater to various learning objectives and difficulty levels. This paper introduces Quiz Master, an AI-powered platform for automated MCQ quiz generation, correction, and feedback, developed using open-source tools such as Meta Llama, Ollama, LangChain, and Streamlit. Quiz Master delivers an engaging and personalized learning experience, allowing students to actively participate in quizzes while receiving instant, adaptive feedback. By optimizing quiz creation and reducing educator workload, Quiz Master demonstrates the potential of LLMs to enhance educational practices, making learning more enjoyable, interactive, and effective.
ER  -
TY  - CONF
AU  - Böck, M.
AU  - Habchi, S.
AU  - Nayrolles, M.
AU  - Cito, J.
T1  - Performance Prediction From Source Code Is Task and Domain Specific
SP  - 35–42
EP  - 35–42
M3  - https://doi.org/10.1109/ICPC58990.2023.00015
KW  - Adaptation models;Codes;Deep Learning;Defect Prediction;Repository Mining;Runtime;Software performance;Source coding;Static analysis;Training data;Video games
N2  - Performance is key to the success and adoption of software systems. In video games, performance is commonly highlighted as one of the top quality concerns raised by players. To check the performance of their systems, development teams tend to rely on profiling and monitoring tools, which observe program executions to identify regressions. The usage of static analysis tools for this purpose has been so far limited. Lately, the success of Large Language Models in many code analytics tools led to attempts to leverage them in static performance analysis. These studies showed promising results in predicting runtime and regressions on large public datasets. In this paper, we evaluate the usability of such models in practice, and particularly in the domain of video games. We train a state-of-the-art neural network on the Code4Bench dataset to predict runtime regressions for programming competition programs, then evaluate its ability to generalize to new domains. Our results show that these models achieve great results (e.g. 95.73% accuracy for performance comparison) on the original domain for programs solving in-sample programming tasks, yet fail to generalize to out-of-sample tasks. Furthermore, we show that transfer techniques such as domain adversarial adaptation and model fine-tuning are not sufficient to transfer these models to the target industrial domain of AAA games.
ER  -
TY  - CONF
AU  - Bodepudi, N. R.
AU  - Boddeti, K. Sabareesh
AU  - Palle, A. Reddy
AU  - Bhukya, L.
AU  - Bolla, S. D. T. Reddy
AU  - Sreedevi, C.
T1  - SoulEase : AI Enhanced Personal Journal - Text Based Emotion Detection
SP  - 1155–1162
EP  - 1155–1162
M3  - https://doi.org/10.1109/ICCPCT61902.2024.10673121
KW  - AI-generated affirmations;Deep Learning;Dynamic scheduling;Dynamic task scheduling;Emotion recognition;Mood;Mood analytics;Natural Language Processing (NLP);Productivity;Schedules;Sentiment analysis;stress assessment
N2  - Self-management is an art of managing your thoughts, feelings and actions that helps you achieve goals in life. In today's expeditious world individual's often struggle to maintain equilibrium between their personal life and work. The proposed web application uses NLP to evaluate the user behavior and mood and prepare a customized routine that balances productivity and rest by dynamically scheduling user's tasks. Current NLP based systems for assessing behavior and mood lack the capacity for thorough and flexible scheduling. It also involves libraries like NLTK. TensorFlow is used which is a machine learning model used to personalize suggestions. The web application analyses journals entered by user using NLP and detect user's emotions. The application gives AI generated prompts for completing their journal. When user enters their daily tasks, this application analyses the previous works of user and plan a daily schedule for them. The application also motivates them to complete their tasks by AI-generated positive affirmations through email. It analyses user's answers of questionnaires designed for stress assessment. This model can take audio, video journaling. The web application integrates with calendar events by using google calendar API. It uses google analytics for mood tracking and analytics. The functionality of the application would be supported by strong frontend and backend architectures that make use of ReactJS for frontend and Node.js, MySQL, Firebase, for backend. The proposed app can improve sentiment analysis and it is a unique combination of different features and is expected to be 2% higher than any existing models.
ER  -
TY  - CONF
AU  - Borg, Markus
T1  - Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring
SP  - 37–41
EP  - 37–41
M3  - https://doi.org/10.1109/IDE66625.2025.00012
KW  - AI refactoring;Calibration;Codes;Collaboration;Conferences;Human factors;Industries;Large language models;Security;Software;software maintainability;Testing;trust
N2  - In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions.
ER  -
TY  - CONF
AU  - Bappon, Suborno Deb
AU  - Mondal, Saikat
AU  - Roy, Banani
T1  - AUTOGENICS: Automated Generation of Context-Aware Inline Comments for Code Snippets on Programming Q&A Sites Using LLM
SP  - 24–35
EP  - 24–35
M3  - https://doi.org/10.1109/SCAM63643.2024.00013
KW  - Codes;Inline comments;Java;Large Lan-guage Models;Noise;Noise measurement;programming;Python;Source coding;Stack overflow;Standards;Surveys;Tool Support;Usability;User Study
N2  - Inline comments in the source code facilitate easy comprehension, reusability, and enhanced readability. However, code snippets in answers on Q&A sites like Stack Overflow (SO) often lack comments because answerers volunteer their time and often skip comments or explanations due to time constraints. Existing studies show that these online code examples are difficult to read and understand, making it difficult for developers (espe-cially novices) to use them correctly and leading to misuse. Given these challenges, we introduced AUTOGENICS, a tool designed to integrate with SO to generate effective inline comments for code snippets in SO answers exploiting large language models (LLMs). Our contributions are threefold. First, we randomly select 400 answer code snippets (200 Python + 200 Java) from SO and gener-ate inline comments for them using LLMs (e.g., Gemini). We then manually evaluate these comments' effectiveness using four key metrics: accuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate promising effectiveness in generating inline comments for SO answer code snippets. Second, we surveyed 14 active SO users to perceive the effectiveness of these inline comments. The survey results are consistent with our previous manual evaluation. However, according to our evaluation, LLMs-generated comments are less effective for shorter code snippets and sometimes produce noisy comments. Third, to address the gaps, we introduced AUTOGENICS that extracts additional context from question texts and generates context-aware inline comments. It also optimizes comments by removing noise (e.g., comments in import statements and variable declarations). We evaluate the effectiveness of AUTOGENICS-generated comments using the same four metrics that outperform those of standard LLMs. AUTOGENICS might (a) enhance code comprehension with context-aware inline comments, (b) save time, and improve developers' ability to learn and reuse code more accurately.
ER  -
TY  - CONF
AU  - Fakhoury, S.
AU  - Naik, A.
AU  - Sakkas, G.
AU  - Chakraborty, S.
AU  - Musuvathi, M.
AU  - Lahiri, S. K.
T1  - Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation
SP  - 390–391
EP  - 390–391
M3  - https://doi.org/10.1145/3639478.3643525
KW  - Accuracy;Codes;LLM4Code;Natural languages;Software engineering;User intent formulation;User Study
N2  - We introduce a novel workflow, TICODER, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TICODER partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TICODER at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TICODER through a user study with 15 participants.
ER  -
TY  - CONF
AU  - Fan, A.
AU  - Gokkaya, B.
AU  - Harman, M.
AU  - Lyubarskiy, M.
AU  - Sengupta, S.
AU  - Yoo, S.
AU  - Zhang, J. M.
T1  - Large Language Models for Software Engineering: Survey and Open Problems
SP  - 31–53
EP  - 31–53
M3  - https://doi.org/10.1109/ICSE-FoSE59343.2023.00008
KW  - Automated Program Repair;Documentation generation;Generative AI;Genetic Improvement;Human-Computer Interaction;Large language models;Maintenance engineering;Refactoring;Reliability engineering;Requirements engineering;Search Based Software Engineering (SBSE);Software;Software Analytics;Software engineering;Software Engineering Education;Software Maintenance and Evolution;Software Processes;Software reliability;Software testing;Surveys;Testing
N2  - This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.
ER  -
TY  - CONF
AU  - Ferdousi, Rahatara
AU  - Hossain, M. Anwar
AU  - El Saddik, Abdulmotaleb
T1  - TextureMeDefect: LLM-based Synthetic Railway Defect Texture on Mobile Devices
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/ICCE63647.2025.10930185
KW  - AI;AI-tool;Costs;Defect;Electric potential;Engines;Entertainment industry;GPT;Image synthesis;LLM;Mobile applications;Mobile Device;Rail transportation;Railway;Smart phones;Software;Texture;Usability
N2  - Texture image generation has been studied for various applications, including gaming and entertainment. However, context-specific realistic texture generation for industrial applications, such as generating defect textures on railway components, remains unexplored. A mobile-friendly, LLM-based tool that generates fine-grained defect characteristics offers a solution to the challenge of understanding the impact of defects from actual occurrences. We introduce TextureMeDefect, an innovative tool leveraging an LLM- based AI-Inferencing engine. The tool allows users to create realistic defect textures interactively on images of railway components taken with smartphones or tablets. We conducted a multifaceted evaluation to assess the relevance of the generated texture, time, and cost in using this tool on iOS and Android platforms. We also analyzed the software usability score (SUS) across three scenarios. TextureMeDefect outperformed traditional image generation tools by generating meaningful textures faster, showcasing the potential of AI-driven mobile applications on consumer-grade devices.
ER  -
TY  - CONF
AU  - Joldea, Andrei-Răzvan
AU  - Cernăzanu-Glăvan, Diana
AU  - Sârbu, Vlad
AU  - Bulzan, Andrei-Ştefan
T1  - Multimodal AI for Romanian University Support: An LLM, RAG and Voice Approach
SP  - 000189–000194
EP  - 000189–000194
M3  - https://doi.org/10.1109/SACI66288.2025.11030141
KW  - Assistant;Chatbot;Chatbots;Computational modeling;Domain-specific fine-tuning;Informatics;Information retrieval;Large language models;LLM;Question answering (information retrieval);RAG;Retrieval augmented generation;Romanian;Software development management;Speech to text;STT;Text to speech;TTS
N2  - The era of large language models (LLMs) brings forth a new wave of automation to many fields of activity. In this work we employ the AI advancements catalyzed by these LLMs to create a smart university assistant. A chatbot that comes to assist university enrolled students and staff on administrative, legislative and public interest topics. To this end, we develop a platform that combines Large Language Models, Retrieval Augmented Generation, Speech-to-Text and Text-to-Speech technologies to automate accessibility to university-related information. We start from openly available models and resources, adapt and finetune them to our target - Romanian question answering with information retrieval - and then release our solutions publicly at https://github.com/Andrei481/RomanianChatbot.
ER  -
TY  - JOUR
AU  - Joosten, J.
AU  - Bilgram, V.
AU  - Hahn, A.
AU  - Totzek, D.
T1  - Comparing the Ideation Quality of Humans With Generative Artificial Intelligence
JO  - IEEE Engineering Management Review
Y1  - 2024
VL  - 52
IS  - 2
SP  - 153–164
EP  - 153–164
M3  - https://doi.org/10.1109/EMR.2024.3353338
KW  - AI-augmented innovation;Artificial intelligence;Artificial Intelligence (AI);Chatbots;ChatGPT;Companies;Creativity;Generative AI;idea generation;innovation;Large language models (LLMs);Task analysis;Technological innovation
N2  - Traditionally, ideating new product innovations is primarily the responsibility of marketers, engineers, and designers. However, a rapidly growing interest lies in leveraging generative artificial intelligence (AI) to brainstorm new product and service ideas. This study conducts a comparative analysis of ideas generated by human professionals and an AI system. The results of a blind expert evaluation show that AI-generated ideas score significantly higher in novelty and customer benefit, while their feasibility scores are similar to those of human ideas. Overall, AI-generated ideas comprise the majority of the top-performing ideas, while human-generated ideas scored lower than expected. The executive's emotional and cognitive reactions were measured during the evaluation to check for potential biases and showed no differences between the idea groups. These findings suggest that, under certain circumstances, companies can benefit from integrating generative AI into their traditional idea-generation processes.
ER  -
TY  - CONF
AU  - Kannadasan, Tamilarasan
T1  - Custom LLM Architectures for Context-Aware User Interactions in Web Applications
SP  - 727–732
EP  - 727–732
M3  - https://doi.org/10.1109/ICACRS62842.2024.10841706
KW  - Adaptation models;Computational modeling;Computer architecture;Context modeling;Context-Aware;Data privacy;Large language models;Real-World Datasets;Reviews;Scalability;Service-oriented architecture;Time factors;Transforms;User Interaction;Web Applications
N2  - The rapid evolution that continuously occurs in web applications makes personalization and intuitiveness within the experiences of users all the more important. The following study deals with the development of an LLM architecture customized for contextually aware interactions within web environments. Using these kinds of datasets, obtained from real user interactions, together with behavioral analytics and contextual metadata coming from a variety of sources such as the Microsoft COCO dataset and Amazon Customer Reviews, we train LLMs to understand user needs through explicit interaction patterns or contextual cues. To this end, we integrate state-of-the-art natural language processing methodologies with contextual embedding strategies that empower models to dynamically adapt responses, enhance user engagement, and smoothen workflows. We perform extensive experimentation and user testing, demonstrating large improvements in user satisfaction and efficiency of interaction over traditional static models. Furthermore, we discuss a number of challenges related to data privacy, model scalability, and real-time processing that need to be resolved to make such custom LLMs practical in a wide range of web application settings. This work underlines how context-aware architectures can bring revolutionary changes to LLMs for user interactions, offering more responsive, intelligent, and user-centric web applications.
ER  -
TY  - CONF
AU  - Kannadasan, Tamilarasan
T1  - Optimizing LLM Architectures for Real-Time Applications in Full-Stack Development
SP  - 721–726
EP  - 721–726
M3  - https://doi.org/10.1109/ICACRS62842.2024.10841540
KW  - Accuracy;Adaptation models;Architecture Optimization;Computational modeling;Full-Stack Development;Large language models;Optimization;Public Dataset;Quantization (signal);Real-Time Applications;Real-time systems;Resource management;Throughput;User experience
N2  - As real-time applications become increasingly integral to full-stack development, the demand for efficient and scalable Large Language Model (LLM) architectures has surged. This study explores optimization strategies for LLMs tailored to meet the stringent performance and latency requirements of real-time environments. We investigate architectural enhancements, including model pruning, quantization, and parallel processing techniques, to reduce computational overhead without compromising accuracy. To validate the proposed approach, we employ the Common Crawl Corpus dataset as a comprehensive case study, leveraging its extensive and diverse textual data to simulate real-world application scenarios. Our experiments demonstrate significant improvements in response times and resource utilization, enabling seamless integration of LLMs into full-stack frameworks. Additionally, we address challenges related to data handling and model adaptability, ensuring that optimized architectures maintain robustness across dynamic workloads. The findings highlight the potential of LLM optimizations to bridge the gap between advanced natural language processing capabilities and the immediate demands of real-time application development. This work provides a foundational framework for developers aiming to harness the power of LLMs in full-stack projects, paving way for more responsive and intelligent web and software solutions.
ER  -
TY  - CONF
AU  - Karli, U. B.
AU  - Chen, J. T.
AU  - Antony, V. N.
AU  - Huang, C. M.
T1  - Alchemist: LLM-Aided End-User Development of Robot Applications
SP  - 361–370
EP  - 361–370
KW  - Code Generation;end-user development;Human-robot interaction;Large language models;Logic;Natural languages;Organizations;robot programming;Robots;Usability
N2  - Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming–moving from the conventional process of user specifying programming logic to an iterative, collaborative process in which the user specifies desired program outcomes while LLM produces detailed specifications. We introduce a novel integrated development system, Alchemist, that leverages LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, aiming to reduce the required knowledge for developing robot applications. We present a detailed examination of our system design and provide an exploratory study involving true end-users to assess capabilities, usability, and limitations of our system. Through the design, development, and evaluation of our system, we derive a set of lessons learned from the use of LLMs in robot programming. We discuss how LLMs may be the next frontier for democratizing end-user development of robot applications.CCS CONCEPTS• Human-centered computing; • Computer systems organization → Robotics;
ER  -
TY  - CONF
AU  - Khan, Farhan
AU  - Tulon, Tanima Ahamed
AU  - Masrur, Noor
AU  - Hasan, Md. Jihanul
AU  - Badrul, Tasnuba
AU  - Islam, Ashraful
T1  - Development and Evaluation of ShasthoBondhu: mHealth App for Guiding Emergency Remote Healthcare with Wearable IoT and AI Fusion
SP  - 01–10
EP  - 01–10
M3  - https://doi.org/10.1109/ASET60340.2024.10708738
KW  - Biomedical monitoring;emergency;Google Fit;healthcare;Internet of Things;IoT;MAUQ;Medical services;mHealth;Statistical analysis;Systematics;Technological innovation;Transforms;UCD;UEQ;Usability;User centered design;User experience;wearable
N2  - With the increasing prevalence of chronic diseases among the elderly population in Bangladesh, there is a critical need for essential solutions to address remote healthcare challenges. This research explores the integration of mobile health (mHealth) technology with wearable Internet of Things (IoT) devices to enhance remote healthcare services. Through a systematic four-step approach encompassing literature review for problem statement identification, needs assessment through quantitative analysis of research questions (RQs), app design and development utilizing User-Centered Design (UCD) and Rapid Iterative Testing and Evaluation (RITE) method, and usability evaluation through User Experience Questionnaire (UEQ), and mHealth App Usability Questionnaire (MAUQ), ShasthoBondhu, a Flutter-based cross-platform smartphone app, offers features such as automated vital monitoring and alerting, remote accessibility of account, location sharing based on user's vitals, convenient doctor appointments, instant emergency calls and real-time interaction with a virtual medical assistant powered by Large Language Model (LLM) tools. In addition, the app interfaces in bilingual format (Bangla and English) for seamless independent interactions. The assessment results, based on the 8-item UEQ-Sand 6-item MAUQ, showcased remarkable outcomes: UEQ's pragmatic and hedonic qualities surpassed benchmarks with means of 2.35 and 1.80, respectively, while MAUQ achieved an impressive 4.3 out of 5 on the rating scale. Henceforth, ShasthoBondhu charts the path forward, offering a scalable, user-centered solution with the fusion of mHealth technology, wearable IoT gadgets, and AI innovations to transform remote healthcare services in Bangladesh.
ER  -
TY  - CONF
AU  - Koualty, Rand
AU  - Chou, Nien-Ying
AU  - Alabdallah, Suleiman
T1  - Generative AI Agents, Build a Multilingual ChatGPT-based Customer Service Chatbot
SP  - 5–10
EP  - 5–10
M3  - https://doi.org/10.1109/FLLM63129.2024.10852436
KW  - Chatbot;Chatbots;ChatGPT;Customer Service;Generative AI agents;Large language models;LLMs;Multilingual;Operating systems;Privacy;Regulation;Reliability;Testing;Training;User experience
N2  - This paper showcases the development and deployment of a generative AI-based chatbot, using cutting-edge natural language processing techniques to deliver an interactive and informative user experience. Accessible through web browsers on various operating systems, the chatbot aims to maximize accessibility. The chatbot demonstrates the capabilities of the ChatGPT model from OpenAI, seamlessly integrating with APIs and adhering to GDPR regulations to ensure reliability and privacy compliance. Comprehensive evaluations were conducted to optimize the chatbot's performance and development efforts. The results validate the effectiveness of the approach, with user acceptance testing providing valuable feedback for further improvements. The results show that the proposed conversational model which was trained and worked based on ChatGPT has a superior performance to traditional conversational models, and could work optimally by applying any language without any additional training.
ER  -
TY  - JOUR
AU  - Lee, Dong-Kyu
AU  - Joe, Inwhee
T1  - A GPT-Based Code Review System With Accurate Feedback for Programming Education
JO  - IEEE Access
Y1  - 2025
VL  - 13
SP  - 105724–105737
EP  - 105724–105737
M3  - https://doi.org/10.1109/ACCESS.2025.3581139
KW  - Accuracy;Automation;Chatbots;Codes;Education;GPT-4o;LangChain;Large language models (LLMs);learner-friendly code reviews;programming education;Programming profession;Proposals;Reviews;Training;Usability
U1  - 2169-3536
N2  - The increasing demand for programming education and growing class sizes require immediate and personalized feedback. However, integrating Large Language Models (LLMs) like ChatGPT in introductory programming courses raises concerns about AI-assisted cheating. In large-scale settings, faulty code submissions may lead LLMs to overanalyze, causing unnecessary token consumption. This paper proposes a GPT-4o-based code review system that provides accurate feedback while reducing token usage and preventing AI-assisted cheating. Unlike general-purpose LLM tools for professionals, the system is pedagogically designed for primary and secondary students by focusing on review necessity and learner-friendly feedback. The system features a Code Review Module (CRM) that reduces token usage via a Review Necessity Chain (RNC), and Code Correctness Check Module (CCM) combining test case validation with LLM-based assessment. To prevent AI-assisted cheating, the system provides automated feedback on submitted code without prompting and revealing correct answers, which are accessed only through the ``Ask Code Tutor'' button. In usability test, the system detected up to 42.86% more errors than a conventional online judge. BERTScore analysis showed that over 80% of the system-generated reviews were semantically aligned with human feedback. A performance comparison with state-of-the-art systems demonstrated a blocking success rate of 86%, with a comparable review omission rate. These results indicate that the system provides more accurate feedback than conventional automated code reviews, while achieving token efficiency and supporting self-directed learning through educational feedback. Thus, it can serve as a practical solution for scalable programming education in primary and secondary classes.
ER  -
TY  - CONF
AU  - Li, Bohang
AU  - Zhang, Kai
AU  - Sun, Yiping
AU  - Zou, Jianke
T1  - Research on Travel Route Planning Optimization based on Large Language Model
SP  - 352–357
EP  - 352–357
M3  - https://doi.org/10.1109/DOCS63458.2024.10704489
KW  - Accuracy;Graph search algorithm;Heuristic algorithms;Large Language Model;Large language models;Optimization;Planning;Prediction algorithms;Predictive models;Real-time systems;Route optimization;Spatial databases;Time factors;Travel route planning
N2  - In a trip planning service, travelers first set a starting point, a destination, and a sequential list of specific points of interest types (e.g., museums, restaurants, and parks). Based on this information, the service searches the spatial database to customize the best travel itinerary for the tourist. However, in previous studies, planners only considered the time factor when designing the optimal route, and failed to fully consider the quality of each point of interest. In this study, we specifically leveraged the capabilities of large language model to parse and respond to complex travel-related user queries. To apply large language model to route planning, we fine-tuned the model to understand geotagging and user travel preferences. We have introduced a novel graph search algorithm combined with large language model output, which optimizes the route search process to provide optimal travel recommendations by taking into account various factors such as distance length, budget constraints, and popularity of tourist attractions. In addition, we have integrated real-time traffic data and historical travel data to further improve the prediction accuracy and application usefulness of the model. In the experimental validation phase, we designed a series of benchmarks to compare the performance of the system with traditional algorithms and other machine learning-based route planning methods. Experimental results show that our model has a significant improvement compared with traditional methods in improving the speed and accuracy of path selection.
ER  -
TY  - JOUR
AU  - Li, Qing
AU  - Xiong, Yanxu
AU  - Li, Zonghang
AU  - Ma, Chongxi
AU  - Yu, Hongfang
AU  - Sun, Gang
AU  - Luo, Long
AU  - Zhang, Zhaofeng
T1  - KlonetAI: Automating (Com)2Nets Management With Human Language Intents
JO  - IEEE Network
Y1  - 2025
VL  - 39
IS  - 3
SP  - 12–19
EP  - 12–19
M3  - https://doi.org/10.1109/MNET.2024.3507801
KW  - Agent-based modeling;Automation;Complex networks;Configuration management;Graphics processing units;Knowledge based systems;Large language models;Machine Learning;Natural language processing;Performance evaluation;Servers;Software defined networking;Telecommunication network management;Virtualization
U1  - 1558-156X
N2  - The communication and computation integrated network architecture ((Com)2Net) simplifies the deployment of applications such as distributed machine learning. However, the management and operation of the (Com)2Net pose significant burdens and consume a substantial amount of time due to its large-scale and complexity nature. Intent-based networking technology and Large Language Models (LLMs) are beneficial for the automation configuration and management of (Com)2Nets. In this article, we introduce a network automation configuration and management architecture. It implements network automation configuration based on user intents in natural language, reducing the network management complexity. The network management agent, based on a LLM, is proposed to translate user intents in natural language into machine instructions. Finally, we present two use cases and real-world evaluations to validate the performance of this architecture. The demo and code are open available on: https://github.com/Lizonghang/KlonetAI.
ER  -
TY  - CONF
AU  - Li, Youwei
AU  - Li, Yangyang
AU  - Yang, Yangzhao
T1  - Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model
SP  - 609–614
EP  - 609–614
M3  - https://doi.org/10.1109/DTPI61353.2024.10778901
KW  - Adaptation models;Agent;App Automation Testing;Automation;Deep Learning;Digital twins;Large Language Model;Large language models;Logic;Mobile applications;Mobile handsets;Natural languages;Testing
N2  - This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.
ER  -
TY  - CONF
AU  - de Lima, Vitor Mesaque Alves
AU  - Barbosa, Jacson Rodrigues
AU  - Marcacini, Ricardo Marcodes
T1  - iRisk: A Scalable Microservice for Classifying Issue Risks Based on Crowdsourced App Reviews
SP  - 858–862
EP  - 858–862
M3  - https://doi.org/10.1109/ICSME58944.2024.00091
KW  - App Reviews;Computer architecture;Issue Prioritization;Large Language Model;Microservice architectures;Monitoring;Opinion Mining;Reviews;Risk analysis;Risk Matrix;Risk mitigation;Software development management;Software maintenance;Stakeholders;User experience
N2  - Analyzing mobile app reviews is essential for identifying trends and issue patterns that affect user experience and app reputation in app stores. A risk matrix provides a straightforward, intuitive method to prioritize software maintenance actions to mitigate negative ratings. However, manually constructing a risk matrix is time-consuming, and stakeholders often struggle to understand the context of risks due to varied descriptions and the sheer volume of reviews. Therefore, machine learning-based methods are needed to extract risks and classify their priority effectively. While existing studies have automated risk matrix generation in software development, they have not explored app reviews or utilized Large Language Models (LLMs) in a scalable architecture. To address this gap, we present iRisk (scalable microservice for classifying issue Risks), a tool for generating a risk matrix based on crowdsourced app reviews using LLM. We present i-LLAMA, a fine-tuned version of LLaMA 3, optimized to detect and prioritize app-related issues using a risk analysis dataset of reviews categorized by severity and likelihood of occurrence. This dataset is also publicly available. Our contributions include the open-source resources to support the software maintenance and evolution industry, fine-tuning of LLaMA 3, and a scalable microservice architecture to handle large volumes of data. The iRisk can manage app issues and risks and provide an automated dashboard and visualizations for decision-making, monitoring, and risk mitigation. The tool is available on GitHub11https://github.com/vitormesaque/iRisk, and a presentation about the tool can be found in this video22https://irisk.mappidea.com.
ER  -
TY  - JOUR
AU  - Liu, M.
AU  - Zhang, J.
AU  - Nyagoga, L. M.
AU  - Liu, L.
T1  - Student-AI Question Cocreation for Enhancing Reading Comprehension
JO  - IEEE Transactions on Learning Technologies
Y1  - 2024
VL  - 17
SP  - 815–826
EP  - 815–826
M3  - https://doi.org/10.1109/TLT.2023.3333439
KW  - Artificial intelligence;Authoring systems;Computational modeling;Educational technology;Federated learning;Human computer interaction;Internet;Natural language processing;Task analysis;Writing
N2  - Student question generation (SQG) is an effective strategy for improving reading comprehension. It helps students improve their understanding of reading materials, metacognitively monitor their comprehension, and self-correct comprehension gaps. Internet technologies have been used to facilitate SQG process through intensive peer support. However, the availability, level of task commitment, and capabilities of student peers have emerged as significant concerns, particularly in light of the global pandemic and the subsequent postpandemic era. Thus, this article presents a student-artificial intelligence (AI) cocreation tool called CoAsker for supporting question generation. Following recent human-computer interaction (HCI) research in human-AI collaborative writing, CoAsker first allows students to provide question clues and answers and then uses a state-of-the-art pretrained language model, T5-PEGASUS, to generate questions. Finally, the student can use this AI question directly or perform reflection by comparing his or her questions with the AI question. An empirical study was conducted to examine the quality of AI questions and the effect of this tool on student engagement and reading comprehension. The results of the study show that students using this tool (treatment) were more engaged in generating low-level cognitive questions and performed better in acquiring knowledge than those using a traditional online question generation tool (control). These results indicate that student-AI question cocreation is beneficial to SQG training and educational assessment for reading comprehension, such as repeated practices.
ER  -
TY  - CONF
AU  - Kim, Yeonwoo
AU  - Lee, Junhyeok
AU  - Han, Ju Hyuk
AU  - Kim, Minjae
AU  - Lee, Howook
AU  - Lee, Won Hee
T1  - Agentic LLM Workflows for Personalized User Experience Questionnaire Generation
SP  - 1–4
EP  - 1–4
M3  - https://doi.org/10.1109/ICCE-Asia63397.2024.10773955
KW  - Accuracy;Agentic Workflow;Data models;Gaze tracking;Large language models;Physiology;Real-time systems;Training data;User centered design;User experience;User Experience Questionnaire
N2  - Effective user experience (UX) evaluation requires personalized assessment methods that adapt to individual user characteristics and real-time context. This study introduces the User Experience Questionnaire generation workflow using multiple LLMs (UEQ-mLLM), a system that generates tailored questionnaires based on user data collected through a multimodal interactive dashboard. By leveraging user information and states, UEQ-mLLM generates questionnaires that enhance the accuracy and depth of UX evaluations. Comparative analysis against a single LLM-based approach using the G-Eval framework demonstrated a significant performance improvement of 20.62% for UEQ-mLLM. This work highlights the potential of utilizing multiple LLMs to generate effective UX questionnaires and contributes to the advancement of user-centered design methodologies.
ER  -
TY  - CONF
AU  - Jacobs, S.
AU  - Jaschke, S.
T1  - Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/CSEET62301.2024.10663001
KW  - Codes;Conferences;Feedback;GPT-4;Large language models;programming education;Programming profession;Recording;Retrieval augmented generation;Videos
N2  - This paper presents the use of Retrieval Augmented Generation (RAG) to improve the feedback generated by Large Language Models for programming tasks. For this purpose, corresponding lecture recordings were transcribed and made available to the Large Language Model GPT-4 as external knowledge source together with timestamps as metainformation by using RAG. The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture. In an exercise platform developed to solve programming problems for an introductory programming lecture, students can request feedback on their solutions generated by GPT-4. For this task GPT-4 receives the students' code solution, the compiler output, the result of unit tests and the relevant passages from the lecture notes available through the use of RAG as additional context. The feedback generated by GPT-4 should guide students to solve problems independently and link to the lecture content, using the time stamps of the transcript as meta-information. In this way, the corresponding lecture videos can be viewed immediately at the corresponding positions. For the evaluation, students worked with the tool in a workshop and decided for each feedback whether it should be extended by RAG or not. First results based on a questionnaire and the collected usage data show that the use of RAG can improve feedback generation and is preferred by students in some situations. Due to the slower speed of feedback generation, the benefits are situation dependent.
ER  -
TY  - CONF
AU  - Jacob, T. Prem
AU  - Bizotto, B. L. S.
AU  - Sathiyanarayanan, M.
T1  - Constructing the ChatGPT for PDF Files with Langchain - AI
SP  - 835–839
EP  - 835–839
M3  - https://doi.org/10.1109/ICICT60155.2024.10544643
KW  - Chatbots;ChatGPT;Chroma DB;Deep Learning;Information retrieval;LangChain;OpenAI;Pinecone;Portable document format;Probability density function;Redis;Semantic search;Transformers;User experience;Vector Embeddings
N2  - Queries in PDFs can be time-consuming and labor-intensive because of the unstructured nature of the PDF document type and the need for accurate and relevant search results. By applying cutting-edge algorithms for natural language processing to examine PDF documents and extract relevant data, LangChain solves these difficulties. It makes use of an easy search interface, adjustable filters, and efficient indexing and retrieval mechanisms to enhance the search experience. To efficiently retrieve relevant information from PDF documents, users can annotate critical portions, store queries, and create bookmarks with LangChain. The characteristics of LangChain improve overall productivity and greatly simplify PDF querying. Semantic search, driven by the latest Transformer language models, represents a significant evolution in information retrieval systems. This research work explores the capabilities of semantic search to efficiently retrieve documents from large collections in response to natural language queries. Unlike traditional keyword-based approaches, semantic search connects the power of Transformer models to discern meaning, providing users with more contextually relevant and accurate results within seconds. This technology not only enhances the user experience by delivering superior matches from document collections but also lays the foundation for tackling more intricate tasks, like text summarization and question-answering. The research investigates the impact of semantic search on information retrieval efficiency and accuracy, comparing its performance with conventional methods. The findings presented herein not only showcase the immediate benefits of semantic search but also open paths for future research and development in natural language processing and its applications.
ER  -
TY  - CONF
AU  - Izadi, M.
AU  - Katzy, J.
AU  - van Dam, T.
AU  - Otten, M.
AU  - Popescu, R. M.
AU  - van Deursen, A.
T1  - Language Models for Code Completion: A Practical Evaluation
SP  - 956–968
EP  - 956–968
M3  - https://doi.org/10.1145/3597503.3639138
KW  - Analytical models;Automatic Code Completion;CodeGPT;Codes;Data models;evaluation;ide;InCoder;language models;Open Source;Predictive models;Training;Training data;Transformers;UniXcoder
N2  - Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outper-formed the other models across all programming languages, high-lighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the models' predictions, we found that 66.3% of failures were due to models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.
ER  -
TY  - CONF
AU  - Franzosi, Diogo Buarque
AU  - Alégroth, Emil
AU  - Isaac, Maycel
T1  - LLM-Based Labelling of Recorded Automated GUI-Based Test Cases
SP  - 453–463
EP  - 453–463
M3  - https://doi.org/10.1109/ICST62969.2025.10988984
KW  - Automation;Graphical user interfaces;Labeling;Manuals;Software development management;Software testing;Surveys;Testing;Transforms;Usability
N2  - Graphical User Interface (GUI) based testing is a commonly used practice in industry. Although valuable and, in many cases, necessary, it is associated with challenges such as high cost and requirements on both technical and domain expertise. Augmented testing, a novel approach to GUI test automation, aims to mitigate these challenges by allowing users to record and render test cases and test data directly on the GUI of the system under test (SUT). In this context, Scout is an augmented testing tool that captures system states and transitions during manual interaction with the SUT, storing them in a test model that is visually represented in the form of state trees and reports. While this representation provides basic overview of a test suite, e.g. about its size and number of scenarios, it is limited in terms of analysis depth, interpretability, and reproducibility. In particular, without human state labeling, it is challenging to produce meaningful and easily understandable test reports. To address this limitation, we present a novel solution and a demonstrator, integrated into Scout, which leverages large language models (LLMs) to enrich the model-based test case representation by automatically labeling and describing states and describing transitions. We conducted two experiments to evaluate the impact of the solution. First, we compared LLM-enhanced reports with expert-generated reports using embedding distance evaluation metrics. Second, we assessed the usability and perceived value of the enhanced reports through an industrial survey. The results of the study indicate that the plugin can improve readability, actionability, and interpretability of test reports. This work contributes to the automation of GUI testing by reducing the need for manual intervention, e.g. labeling, and technical expertise, e.g. to understand test case models. Although the solution is studied in the context of augmented testing, we argue for the solution's generalizability to related test automation techniques. In addition, we argue that this approach enables actionable insights and lays the groundwork for further research into autonomous testing based on Generative AI.
ER  -
TY  - CONF
AU  - Galimzyanov, Timur
AU  - Titov, Sergey
AU  - Golubev, Yaroslav
AU  - Bogomolov, Egor
T1  - Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code
SP  - 503–507
EP  - 503–507
M3  - https://doi.org/10.1109/MSR66628.2025.00083
KW  - Benchmark testing;Codes;Data models;Data visualization;Large language models;Libraries;Natural languages;Software;User experience;Visualization
N2  - This paper introduces the human-curated Pandas-PlotBench dataset, designed to evaluate language models' effectiveness as assistants in visual data exploration. Our benchmark focuses on generating code for visualizing tabular data—such as a Pandas DataFrame—based on natural language instructions, complementing current evaluation tools and expanding their scope. The dataset includes 175 unique tasks. Our experiments assess several leading Large Language Models (LLMs) across three visualization libraries: Matplotlib, Seaborn, and Plotly. We show that the shortening of tasks has a minimal effect on plotting capabilities, allowing for the user interface that accommodates concise user input without sacrificing functionality or accuracy. Another of our findings reveals that while LLMs perform well with popular libraries like Matplotlib and Seaborn, challenges persist with Plotly, highlighting areas for improvement. We hope that the modular design of our benchmark will broaden the current studies on generating visualizations. Our dataset and benchmark code is available online: https://huggingface. co/datasets/JetBrains-Research/PandasPlotBench; https://github.com/JetBrains-Research/PandasPlotBench.
ER  -
TY  - CONF
AU  - Gallagher, S. K.
AU  - Ratchford, J.
AU  - Brooks, T.
AU  - Brown, B.
AU  - Heim, E.
AU  - McMillan, S.
AU  - Nichols, W. R.
AU  - Rallapalli, S.
AU  - Smith, C.
AU  - VanHoudnos, N.
AU  - Winski, N.
AU  - Mellinger, A. O.
T1  - Assessing LLMs for High Stakes Applications
SP  - 103–105
EP  - 103–105
M3  - https://doi.org/10.1145/3639477.3639720
KW  - HCI;Large language models;Measurement;metrics;Reliability;scaling;Security;Software engineering;TEVV;trust;Tuning
N2  - Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in re-producibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.
ER  -
TY  - JOUR
AU  - Ge, J.
AU  - Chang, C.
AU  - Zhang, J.
AU  - Li, L.
AU  - Na, X.
AU  - Lin, Y.
AU  - Wang, F. Y.
T1  - LLM-Based Operating Systems for Automated Vehicles: A New Perspective
JO  - IEEE Transactions on Intelligent Vehicles
Y1  - 2024
VL  - 9
IS  - 4
SP  - 4563–4567
EP  - 4563–4567
M3  - https://doi.org/10.1109/TIV.2024.3399813
KW  - Automated vehicles;Cognition;Hardware;LLM;Memory management;operating system;Operating systems;Safety;Software;Task analysis
N2  - The deployment of large language models (LLMs) brings challenges to intelligent systems because its capability of integrating large-scale training data facilitates contextual reasoning. This paper envisions a revolution of the LLM based (Artificial) Intelligent Operating Systems (IOS, or AIOS) to support the core of automated vehicles. We explain the structure of this LLM-OS and discuss the resulting benefits and implementation difficulties.
ER  -
TY  - CONF
AU  - Gokkaya, Betul
T1  - Leveraging Large Language Models for Security Patch Detection and CWE Classification
SP  - 1–10
EP  - 1–10
M3  - https://doi.org/10.1109/ICHORA65333.2025.11017081
KW  - Accuracy;CWE Classification;Human computer interaction;Large language models;LLMs;Measurement;Open source software;Optimization;Prompt engineering;Robots;Security;Security Patch Detection;Silent Fix Identification;Source coding
N2  - Open-source software is critical for modern digital infrastructure, yet security vulnerabilities remain a significant concern as attackers exploit unpatched systems. Large Language Models (LLMs) have shown promise in vulnerability detection, but their ability to detect security patches solely based on code changes remains underexplored. This capability is crucial for identifying security patches when commit messages lack explicit security labels. This study evaluates six LLMs, e.g., GPT-4o, Claude 3.5 Haiku, and DeepSeek V3, using various prompting approaches to assess their capacity to distinguish between security and non-security patches and analyze their effectiveness in classifying security patches into their corresponding CWE categories. The results show that LLMs can detect security patches, but performance varies across models and prompting strategies. DeepSeek V3 (Chain-of-Thought) and GPT-4o (Zero-Shot) demonstrate the most consistent performance across all evaluation metrics, each achieving over 70% in accuracy, precision, recall, and F1-score. New prompting techniques have also led to notable improvements in certain areas, particularly in precision. However, CWE classification remains a major challenge, with most models misclassifying over 70% of security patches. Even the best-performing model, Claude Haiku 3.5 (Few-Shot), achieves only 31.1% accuracy, with memory-related vulnerabilities like out-of-bounds write and use-after-free being the most frequently misclassified. These findings highlight the potential of LLMs in security patch detection but emphasize the need for improved CWE classification. Source code available at: https://github.com/betulgkkaya/LLMs\_Patch\_Detection.git.
ER  -
TY  - CONF
AU  - Gu, Z.
AU  - Zhu, Q.
AU  - He, H.
AU  - Yu, Z.
AU  - Lan, T.
AU  - Yuan, S.
T1  - Multi-Level Knowledge-Enhanced Prompting for Empathetic Dialogue Generation
SP  - 3170–3175
EP  - 3170–3175
M3  - https://doi.org/10.1109/CSCWD61410.2024.10580095
KW  - Artificial intelligence;Computational modeling;Emotion recognition;empathetic dialogue system;Federated learning;Knowledge based systems;knowledge-enhanced prompting;Large Language Model;Large language models;Task analysis;User experience
N2  - Empathetic dialogue systems can recognize users' emotions and provide appropriate responses, which are crucial for enhancing the user experience. However, existing empathetic dialogue systems often fall short in understanding some complex implicit emotions. To address this problem, we propose a multi-level knowledge-enhanced prompting approach to achieve more effective empathetic dialogue generation effect. We first acquire topic words and emotional keywords as low-level emotional knowledge. Next, we retrieve dialogue samples that are most similar in topic and emotional attributes, forming mid-level emotional knowledge. Subsequently, we guide a large language model (LLM) to generate high-level comprehensive emotional knowledge based on the information from the previous two levels and the dialogue context. Finally, based on the emotional knowledge, we further guide LLM to generate empathetic responses. The research results indicate that our multi-level knowledge-enhanced prompting approach outperforms other baselines.
ER  -
TY  - CONF
AU  - Gupta, A. D.
AU  - Danishan
AU  - Kumar, A.
AU  - Chaudhary, I.
AU  - Yasir, A. M.
AU  - Kumar, N.
T1  - My Assistant SRSTC: Speech Recognition and Speech to Text Conversion
SP  - 394–400
EP  - 394–400
M3  - https://doi.org/10.1109/IC3SE62002.2024.10593324
KW  - Acoustic modeling;Deep Learning;Human computer interaction;Measurement;Natural language processing;Natural language processing NLP;Productivity;Response generation;Semantic parsing;Speech recognition;Text Recognition;Text-to-speech (TTS);Virtual assistant;Virtual assistants
N2  - `My Assistant,' a speech-enabled virtual assistant created to promote smooth human-machine interaction, is presented in this research study. By utilizing cutting-edge speech recognition and natural language processing technologies, My Assistant can translate spoken words into text, carry out user commands, and provide contextually appropriate answers. The study highlights the significance of interacting with other systems and APIs while discussing the fundamental methods of voice recognition, task execution, and answer generation [1]. Metrics for assessing speech recognition precision and user contentment are suggested, accompanied with comparisons with current personal assistants. The results show how effective and promising My Assistant is for raising user convenience and productivity [2].
ER  -
TY  - CONF
AU  - Haji, K. E.
AU  - Brandt, C.
AU  - Zaidman, A.
T1  - Using GitHub Copilot for Test Generation in Python: An Empirical Study
SP  - 45–55
EP  - 45–55
KW  - Codes;Runtime;Software;Source coding;Syntactics;Test pattern generators;Writing
N2  - Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28 % of the tests generated by Copilot are passing tests; 54.72 % of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45 % of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.
ER  -
TY  - JOUR
AU  - Haldar, Susmita
AU  - Pierce, Mary
AU  - Fernando Capretz, Luiz
T1  - Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning
JO  - IEEE Access
Y1  - 2025
VL  - 13
SP  - 46070–46090
EP  - 46070–46090
M3  - https://doi.org/10.1109/ACCESS.2025.3545882
KW  - Accuracy;Capstone project;Chatbots;ChatGPT;Education;Generative AI;Industries;Large language models;Microsoft Copilot;Sentiment analysis;Software engineering;Software testing;software testing education;Systematic literature review
U1  - 2169-3536
N2  - Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students' responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.
ER  -
TY  - CONF
AU  - Hoffmann, J.
AU  - Frister, D.
T1  - Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models
SP  - 76–77
EP  - 76–77
KW  - Codes;Computational modeling;Data models;Large language models;Machine Learning;Mobile Testing;Software;Software testing;Training data;Transformers
N2  - Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13-15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.
ER  -
TY  - CONF
AU  - Hu, Nan
AU  - Fan, Ming
AU  - Lei, Jingyi
AU  - He, Jiaying
AU  - Hou, Zhe
T1  - ELDetector: An Automated Approach Detecting Endless-loop in Mini Programs
SP  - 104–112
EP  - 104–112
M3  - https://doi.org/10.1109/Forge66646.2025.00019
KW  - Accuracy;Authorization;automated GUI testing;Collaboration;endless-loop;Foundation models;Graphical user interfaces;Large Language Model;Large language models;Measurement;mini program;Object recognition;Software engineering;User experience
N2  - In recent years, mini-programs have rapidly gained popularity and are widely used in payment, travel, shopping and other fields, greatly enhancing the convenience of users' lives. However, these services usually require access to sensitive personal information such as phone numbers, location information, ID numbers and other permissions. In the process of using them, users may frequently encounter permission requests, and sometimes even be forced to authorize them, leading to poor usage experience or even falling into an endless-loop authorization cycle that is difficult to exit. Unfortunately, most of the existing studies are fragmented and only deal with individual issues of personal information usage in mini-programs, lacking a comprehensive analysis of how permission requests affect user experience. To address this problem, this paper proposes an automated tool called ELDetector that automatically traverses mini-program pages through dynamic analysis and detects authorization endless-loops with the assistance of the Large Language Model (LLM). We find that authorization endless-loops of mini-programs are mainly classified into two categories: single-page endless-loop and multipage endless-loop, based on the number of pages caught in the loop. We evaluated ELDetector on 97 popular mini-programs with an accuracy of 79.4% in detecting the authorization endless-loop problem, of which 15 mini-programs have been fixed by the developers. In addition, with the help of the Large Language Model (LLM), ELDetector is 54% faster than the traditional monkey test in identifying authorization endless-loop entrance points.
ER  -
TY  - CONF
AU  - Huang, Y.
T1  - Generating User Experience Based on Personas with AI Assistants
SP  - 181–183
EP  - 181–183
M3  - https://doi.org/10.1145/3639478.3639810
KW  - Automation;Guidelines;Proposals;Reviews;Software engineering;Standards;User experience
N2  - Traditional UX development methodologies focus on developing ``one size fits all'' solutions and lack the flexibility to cater to diverse user needs. In response, a growing interest has arisen in developing more dynamic UX frameworks. However, existing approaches often cannot personalise user experiences and adapt to user feedback in realtime. Therefore, my research introduces a novel approach of combining Large Language Models and personas, to address these limitations. The research is structured around three areas: (1) a critical review of existing adaptive UX practices and the potential for their automation; (2) an investigation into the role and effectiveness of personas in enhancing UX adaptability; and (3) the proposal of a theoretical framework that leverages LLM capabilities to create more dynamic and responsive UX designs and guidelines.
ER  -
TY  - CONF
AU  - Huq, Syed Fatiul
AU  - Tafreshipour, Mahan
AU  - Kalcevich, Kate
AU  - Malek, Sam
T1  - Automated Generation of Accessibility Test Reports from Recorded User Transcripts
SP  - 204–216
EP  - 204–216
M3  - https://doi.org/10.1109/ICSE55347.2025.00043
KW  - crowd-sourced software testing;Large language models;Reproducibility of results;Semantics;Software;software accessibility;Software engineering;Software testing;Systematics;Testing;Usability;Video recording
N2  - Testing for accessibility is a significant step when developing software, as it ensures that all users, including those with disabilities, can effectively engage with web and mobile applications. While automated tools exist to detect accessibility issues in software, none are as comprehensive and effective as the process of user testing, where testers with various disabilities evaluate the application for accessibility and usability issues. However, user testing is not popular with software developers as it requires conducting lengthy interviews with users and later parsing through large recordings to derive the issues to fix. In this paper, we explore how large language models (LLMs) like GPT 4.0, which have shown promising results in context comprehension and semantic text generation, can mitigate this issue and streamline the user testing process. Our solution, called Reca11, takes in auto-generated transcripts from user testing video recordings and extracts the accessibility and usability issues mentioned by the tester. Our systematic prompt engineering determines the optimal configuration of input, instruction, context and demonstrations for best results. We evaluate Reca11's effectiveness on 36 user testing sessions across three applications. Based on the findings, we investigate the strengths and weaknesses of using LLMs in this space.
ER  -
TY  - CONF
AU  - Israilidis, J.
AU  - Chen, W. Y.
AU  - Tsakalerou, M.
T1  - Software Development and Education: Transitioning Towards AI Enhanced Teaching
SP  - 1–6
EP  - 1–6
M3  - https://doi.org/10.1109/EDUCON60312.2024.10578564
KW  - AI tools;Education;Ethics;Focusing;Generative AI;Privacy;Product development;Software;Software development;Surveys
N2  - This paper investigates the impact of large language model (LLM) AI tools, such as ChatGPT and Copilot, on software development education, focusing on usability, efficiency, and effectiveness in real-world scenarios. The research employs a quantitative approach, utilizing a survey of 50 software developers with varying levels of experience. Preliminary findings suggest that AI tools have a positive influence on expediting coding tasks and automating text generation, particularly in the early stages of product development. Challenges related to customization, accuracy, and transparency, as well as concerns about their potential impacts on employment, personal privacy, and ethical boundaries, have been identified. Pointers and initial recommendations for transitioning to AI-enhanced teaching and optimizing interactions between learners and generative AI practices are provided.
ER  -
TY  - CONF
AU  - Ivanov, R.
AU  - Velkova, V.
T1  - Microservice-Based Interface to ChatGPT
SP  - 1–5
EP  - 1–5
M3  - https://doi.org/10.1109/AQTR61889.2024.10554146
KW  - AMQP;Chatbots;GPT API;Microservice architectures;microservices;MSA;Museums;Programming profession;Protocols;Reliability;Robots
N2  - In today's digitally connected world, the emergence of conversational artificial intelligence powered by generative language models has ushered in a new era of human-computer interaction. Chatbots using these technologies are increasingly being used in a variety of scientific as well as social domains. These intelligent conversational agents, powered by advances in generative language models, offer a wide range of applications from customer support and healthcare to software development and education. This paper discusses the development of a microservice that works as an interface to ChatGPT through the GPT API. The goal is to facilitate the integration of next generation chatbots to distributed architecture services. Access to the microservice is implemented using an Advanced Message Queuing Protocol (AMQP) message broker. To conduct the experiments, a microservice was developed that provides a REST interface to the proposed microservice for clients that do not support the AMQP protocol.
ER  -
TY  - CONF
AU  - Zhu, Minhao
AU  - Gu, Huanhuan
AU  - Che, Xun
AU  - Chen, Jingfei
AU  - Zhao, Qian
AU  - Liu, Fan
AU  - Zheng, Yu
T1  - A Novel Diversified API Recommendation for Power System Sensors
SP  - 17–22
EP  - 17–22
M3  - https://doi.org/10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00027
KW  - Accuracy;api recommendation;Data mining;industrial Internet of Things;Internet of Things;IoT sensor;Large language models;power system;Power systems;recommendation system;Recommender systems;Sensor systems;Smart grids;Soft sensors;Vectors
N2  - The collaborative interaction between power systems and the Internet of Things (IoT) is strengthening, with IoT devices facilitating real-time monitoring and governance of the power grid, pushing power systems to the next frontier: the smart grid. Nevertheless, the reliance of power systems on a vast array of IoT devices, each with its unique API, makes the development of a unified smart grid software solution extremely complex. Most existing research focuses on accuracy in recommendations, thus neglecting users' needs for functional diversity. To address this issue, we propose a diversified API recommendation approach that suggests a variety of functional APIs from power system sensors to users. We start by converting API labels into feature vectors based on a pre-trained language model and calculate the similarity between APIs through clustering. Subsequently, we construct an API graph to model the functional similarity relationships between APIs. Finally, we generate a minimum weighted tree to obtain a combination of APIs that meets the requirements and ensures diversity. We demonstrate the advantages of our proposed method in terms of diversity through a case study.
ER  -
TY  - CONF
AU  - Zimmermann, D.
AU  - Koziolek, A.
T1  - GUI-Based Software Testing: An Automated Approach Using GPT-4 and Selenium WebDriver
SP  - 171–174
EP  - 171–174
M3  - https://doi.org/10.1109/ASEW60602.2023.00028
KW  - Calculators;Codes;Conferences;Deep Learning;GPT-4;language models;Selenium;Software testing;Test Automation;Training data;UI Testing
N2  - This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.
ER  -
TY  - JOUR
AU  - Deng, L. J.
AU  - Zhong, Q.
AU  - Song, J. C.
AU  - Lei, H.
AU  - Li, W. J.
T1  - LLM-Based Unknown Function Automated Modeling in Sensor-Driven Systems for Multi-Language Software Security Verification
JO  - SENSORS
Y1  - 2025
VL  - 25
IS  - 9
M3  - https://doi.org/10.3390/s25092683
KW  - LLM;Sensors;symbolic execution;vulnerability verification;WebAssembly
U1  - 1424-8220
N2  - The rapid expansion of the Internet of Things (IoT) has made software security and reliability a critical concern. With multi-language programs running on edge computing, embedded systems, and sensors, each connected device represents a potential attack vector, threatening data integrity and privacy. Symbolic execution is a key technique for automated vulnerability detection. However, unknown function interfaces, such as sensor interactions, limit traditional concrete or concolic execution due to uncertain function returns and missing symbolic expressions. Compared with system simulation, the traditional method is to construct an interface abstraction layer for the symbolic execution engine to reduce the cost of simulation. Nevertheless, the disadvantage of this solution is that the manual modeling of these functions is very inefficient and requires professional developers to spend hundreds of hours. In order to improve efficiency, we propose an LLM-based automated approach for modeling unknown functions. By fine-tuning a 20-billion-parameter language model, it automatically generates function models based on annotations and function names. Our method improves symbolic execution efficiency, reducing reliance on manual modeling, which is a limitation of existing frameworks like KLEE. Experimental results primarily focus on comparing the usability, accuracy, and efficiency of LLM-generated models with human-written ones. Our approach was integrated into one verification platform project and applied to the verification of smart contracts with distributed edge computing characteristics. The application of this method directly reduces the manual modeling effort from a month to just a few minutes. This provides a foundational validation of our method's feasibility, particularly in reducing modeling time while maintaining quality. This work is the first to integrate LLMs into formal verification, offering a scalable and automated verification solution for sensor-driven software, blockchain smart contracts, and WebAssembly systems, expanding the scope of secure IoT development.
ER  -
TY  - JOUR
AU  - Filter, M.
AU  - Schüler, T.
AU  - Ben Romdhane, R.
T1  - Food Safety Knowledge Exchange (FSKX) format: Current status and strategic development plans based on a SWOT analysis
JO  - MICROBIAL RISK ANALYSIS
Y1  - 2024
VL  - 27-28
M3  - https://doi.org/10.1016/j.mran.2024.100309
KW  - Data standards;FAIR data;Knowledge exchange;Linked models
U1  - 2352-3530
N2  - The Food Safety Knowledge Exchange (FSKX) format is a community-driven effort initially created to promote the efficient exchange of data and models in the food safety domain. Over the past years this effort was driven by the Risk Assessment Knowledge Integration Platform (RAKIP) Initiative that also provided a number of software tools and FSKX-compliant model files via their website https://foodrisklabs.bfr.bund.de/rakip-initiative/. This paper describes the results of a SWOT analysis that was conducted to identify strategic avenues for enhancing FSKX's usability and adoption. The SWOT analysis identified a number of recommendations for the future evolution of FSKX. First, it is recommended to reduce the complexity of the annotation schema to ease the adoption of the format. Second, a clear distinction between the descriptive part of FSKX and the executable part is proposed. To promote the broad usage of FSKX-compliant models, it is also recommended to develop and provide FSKX-compliant APIs and resources that facilitate cloud-based execution. As part of the research to prioritize future FSKX development options, we also considered the implications of the emerging generative AI technologies, particularly which impact large language models (LLMs) might have in supporting the adoption of FSKX by the research community. Recognizing the format's application potential beyond the food safety domain, we then proposed to re-brand the FSKX acronym as \textquotedblFAIR Scientific Knowledge Exchange Format\textquotedbl which better reflects its broad applicability in various scientific domains. Our research findings suggest that with the implementation of the improvements identified by the SWOT analysis and the broader availability of generative AI technologies the broad adoption of FSKX as a method to share data and models in a FAIR way comes into reach.
ER  -
TY  - JOUR
AU  - Król, K.
T1  - Between Truth and Hallucinations: Evaluation of the Performance of Large Language Model-Based AI Plugins in Website Quality Analysis
JO  - APPLIED SCIENCES-BASEL
Y1  - 2025
VL  - 15
IS  - 5
M3  - https://doi.org/10.3390/app15052292
KW  - ACCESSIBILITY;AI hallucinations;AI plugin;cautious approach;ChatGPT;GPT model;LLM;quality audit;TECHNOLOGIES;Usability
U1  - 2076-3417
N2  - Featured Application The AI plugins can support website and web application auditing, facilitating preliminary evaluation of quality attributes, particularly regarding search engine optimisation and accessibility.Abstract Although large language models (LLMs) like the Generative Pre-trained Transformer (GPT) are growing increasingly popular, much remains to learn about their potential for website quality auditing. The article evaluates the performance of LLM AI plugins (GPT models) in website and web application auditing. The author built and tested two original ChatGPT-4o Plus (OpenAI) plugins: Website Quality Auditor (WQA) and WebGIS Quality Auditor (WgisQA). Their performance was cautiously and carefully analysed and compared to traditional auditing tools. The results demonstrated the limitations of the AI plugins, including their propensity for false outcomes. The general conclusion is that using AI tools without considering their characteristics may lead to the propagation of AI hallucinations in audit reports. The study fills in the research gap with the results on the capabilities and limitations of AI plugins in the context of auditing. It also suggests further directions for improvement.
ER  -
TY  - JOUR
AU  - Montella, R.
AU  - de Vita, C. G.
AU  - Mellone, G.
AU  - Ciricillo, T.
AU  - Caramiello, D.
AU  - Di Luccio, D.
AU  - Kosta, S.
AU  - Damasevicius, R.
AU  - Maskeliunas, R.
AU  - Queirós, R.
AU  - Swacha, J.
T1  - Leveraging Large Language Models to Support Authoring Gamified Programming Exercises
JO  - APPLIED SCIENCES-BASEL
Y1  - 2024
VL  - 14
IS  - 18
M3  - https://doi.org/10.3390/app14188344
KW  - Artificial intelligence;educational tools;gamification;programming education
U1  - 2076-3417
N2  - Featured Application The presented solution can be applied to simplify and hasten the development of gamified programming exercises conforming to the Framework for Gamified Programming Education (FGPE) standard.Abstract Skilled programmers are in high demand, and a critical obstacle to satisfying this demand is the difficulty of acquiring programming skills. This issue can be addressed with automated assessment, which gives fast feedback to students trying to code, and gamification, which motivates them to intensify their learning efforts. Although some collections of gamified programming exercises are available, producing new ones is very demanding. This paper presents GAMAI, an AI-powered exercise gamifier, enriching the Framework for Gamified Programming Education (FGPE) ecosystem. Leveraging large language models, GAMAI enables teachers to effortlessly apply storytelling to describe a gamified scenario, as GAMAI decorates natural language text with the sentences needed by OpenAI APIs to contextualize the prompt. Once a gamified scenario has been generated, GAMAI automatically produces exercise files in a FGPE-compatible format. According to the presented evaluation results, most gamified exercises generated with AI support were ready to be used, with no or minimum human effort, and were positively assessed by students. The usability of the software was also assessed as high by the users. Our research paves the way for a more efficient and interactive approach to programming education, leveraging the capabilities of advanced language models in conjunction with gamification principles.
ER  -
TY  - JOUR
AU  - Posedaru, B. S.
AU  - Batagan, L.
AU  - Bologa, R.
AU  - Placinta, D. D.
AU  - Mirea, C. M.
T1  - Software Architecture for Improving Scraping Systems Using Artificial Intelligence
JO  - ECONOMIC COMPUTATION AND ECONOMIC CYBERNETICS STUDIES AND RESEARCH
Y1  - 2024
VL  - 58
IS  - 3
SP  - 143–160
EP  - 143–160
M3  - https://doi.org/10.24818/18423264/58.3.24.09
KW  - ChatGPT;intelligent;Large Language Model (LLM);OpenAI;Web scraper
U1  - 1842-3264
N2  - This paper explores diverse innovative Web scraping techniques. It initially introduces a machine learning (ML) approach capable of adapting to changing HTML structures for continuous scraping. Following this, an automated method is detailed, specifically designed to extract data from dense web pages using subject detection and node density techniques. Lastly, the article covers a computer vision-based scraping strategy that employs object recognition and OCR to visually analyse and interpret websites. These methods seek to increase online scraping efficiency and reduce the reliance on HTML structure. The present study proposes an autonomous Web scraper system that integrates Web scraping, Artificial Intelligence (AI), and Natural Language Processing (NLP) techniques, leveraging ChatGPT's natural language understanding capabilities to achieve the desired results. The recommended JavaScript program uses NLP techniques to produce webpage lists for examination and lets users query data in natural language. By utilising cutting-edge AI and ML approaches, it promises to increase Web scraping's usability and effectiveness. The article details upcoming work, such as HTML clean-up and DOM parsing advancements, and examines the benefits of the suggested approach over the currently available tools.
ER  -
TY  - JOUR
AU  - Rodriguez, D. V.
AU  - Lawrence, K.
AU  - Gonzalez, J.
AU  - Brandfield-Harvey, B.
AU  - Xu, L.
AU  - Tasneem, S.
AU  - Levine, D. L.
AU  - Mann, D.
T1  - Leveraging Generative AI Tools to Support the Development of Digital Solutions in Health Care Research: Case Study
JO  - JMIR HUMAN FACTORS
Y1  - 2024
VL  - 11
M3  - https://doi.org/10.2196/52885
KW  - app;application;applications;apps;Artificial intelligence;behavior change;behaviour change;ChatGPT;developer;developers;diabetes;diabetes prevention;diabetic;digital health;digital prescription;engagement;GenAI;generative;language model;language models;LLM;LLMs;mHealth;mobile health;Natural language processing;NLP;Software;Software engineering
U1  - 2292-9495
N2  - Background: Generative artificial intelligence has the potential to revolutionize health technology product development by improving coding quality, efficiency, documentation, quality assessment and review, and troubleshooting. Objective: This paper explores the application of a commercially available generative artificial intelligence tool (ChatGPT) to the development of a digital health behavior change intervention designed to support patient engagement in a commercial digital diabetes prevention program. Methods: We examined the capacity, advantages, and limitations of ChatGPT to support digital product idea conceptualization, intervention content development, and the software engineering process, including software requirement generation, software design, and code production. In total, 11 evaluators, each with at least 10 years of experience in fields of study ranging from medicine and implementation science to computer science, participated in the output review process (ChatGPT vs human -generated output). All had familiarity or prior exposure to the original personalized automatic messaging system intervention. The evaluators rated the ChatGPT-produced outputs in terms of understandability, usability, novelty, relevance, completeness, and efficiency. Results: Most metrics received positive scores. We identified that ChatGPT can (1) support developers to achieve high -quality products faster and (2) facilitate nontechnical communication and system understanding between technical and nontechnical team members around the development goal of rapid and easy -to -build computational solutions for medical technologies. Conclusions: ChatGPT can serve as a usable facilitator for researchers engaging in the software development life cycle, from product conceptualization to feature identification and user story development to code generation. Trial Registration: ClinicalTrials.gov NCT04049500; https://clinicaltrials.gov/ct2/show/NCT04049500
ER  -
